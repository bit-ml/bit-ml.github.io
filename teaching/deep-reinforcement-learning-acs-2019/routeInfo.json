{"path":"teaching/deep-reinforcement-learning-acs-2019","templateID":3,"sharedPropsHashes":{"galleries":"HRMdc"},"localProps":{"post":{"data":{"slug":"deep-reinforcement-learning-acs-2019","categories":"event","featured_img":"/galleries/courses/atari_collage.png","date":"September-20-2019","title":"Deep Reinforcement Learning ","id":6},"messages":[],"history":["./content/collections/courses/rl_acs_2019.md","content/collections/courses/rl_acs_2019.html"],"cwd":"/home/zademn/Work/bit/bit-ml","contents":"<h1>Deep Reinforcement Learning <span>Advanced methods in AI module</span></h1>\n<p>For the <em>Advanced Methods in Artificial Intelligence</em> course we created\na four lectures and practicals module covering essential topics in Deep\nReinforcement Learning.</p>\n<p>Students learned about specific issues in training RL agents with non-linear\nfunction approximation and implemented during the practical a wide range\nof algorithms.</p>\n<ul>\n<li><strong>Prerequisites:</strong> Basic PyTorch knowledge.</li>\n<li><strong>Instructor:</strong> Florin Gogianu.</li>\n</ul>\n<h2>Syllabus</h2>\n<ol>\n<li><strong>Introduction to RL.</strong> Covers motivation, core concepts, value functions,</li>\n</ol>\n<p>Bellman equation and TD learning |\n<a href=\"https://floringogianu.github.io/courses/01_introduction/\">slides</a>.</p>\n<ul>\n<li><strong>Practical:</strong> Policy evaluation, Q-Learning, SARSA, Expected SARSA in</li>\n</ul>\n<p>a tabular setting. Used and modified with the permission of Diana Borsa.\n| <a href=\"https://github.com/floringogianu/rl-module-labs/blob/master/01_introduction.ipynb\">notebook</a>.\n2. <strong>Approximate methods.</strong> MDP definition, introduction to approximate\nsolution methods, geometrical intuition, non-linear function approximation, deadly triad,\nDeep Q-Networks, action overestimation, maximization bias and Double\nQ-Learning, Dueling DQN, Prioritized Experience Replay, Distributional RL,\nRainbow, Auxiliary Tasks |\n<a href=\"https://floringogianu.github.io/courses/02_approximate_solutions/\">slides</a>.</p>\n<ul>\n<li><strong>Practical:</strong> Implement DQN, Double-DQN and Dueling DQN on a variety of</li>\n</ul>\n<p>MiniGrid environments | <a href=\"https://github.com/floringogianu/rl-module-labs/blob/master/02_rainbow.ipynb\">notebook</a>.\n3. <strong>Policy Gradient Methods.</strong> Motivation, intuitions, policy gradient\ntheorem, REINFORCE, baselines, advantage function, generalized advantage\nfunctions, Asynchronous Actor Critic, optimization perspective and intro to\nTRPO | <a href=\"https://floringogianu.github.io/courses/03_policy_gradient\">slides</a>.\n- <strong>Practical:</strong> Implement REINFORCE, value-function baseline, Advantage\nActor-Critic, Generalized Advantage Actor-Critic on a range of discrete\nactions environments |\n<a href=\"https://github.com/floringogianu/rl-module-labs/blob/master/03_policy_gradient.ipynb\">notebook</a>.\n4. <strong>Advanced Policy Gradient Methods.</strong> Detailed TRPO, PPO, IMPALA,\nimportance sampling, V-Trace and a special section with RL applications in\nmachine learning | <a href=\"https://floringogianu.github.io/courses/04_advanced_pg\">slides</a>.</p>\n<h2>Administrative details</h2>\n<ul>\n<li><strong>When</strong>: October - November 2020, on Monday evening</li>\n<li><strong>Where</strong>: Faculty of Automatic Control and Computer Science, Politehnica</li>\n</ul>\n<p>University of Bucharest</p>\n<ul>\n<li><strong>Lectures</strong>: [lecture</li>\n</ul>\n<p>1](<a href=\"https://floringogianu.github.io/courses/01_introduction/\">https://floringogianu.github.io/courses/01_introduction/</a>), <a href=\"https://floringogianu.github.io/courses/02_approximate_solutions/\">lecture\n2</a>,\n<a href=\"https://floringogianu.github.io/courses/03_policy_gradient\">lecture 3</a>,\n<a href=\"https://floringogianu.github.io/courses/04_advanced_pg\">lecture 4</a></p>\n<ul>\n<li><strong>Practicals</strong>: <a href=\"https://github.com/floringogianu/rl-module-labs\">notebooks</a></li>\n</ul>\n"}}}
