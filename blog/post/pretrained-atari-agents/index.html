<!DOCTYPE html><html lang="en"><head><title data-react-helmet="true">:joystick: Pretrained Atari Agents | Bitdefender Research</title><meta data-react-helmet="true" name="description" content="🕹️ Pretrained Atari AgentsReleasing trained models in computer vision and natural language processing has been a major source of progress for the research in these fields and a significant catalyst for the adaption of deep learning models in the industry. By comparison, RL agents pretrained on otherwise resource and time intensive benchmarks such..."/><meta data-react-helmet="true" name="keywords" content="blog,reinforcement-learning"/><meta data-react-helmet="true" property="og:title" content=":joystick: Pretrained Atari Agents | Bitdefender Research"/><meta data-react-helmet="true" property="og:description" content="🕹️ Pretrained Atari AgentsReleasing trained models in computer vision and natural language processing has been a major source of progress for the research in these fields and a significant catalyst for the adaption of deep learning models in the industry. By comparison, RL agents pretrained on otherwise resource and time intensive benchmarks such..."/><meta data-react-helmet="true" property="og:site_name" content="https://bit-ml.github.io"/><meta data-react-helmet="true" property="article:tag" content="blog"/><meta data-react-helmet="true" property="article:tag" content="reinforcement-learning"/><meta data-react-helmet="true" property="og:image" content="https://bit-ml.github.io/galleries/atari_agents_2022/collage.png"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:site" content="@Bitdefender"/><meta data-react-helmet="true" name="twitter:title" content=":joystick: Pretrained Atari Agents | Bitdefender Research"/><meta data-react-helmet="true" name="twitter:description" content="🕹️ Pretrained Atari AgentsReleasing trained models in computer vision and natural language processing has been a major source of progress for the research in these fields and a significant catalyst for the adaption of deep learning models in the industry. By comparison, RL agents pretrained on otherwise resource and time intensive benchmarks such..."/><meta data-react-helmet="true" name="twitter:image" content="https://bit-ml.github.io/galleries/atari_agents_2022/collage.png"/><meta data-react-helmet="true" itemProp="description" content="🕹️ Pretrained Atari AgentsReleasing trained models in computer vision and natural language processing has been a major source of progress for the research in these fields and a significant catalyst for the adaption of deep learning models in the industry. By comparison, RL agents pretrained on otherwise resource and time intensive benchmarks such..."/><meta data-react-helmet="true" itemProp="keywords" content="blog,reinforcement-learning"/><meta data-react-helmet="true" itemProp="image" content="https://bit-ml.github.io/galleries/atari_agents_2022/collage.png"/><link rel="preload" as="script" href="https://bit-ml.github.io/bootstrap.f160fe0b.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/templates/src/pages/Post.5cee3c5c.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/main.95926ac4.js"/><link rel="preload" as="style" href="https://bit-ml.github.io/styles.04581cb1.css"/><link rel="stylesheet" href="https://bit-ml.github.io/styles.04581cb1.css"/><meta charSet="UTF-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><link rel="apple-touch-icon" href="icon.png"/><style data-styled-components="goldxz gFasvY cSOdgJ hAChiG fGODKX iRDKCt jSdCWo kAktca gMsmUp kSsCZe fJVWOL lhqqFe itWYlR">
/* sc-component-id: sc-global-16140688 */
html{line-height:1.15;-webkit-text-size-adjust:100%;} body{margin:0;} h1{font-size:2em;margin:0.67em 0;} hr{box-sizing:content-box;height:0;overflow:visible;} pre{font-family:monospace,monospace;font-size:1em;} a{background-color:transparent;-webkit-text-decoration:none;text-decoration:none;} abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;} b,strong{font-weight:bolder;} code,kbd,samp{font-family:monospace,monospace;font-size:1em;} small{font-size:80%;} sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;} sub{bottom:-0.25em;} sup{top:-0.5em;} figure{margin:0;} img{border-style:none;max-width:100%;} button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0;} button,input{overflow:visible;} button,select{text-transform:none;} button,[type="button"],[type="reset"],[type="submit"]{-webkit-appearance:button;} button::-moz-focus-inner,[type="button"]::-moz-focus-inner,[type="reset"]::-moz-focus-inner,[type="submit"]::-moz-focus-inner{border-style:none;padding:0;} button:-moz-focusring,[type="button"]:-moz-focusring,[type="reset"]:-moz-focusring,[type="submit"]:-moz-focusring{outline:1px dotted ButtonText;} fieldset{padding:0.35em 0.75em 0.625em;} legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;} progress{vertical-align:baseline;} textarea{overflow:auto;} [type="checkbox"],[type="radio"]{box-sizing:border-box;padding:0;} [type="number"]::-webkit-inner-spin-button,[type="number"]::-webkit-outer-spin-button{height:auto;} [type="search"]{-webkit-appearance:textfield;outline-offset:-2px;} [type="search"]::-webkit-search-decoration{-webkit-appearance:none;} ::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;} details{display:block;} summary{display:list-item;} template{display:none;} [hidden]{display:none;} html{font-size:19px;box-sizing:border-box;} *,*::before,*::after{box-sizing:inherit;} body{font-family:"Roboto",Helvetica,Arial,sans-serif;line-height:1.78rem;padding:0;background:#FFF;} h1{display:block;margin:0.67em 0;} .math-display{overflow-x:auto;} @media (min-width:74.6875em){.math-display{overflow-x:initial;}} .remark-highlight{font-size:14px;} @media (min-width:74.6875em){.remark-highlight{font-size:16px;}} p.hint.tip,p.hint.error,p.hint.warn{-webkit-letter-spacing:0;-moz-letter-spacing:0;-ms-letter-spacing:0;letter-spacing:0;box-sizing:border-box;font-size:inherit;line-height:1.6rem;word-spacing:0.05rem;background-color:rgba(238,238,238,0.5);border-bottom-right-radius:2px;border-top-right-radius:2px;padding:8px 12px 8px 24px;margin-bottom:16px;position:relative;} p.hint.tip:before,p.hint.error:before,p.hint.warn:before{border-radius:100%;color:#fff;content:'!';font-size:14px;font-weight:700;left:-12px;line-height:20px;position:absolute;height:20px;width:20px;text-align:center;top:12px;} p.hint.tip{border-left:4px solid #27ab83;} p.hint.tip:before{display:none;} p.hint.warn{border-left:4px solid #f0b429;} p.hint.warn:before{background-color:#f0b429;} p.hint.error{border-left:4px solid #ef4e4e;} p.hint.error:before{background-color:#ef4e4e;content:'×';} .content table{border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb;font-size:12px;line-height:1.0rem;} @media (min-width:74.6875em){.content table{font-size:19px;line-height:1.78rem;}} .content caption{color:#000;font:italic 85%/1 arial,sans-serif;padding:1em 0;text-align:center;} .content td,.content th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;font-size:inherit;margin:0;overflow:visible;padding:0.5em 1em;} .content thead{background-color:#e0e0e0;color:#000;text-align:left;vertical-align:bottom;} .content td{background-color:#f2f2f2;} .content tr:nth-child(2n-1) td{background-color:transparent;} .content td{border-bottom:1px solid #cbcbcb;} .content tbody > tr:last-child > td{border-bottom-width:0;} .content td,.content th{border-width:0 0 1px 0;border-bottom:1px solid #cbcbcb;} .content tbody > tr:last-child > td{border-bottom-width:0;}
/* sc-component-id: Navigation__Nav-qabwmo-0 */
.iRDKCt{width:100%;background:transparent;padding:0 1rem;font-family:"Roboto",Helvetica,Arial,sans-serif;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;} .iRDKCt a{padding:0.5rem 0 0.25rem 0;font-style:normal;font-weight:500;line-height:23px;font-size:14px;text-align:right;text-transform:uppercase;color:#333;} .iRDKCt a:last-child{padding:0.5rem;padding-right:0;} @media (min-width:74.6875em){.iRDKCt{padding:0 5rem;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}.iRDKCt a{padding:0.5rem 1rem 0.25rem 1rem;}}
/* sc-component-id: Featured__StyledButtonBack-xm4c49-5 */
.goldxz{position:absolute;top:50%;left:-5px;margin-left:10px;margin-top:-20px;width:40px;height:40px;padding:10px;background:none;border:none;outline:none;border-radius:40px;} @media (min-width:46.0625em){.goldxz{left:10px;width:70px;height:70px;padding:15px;}} @media (min-width:64.0625em){.goldxz{left:10px;width:70px;height:70px;padding:15px;}}
/* sc-component-id: Featured__StyledButtonNext-xm4c49-6 */
.gFasvY{position:absolute;top:50%;right:-5px;margin-right:10px;margin-top:-20px;width:40px;height:40px;padding:10px;background:none;border:none;outline:none;border-radius:40px;} @media (min-width:46.0625em){.gFasvY{right:10px;width:70px;height:70px;padding:15px;}} @media (min-width:64.0625em){.gFasvY{right:10px;width:70px;height:70px;padding:15px;}}
/* sc-component-id: Post__PageWithCoverImg-oyq0rs-0 */
@media (min-width:74.6875em){.cSOdgJ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}.cSOdgJ nav{padding:0;}} @media (min-width:46.0625em){.cSOdgJ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}}
/* sc-component-id: Post-oyq0rs-1 */
.fGODKX{margin:0 auto;padding:0 1rem;} @media (min-width:46.0625em){.fGODKX{max-width:62%;padding:0 2rem;}} @media (min-width:74.6875em){.fGODKX{max-width:62%;padding:0 5rem;}}
/* sc-component-id: Post__PostContent-oyq0rs-2 */
.jSdCWo{margin:0 auto;max-width:720px;} .jSdCWo > h1{font-family:"Exo 2",sans-serif;font-size:2.074rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:2.074rem;} .jSdCWo > h1 > span{display:block;font-size:1.20rem;line-height:2.074rem;} @media (min-width:74.6875em){.jSdCWo > h1{font-size:2.488rem;line-height:3.57895rem;}.jSdCWo > h1 > span{font-size:1.44rem;}} .jSdCWo > h2{font-family:"Exo 2",sans-serif;font-size:1.728rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:2.074rem;} @media (min-width:74.6875em){.jSdCWo > h2{font-size:2.074rem;line-height:3.57895rem;}} .jSdCWo > h3{font-family:"Exo 2",sans-serif;font-size:1.44rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:1.78947rem;} @media (min-width:74.6875em){.jSdCWo > h3{font-size:1.728rem;}} .jSdCWo > h4{font-family:"Exo 2",sans-serif;font-size:1.20rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:1.78947rem;} @media (min-width:74.6875em){.jSdCWo > h4{font-size:1.44rem;}} .jSdCWo > h5{font-family:"Exo 2",sans-serif;font-size:1.20rem;font-weight:600;margin-top:1.78947rem;margin-bottom:0;padding:0;line-height:1.78947rem;} .jSdCWo p{font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:1.00rem;font-weight:300;margin-top:0;margin-bottom:1.78947rem;padding:0;font-style:normal;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;color:#333;} .jSdCWo p > strong{font-weight:500;} .jSdCWo >table{font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:0.694rem;font-weight:300;margin-bottom:1.78947rem;} .jSdCWo >table > strong{font-weight:500;} @media (min-width:74.6875em){.jSdCWo div.wide-content+table{width:920px;margin-left:-100px;}} .jSdCWo a{-webkit-text-decoration:underline;text-decoration:underline;color:#333;} .jSdCWo a:hover{color:#e6212b;} .jSdCWo ul,.jSdCWo ol{margin-top:0;margin-bottom:1.78947rem;font-weight:300;line-height:1.78947rem;} .jSdCWo ul > li ul,.jSdCWo ol > li ul{margin-bottom:0;} .jSdCWo > img,.jSdCWo > p img{margin:0 auto;display:block;} .jSdCWo > blockquote{font-size:1.1rem;font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:italic;font-weight:300;line-height:1.8rem;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;text-align:right;margin-right:0;padding:24px;quotes:"\201E""\201C";} .jSdCWo > blockquote:before{display:inline-block;-webkit-transform:translate(-15px,-15px);-ms-transform:translate(-15px,-15px);transform:translate(-15px,-15px);content:open-quote;color:#edebeb;font-size:5rem;font-weight:400;} .jSdCWo > blockquote > footer{margin-top:10px;font-style:normal;font-weight:400;} .jSdCWo > details summary{cursor:pointer;}
/* sc-component-id: Post__PostHeader-oyq0rs-5 */
.kAktca{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:normal;font-weight:400;line-height:1.8rem;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;}
/* sc-component-id: Post__BackLink-oyq0rs-6 */
.gMsmUp{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;color:#828282;} .gMsmUp:hover{color:#e6212b;}
/* sc-component-id: Post__Date-oyq0rs-7 */
.kSsCZe{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;font-size:1rem;text-align:right;color:#828282;}
/* sc-component-id: Post__PostFooter-oyq0rs-8 */
.fJVWOL{display:inline-block;background:#edebeb;width:100%;}
/* sc-component-id: Post__PostFooterWraper-oyq0rs-9 */
.lhqqFe{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;max-width:720px;padding:0 1rem;margin:0 auto;} .lhqqFe > p{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;color:#828282;} @media (min-width:74.6875em){.lhqqFe{padding-left:0 0 0 100px;}}
/* sc-component-id: Post__CoverImg-oyq0rs-10 */
@media (min-width:46.0625em){.hAChiG{background:#fff url(/galleries/atari_agents_2022/collage.png) no-repeat bottom;background-size:cover;position:-webkit-sticky;position:sticky;top:0;left:0;height:100vh;width:38%;}} @media (min-width:74.6875em){.hAChiG{background:#fff url(/galleries/atari_agents_2022/collage.png) no-repeat bottom;background-size:cover;position:-webkit-sticky;position:sticky;top:0;left:0;height:100vh;width:38%;}}</style></head><body><div id="root"><div class="content" data-reactroot=""><div><div class="Post__PageWithCoverImg-oyq0rs-0 cSOdgJ"><div class="Post__CoverImg-oyq0rs-10 hAChiG"></div><div class="Post-oyq0rs-1 fGODKX"><header><nav class="Navigation__Nav-qabwmo-0 iRDKCt"><a href="https://bit-ml.github.io/">Home</a><a href="https://bit-ml.github.io/#research">Research</a><a href="https://bit-ml.github.io/#teams">Team</a><a href="https://bit-ml.github.io/teaching/lectures-and-courses">Teaching</a></nav></header><section class="Post__PostContent-oyq0rs-2 jSdCWo"><div class="Post__PostHeader-oyq0rs-5 kAktca"><a class="Post__BackLink-oyq0rs-6 gMsmUp active" aria-current="page" href="https://bit-ml.github.io/">&lt;<!-- --> Back Home</a><small class="Post__Date-oyq0rs-7 kSsCZe">published on <!-- -->May 05, 2022</small></div><h1>🕹️ Pretrained Atari Agents</h1>
<p>Releasing trained models in computer vision and natural language processing has been a major source of progress for the research in these fields and a significant catalyst for the adaption of deep learning models in the industry. By comparison, RL agents pretrained on otherwise resource and time intensive benchmarks such as Arcade Learning Environment are rather hard to come by.</p>
<p>Today, our research group within Bitdefender is making available over 2️⃣5️⃣,0️⃣0️⃣0️⃣ agents trained on 60 games in the Arcade Learning Environment. We hope the diversity and the quality of these trained models will help spur new research in multi-task and imitation learning and contribute to the state of reproducibility in deep reinforcement learning.</p>
<p>The performance of these agents closely matches figures published in the literature and have been used as strong baselines in published and unpublished work. The agents included in this release are Munchausen-DQN and Adam-optimised DQN that compare favourably with more complex agents as well as a C51 agent whose performance matches or exceeds the results reported in the paper that introduced it. We provide three independent training runs for each agent-game combination with the exception of on agent for which we only provide two seeds. We plan to continue releasing new models.</p>
<p>Another feature of this release is the extreme ease of experimenting with the agents. No RL frameworks are required, the dependency list is kept to a minimum, the code is self-contained and takes just two files, making it a breeze to quickly load a checkpoint, visualize the gameplay at high resolution and record it. The simplicity of the code also makes it possible to easily modify the scripts for your own purposes. Converting these models for use in other deep learning frameworks should also be possible.</p>
<p>We encourage you coming with suggestions of how to make this repository of trained models better and more useful. Relevant links:</p>
<ul>
<li><img src="https://bit-ml.github.io/galleries/atari_agents_2022/octocat.svg" alt="octocat"/>   <a href="https://github.com/floringogianu/atari-agents#trained-atari-agents">floringogianu/atari-agents</a></li>
<li>📂   <a href="https://share.bitdefender.com/s/qCF7jFxkgx2qJeT">download models</a></li>
</ul>
<h2>How we trained the agents</h2>
<p>A major reason for deciding to publish these models is the sheer amount of time required to run DQN-style algorithms on the Atari benchmark. This is especially difficult for small labs.</p>
<p>For this release we used &quot;only&quot; 20 to 40 GPUs (a wide assortment ranging from GTX Titan to newer RTX consumer models) from our cluster at Bitdefender, for a combined running time of about two months for learning all the agents. Considering  a full run on ALE requires 3 seeds <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em"></span><span class="mord">×</span></span></span></span></span> 60 games and a single DQN-style agent takes a bit over one week this might seem a bit surprising. What made this possible is that we figured out early that you could launch three to four DQN processes on a single GPU provided the replay buffer is stored in the system RAM. The penalty incurred in terms of wall clock times is easily offset by parallelization in this case.</p>
<p>This is one of the reason our agents have been trained using our own PyTorch implementations and while the code used is not readily available we consider publishing it if there is demand for it.</p>
<h3>A word on training and evaluation protocols</h3>
<p>There are two common training and evaluation protocols encountered in the literature. We named them <code>classic</code> and <code>modern</code> across this project:</p>
<ul>
<li><code>classic</code>: it originates from (Mnih, 2015)<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup> Nature paper and it mostly appears in DeepMind papers.</li>
<li><code>modern</code>: it originates from (Machado, 2017)<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup> and a variation of it was adopted by Dopamine<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>. Since then it started to show up more often in recent papers.</li>
</ul>
<p>The main two differences between the two are the way stochasticity is induced in the environment and how the loss of a life is treated.</p>
<p>The current crop of agents is summarized below.</p>
<div class="wide-content"></div>
<table><thead><tr><th align="left">Algorithm</th><th>Protocol</th><th align="center">Games</th><th align="center">Seeds</th><th align="left">Observations</th></tr></thead><tbody><tr><td align="left"><strong>DQN</strong></td><td><code>modern</code></td><td align="center">60</td><td align="center">3</td><td align="left">DQN agent using the settings from <a href="https://github.com/google/dopamine/blob/master/dopamine/jax/agents/dqn/configs/dqn.gin">dopamine</a>. It&#x27;s optimised with Adam and uses MSE instead of Huber loss. <strong>A surprisingly strong agent on this protocol</strong>.</td></tr><tr><td align="left"><strong>M-DQN</strong></td><td><code>modern</code></td><td align="center">60</td><td align="center">3</td><td align="left">DQN above but using the <strong>Munchausen trick</strong><sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>. Even stronger performance.</td></tr><tr><td align="left"><strong>C51</strong></td><td><code>classic</code></td><td align="center">28/57</td><td align="center">3</td><td align="left">Closely follows the original paper<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>.</td></tr><tr><td align="left"><strong>DQN Adam</strong></td><td><code>classic</code></td><td align="center">28/57</td><td align="center">2</td><td align="left">A DQN agent trained according to the Rainbow paper<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>. The exact settings and plots can be found in our paper<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>.</td></tr></tbody></table>
<p>Right off-the bat you can notice that on the <code>classic</code> protocol there are only 28 games out of the usual 57. We trained the two agents on this protocol over one year ago using the now deprecated <code>atari-py</code> project which officially provided the ALE Python bindings in OpenAI&#x27;s Gym. Unfortunately the package came with a large number of ROMs that are not supported by the current, official, <a href="https://github.com/mgbellemare/Arcade-Learning-Environment">ale-py</a> library. The agents trained on the <code>modern</code> protocol (as well as the code we provide for visualising agents) all use the new <code>ale-py</code>. Therefore we decided against providing support for the older library event if it meant dropping half of the trained models. A great resource for reading about this issue is Jesse&#x27;s Farebrother <a href="https://brosa.ca/blog/ale-release-v0.7/#rom-management">ALE v0.7 release notes</a>. Importantly, we found out about the issue while checking the performance of the trained models on the new <code>ale-py</code> back-end and we provide plots showing the remaining 28 agents perform as expected (<a href="https://github.com/floringogianu/atari-agents/blob/main/imgs/c51_g28_confirmation.png">C51_classic</a>, <a href="https://github.com/floringogianu/atari-agents/blob/main/imgs/dqn_g28_confirmation.png">DQN_classic</a>).</p>
<h2>How many checkpoints?</h2>
<p>An agent trained on 200M frames usually produces 200 checkpoints times the number of training seeds. In order not to make the download size overly large <strong>we only include 51 checkpoints per training run</strong>. These are sampled geometrically, with denser checkpoints towards the end of the training. This results in the last 20 checkpoints of the full 200 (last 10% of the training run) and then sparser checkpoints towards the beginning of the run, with only 10 out of 51 from the first half. It looks a bit like this:</p>
<p><img src="https://bit-ml.github.io/galleries/atari_agents_2022/sampling.png" alt="checkpoint sampling"/></p>
<p>Note it&#x27;s not mandatory the best performing checkpoint is included since on some combinations of algorithms and agents the peak performance occurs earlier in training. However this sampling should characterize fairly well the performance of an agent most of the time.</p>
<p>❗✋ If <a href="https://github.com/floringogianu/atari-agents/issues">requested</a>, we can provide the full list of checkpoints for a given agent.</p>
<p>Agents have been trained using PyTorch and the models are stored as compressed <a href="https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html">state_dict</a> pickle files. Since the networks used on ALE are fairly simple these could easily be converted for use in other deep learning frameworks.</p>
<h2>Just how well trained are these agents?</h2>
<p>Our PyTorch implementation of DQN trained using Adam on the <code>modern</code> protocol compares favourable to the exact same agent trained using Dopamine. The plots below have been generated using the tools provided by <a href="https://github.com/google-research/rliable">rliable</a>.</p>
<p><img src="https://bit-ml.github.io/galleries/atari_agents_2022/rliable_comparison.png" alt="dopamine_vs_pytorch"/></p>
<p>A detailed discussion about the performance of DQN + Adam and C51 trained on the <code>classic</code> protocol can be found in our paper<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>, where we used these checkpoints as baselines.</p>
<h2>References</h2>
<div class="footnotes">
<hr/>
<ol>
<li id="fn-2"><a href="https://www.nature.com/articles/nature14236">Mnih, et al. 2015, _Human-level control through deep reinforcement learning</a><a href="#fnref-2" class="footnote-backref">↩</a></li>
<li id="fn-1"><a href="https://arxiv.org/abs/1709.06009">Machado, et al. 2017, <em>Revisiting the Arcade Learning Environment...</em></a><a href="#fnref-1" class="footnote-backref">↩</a></li>
<li id="fn-6"><a href="http://arxiv.org/abs/1812.06110">Castro, et al. 2018, <em>Dopamine: A Research Framework for Deep RL</em></a><a href="#fnref-6" class="footnote-backref">↩</a></li>
<li id="fn-4"><a href="https://arxiv.org/abs/1710.02298">Hessel, et al. 2017, <em>Combining Improvements in Deep RL</em></a><a href="#fnref-4" class="footnote-backref">↩</a></li>
<li id="fn-5"><a href="https://www.semanticscholar.org/paper/Spectral-Normalisation-for-Deep-Reinforcement-an-Gogianu-Berariu/cf04c05f69022f71b60c7b7252af94f11cad5ef1">Gogianu, et al. 2021, <em>Spectral Normalisation...</em></a><a href="#fnref-5" class="footnote-backref">↩</a></li>
<li id="fn-3"><a href="http://proceedings.mlr.press/v70/bellemare17a.html">Bellemare, et al. 2017, <em>A distributional perspective...</em></a><a href="#fnref-3" class="footnote-backref">↩</a></li>
<li id="fn-7"><a href="https://arxiv.org/abs/2007.14430">Vieillard, et al. 2020, <em>Munchausen Reinforcement Learning</em></a><a href="#fnref-7" class="footnote-backref">↩</a></li>
</ol>
</div></section><div class="Post__PostFooter-oyq0rs-8 fJVWOL"><div class="Post__PostFooterWraper-oyq0rs-9 lhqqFe"><p class="Post__Author-oyq0rs-4 itWYlR">written by <!-- -->Florin Gogianu</p></div></div></div></div></div></div></div><script type="text/javascript">window.__CSS_CHUNKS__ = {"main":"https://bit-ml.github.io/styles.04581cb1.css"}</script><script type="text/javascript">
    window.__routeInfo = {"path":"blog/post/pretrained-atari-agents","templateID":2,"sharedPropsHashes":{"galleries":"2gHC7O"},"localProps":null,"allProps":{"post":{"data":{"slug":"pretrained-atari-agents","authors":"Florin Gogianu","categories":"blog, reinforcement-learning","featured_img":"/galleries/atari_agents_2022/collage.png","date":"May-05-2022","title":":joystick: Pretrained Atari Agents","id":1},"messages":[],"history":["./content/collections/posts/atari_agents_2022.md","content/collections/posts/atari_agents_2022.html"],"cwd":"/Users/dtantaru/bit-ml","contents":"<h1>🕹️ Pretrained Atari Agents</h1>\n<p>Releasing trained models in computer vision and natural language processing has been a major source of progress for the research in these fields and a significant catalyst for the adaption of deep learning models in the industry. By comparison, RL agents pretrained on otherwise resource and time intensive benchmarks such as Arcade Learning Environment are rather hard to come by.</p>\n<p>Today, our research group within Bitdefender is making available over 2️⃣5️⃣,0️⃣0️⃣0️⃣ agents trained on 60 games in the Arcade Learning Environment. We hope the diversity and the quality of these trained models will help spur new research in multi-task and imitation learning and contribute to the state of reproducibility in deep reinforcement learning.</p>\n<p>The performance of these agents closely matches figures published in the literature and have been used as strong baselines in published and unpublished work. The agents included in this release are Munchausen-DQN and Adam-optimised DQN that compare favourably with more complex agents as well as a C51 agent whose performance matches or exceeds the results reported in the paper that introduced it. We provide three independent training runs for each agent-game combination with the exception of on agent for which we only provide two seeds. We plan to continue releasing new models.</p>\n<p>Another feature of this release is the extreme ease of experimenting with the agents. No RL frameworks are required, the dependency list is kept to a minimum, the code is self-contained and takes just two files, making it a breeze to quickly load a checkpoint, visualize the gameplay at high resolution and record it. The simplicity of the code also makes it possible to easily modify the scripts for your own purposes. Converting these models for use in other deep learning frameworks should also be possible.</p>\n<p>We encourage you coming with suggestions of how to make this repository of trained models better and more useful. Relevant links:</p>\n<ul>\n<li><img src=\"/galleries/atari_agents_2022/octocat.svg\" alt=\"octocat\">   <a href=\"https://github.com/floringogianu/atari-agents#trained-atari-agents\">floringogianu/atari-agents</a></li>\n<li>📂   <a href=\"https://share.bitdefender.com/s/qCF7jFxkgx2qJeT\">download models</a></li>\n</ul>\n<h2>How we trained the agents</h2>\n<p>A major reason for deciding to publish these models is the sheer amount of time required to run DQN-style algorithms on the Atari benchmark. This is especially difficult for small labs.</p>\n<p>For this release we used \"only\" 20 to 40 GPUs (a wide assortment ranging from GTX Titan to newer RTX consumer models) from our cluster at Bitdefender, for a combined running time of about two months for learning all the agents. Considering  a full run on ALE requires 3 seeds <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>×</mo></mrow><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">×</span></span></span></span></span> 60 games and a single DQN-style agent takes a bit over one week this might seem a bit surprising. What made this possible is that we figured out early that you could launch three to four DQN processes on a single GPU provided the replay buffer is stored in the system RAM. The penalty incurred in terms of wall clock times is easily offset by parallelization in this case.</p>\n<p>This is one of the reason our agents have been trained using our own PyTorch implementations and while the code used is not readily available we consider publishing it if there is demand for it.</p>\n<h3>A word on training and evaluation protocols</h3>\n<p>There are two common training and evaluation protocols encountered in the literature. We named them <code>classic</code> and <code>modern</code> across this project:</p>\n<ul>\n<li><code>classic</code>: it originates from (Mnih, 2015)<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup> Nature paper and it mostly appears in DeepMind papers.</li>\n<li><code>modern</code>: it originates from (Machado, 2017)<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup> and a variation of it was adopted by Dopamine<sup id=\"fnref-6\"><a href=\"#fn-6\" class=\"footnote-ref\">6</a></sup>. Since then it started to show up more often in recent papers.</li>\n</ul>\n<p>The main two differences between the two are the way stochasticity is induced in the environment and how the loss of a life is treated.</p>\n<p>The current crop of agents is summarized below.</p>\n<div class=\"wide-content\"></div>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Algorithm</th>\n<th>Protocol</th>\n<th align=\"center\">Games</th>\n<th align=\"center\">Seeds</th>\n<th align=\"left\">Observations</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><strong>DQN</strong></td>\n<td><code>modern</code></td>\n<td align=\"center\">60</td>\n<td align=\"center\">3</td>\n<td align=\"left\">DQN agent using the settings from <a href=\"https://github.com/google/dopamine/blob/master/dopamine/jax/agents/dqn/configs/dqn.gin\">dopamine</a>. It's optimised with Adam and uses MSE instead of Huber loss. <strong>A surprisingly strong agent on this protocol</strong>.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>M-DQN</strong></td>\n<td><code>modern</code></td>\n<td align=\"center\">60</td>\n<td align=\"center\">3</td>\n<td align=\"left\">DQN above but using the <strong>Munchausen trick</strong><sup id=\"fnref-7\"><a href=\"#fn-7\" class=\"footnote-ref\">7</a></sup>. Even stronger performance.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>C51</strong></td>\n<td><code>classic</code></td>\n<td align=\"center\">28/57</td>\n<td align=\"center\">3</td>\n<td align=\"left\">Closely follows the original paper<sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup>.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>DQN Adam</strong></td>\n<td><code>classic</code></td>\n<td align=\"center\">28/57</td>\n<td align=\"center\">2</td>\n<td align=\"left\">A DQN agent trained according to the Rainbow paper<sup id=\"fnref-4\"><a href=\"#fn-4\" class=\"footnote-ref\">4</a></sup>. The exact settings and plots can be found in our paper<sup id=\"fnref-5\"><a href=\"#fn-5\" class=\"footnote-ref\">5</a></sup>.</td>\n</tr>\n</tbody>\n</table>\n<p>Right off-the bat you can notice that on the <code>classic</code> protocol there are only 28 games out of the usual 57. We trained the two agents on this protocol over one year ago using the now deprecated <code>atari-py</code> project which officially provided the ALE Python bindings in OpenAI's Gym. Unfortunately the package came with a large number of ROMs that are not supported by the current, official, <a href=\"https://github.com/mgbellemare/Arcade-Learning-Environment\">ale-py</a> library. The agents trained on the <code>modern</code> protocol (as well as the code we provide for visualising agents) all use the new <code>ale-py</code>. Therefore we decided against providing support for the older library event if it meant dropping half of the trained models. A great resource for reading about this issue is Jesse's Farebrother <a href=\"https://brosa.ca/blog/ale-release-v0.7/#rom-management\">ALE v0.7 release notes</a>. Importantly, we found out about the issue while checking the performance of the trained models on the new <code>ale-py</code> back-end and we provide plots showing the remaining 28 agents perform as expected (<a href=\"https://github.com/floringogianu/atari-agents/blob/main/imgs/c51_g28_confirmation.png\">C51_classic</a>, <a href=\"https://github.com/floringogianu/atari-agents/blob/main/imgs/dqn_g28_confirmation.png\">DQN_classic</a>).</p>\n<h2>How many checkpoints?</h2>\n<p>An agent trained on 200M frames usually produces 200 checkpoints times the number of training seeds. In order not to make the download size overly large <strong>we only include 51 checkpoints per training run</strong>. These are sampled geometrically, with denser checkpoints towards the end of the training. This results in the last 20 checkpoints of the full 200 (last 10% of the training run) and then sparser checkpoints towards the beginning of the run, with only 10 out of 51 from the first half. It looks a bit like this:</p>\n<p><img src=\"/galleries/atari_agents_2022/sampling.png\" alt=\"checkpoint sampling\"></p>\n<p>Note it's not mandatory the best performing checkpoint is included since on some combinations of algorithms and agents the peak performance occurs earlier in training. However this sampling should characterize fairly well the performance of an agent most of the time.</p>\n<p>❗✋ If <a href=\"https://github.com/floringogianu/atari-agents/issues\">requested</a>, we can provide the full list of checkpoints for a given agent.</p>\n<p>Agents have been trained using PyTorch and the models are stored as compressed <a href=\"https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\">state_dict</a> pickle files. Since the networks used on ALE are fairly simple these could easily be converted for use in other deep learning frameworks.</p>\n<h2>Just how well trained are these agents?</h2>\n<p>Our PyTorch implementation of DQN trained using Adam on the <code>modern</code> protocol compares favourable to the exact same agent trained using Dopamine. The plots below have been generated using the tools provided by <a href=\"https://github.com/google-research/rliable\">rliable</a>.</p>\n<p><img src=\"/galleries/atari_agents_2022/rliable_comparison.png\" alt=\"dopamine_vs_pytorch\"></p>\n<p>A detailed discussion about the performance of DQN + Adam and C51 trained on the <code>classic</code> protocol can be found in our paper<sup id=\"fnref-5\"><a href=\"#fn-5\" class=\"footnote-ref\">5</a></sup>, where we used these checkpoints as baselines.</p>\n<h2>References</h2>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-2\"><a href=\"https://www.nature.com/articles/nature14236\">Mnih, et al. 2015, _Human-level control through deep reinforcement learning</a><a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-1\"><a href=\"https://arxiv.org/abs/1709.06009\">Machado, et al. 2017, <em>Revisiting the Arcade Learning Environment...</em></a><a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-6\"><a href=\"http://arxiv.org/abs/1812.06110\">Castro, et al. 2018, <em>Dopamine: A Research Framework for Deep RL</em></a><a href=\"#fnref-6\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-4\"><a href=\"https://arxiv.org/abs/1710.02298\">Hessel, et al. 2017, <em>Combining Improvements in Deep RL</em></a><a href=\"#fnref-4\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-5\"><a href=\"https://www.semanticscholar.org/paper/Spectral-Normalisation-for-Deep-Reinforcement-an-Gogianu-Berariu/cf04c05f69022f71b60c7b7252af94f11cad5ef1\">Gogianu, et al. 2021, <em>Spectral Normalisation...</em></a><a href=\"#fnref-5\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-3\"><a href=\"http://proceedings.mlr.press/v70/bellemare17a.html\">Bellemare, et al. 2017, <em>A distributional perspective...</em></a><a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-7\"><a href=\"https://arxiv.org/abs/2007.14430\">Vieillard, et al. 2020, <em>Munchausen Reinforcement Learning</em></a><a href=\"#fnref-7\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>\n"},"galleries":{"asm_llm2024":[{"src":"/galleries/asm_llm2024/asmllm_code_repair.png","width":2570,"height":370},{"src":"/galleries/asm_llm2024/asmllm_downstream.png","width":1255,"height":769},{"src":"/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg","width":5760,"height":3840}],"atari_agents_2022":[{"src":"/galleries/atari_agents_2022/collage.png","width":960,"height":1300},{"src":"/galleries/atari_agents_2022/dqn_modern_pytorch_vs_dopamine.png","width":1022,"height":367},{"src":"/galleries/atari_agents_2022/octocat.svg","width":16,"height":16},{"src":"/galleries/atari_agents_2022/rliable_comparison.png","width":875,"height":763},{"src":"/galleries/atari_agents_2022/sampling.png","width":1348,"height":164}],"bgv_scheme2023":[{"src":"/galleries/bgv_scheme2023/fhe_add.drawio.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_add_tiny.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_mul.drawio.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_mul_tiny.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_noise.png","width":390,"height":412},{"src":"/galleries/bgv_scheme2023/fhe_noise_tiny.png","width":390,"height":412},{"src":"/galleries/bgv_scheme2023/fhe_simple.drawio.png","width":538,"height":161},{"src":"/galleries/bgv_scheme2023/fhe_simple_tiny.png","width":538,"height":161},{"src":"/galleries/bgv_scheme2023/lock.jpg","width":1200,"height":630},{"src":"/galleries/bgv_scheme2023/poly.drawio.png","width":342,"height":346},{"src":"/galleries/bgv_scheme2023/poly_tiny.png","width":342,"height":346},{"src":"/galleries/bgv_scheme2023/sec_game.drawio.png","width":350,"height":244},{"src":"/galleries/bgv_scheme2023/sec_game_tiny.png","width":350,"height":244}],"courses":[{"src":"/galleries/courses/atari_collage.png","width":960,"height":1300},{"src":"/galleries/courses/campus.jpg","width":725,"height":1080},{"src":"/galleries/courses/overview_bitdefender.jpg","width":1959,"height":2813},{"src":"/galleries/courses/overview_crypto.jpg","width":1920,"height":960},{"src":"/galleries/courses/overview_fmi.jpg","width":768,"height":600},{"src":"/galleries/courses/overview_pm.png","width":1194,"height":1190},{"src":"/galleries/courses/thumb_bitdefender.png","width":128,"height":128},{"src":"/galleries/courses/thumb_precis.jpg","width":128,"height":128},{"src":"/galleries/courses/thumb_rosedu.png","width":360,"height":360},{"src":"/galleries/courses/thumb_unibuc.png","width":512,"height":512}],"deep_bit_2019_pictures":[{"src":"/galleries/deep_bit_2019_pictures/20190731_150323.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_150331.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_153111.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_153118.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_164741.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_164747.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142509.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142514.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142521.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142523_1.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142925.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_144034.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145138.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145146.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145644.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145647.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145650.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145654.jpg","width":4160,"height":3120}],"deep_bit_2019_posters":[{"src":"/galleries/deep_bit_2019_posters/poster_alin_passwords.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_bogdan_train_with_private_data.webp","width":1920,"height":1440},{"src":"/galleries/deep_bit_2019_posters/poster_cristi_stegano.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_elena+adi_adversarial.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_horia+alin_homoglyph.webp","width":1920,"height":1486},{"src":"/galleries/deep_bit_2019_posters/poster_mada_crypto_nets.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_mircescu_MRI.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_novac_logs_anomalies.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_tudor_fake_news.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_vlad_mouse_auth.webp","width":1920,"height":2715}],"deep_fmi_2019_posters":[{"src":"/galleries/deep_fmi_2019_posters/10. EmoContext_Poster.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/10. Image_Segmentation.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/10. Landmark Recognition _ Visualization.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/10. Political Orientation Classification.webp","width":1920,"height":1440},{"src":"/galleries/deep_fmi_2019_posters/10. Poster_VDSR.webp","width":1920,"height":2714},{"src":"/galleries/deep_fmi_2019_posters/2048_RL.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/A LEARNED REPRESENTATION FOR ARTISTIC STYLE.webp","width":1275,"height":1650},{"src":"/galleries/deep_fmi_2019_posters/Emotion_Detection.webp","width":1920,"height":2688},{"src":"/galleries/deep_fmi_2019_posters/Emotion_Detection_CNN.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/Google Landmark Recognition Challenge.webp","width":1755,"height":2480},{"src":"/galleries/deep_fmi_2019_posters/Image Classification.webp","width":1275,"height":1650},{"src":"/galleries/deep_fmi_2019_posters/Image Semantic Segmentation.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/Image_Colorization.webp","width":1920,"height":2717},{"src":"/galleries/deep_fmi_2019_posters/PosterRL.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/Revising Bi-Axial LSTM for Music Generation.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/Semantic Segmantation.webp","width":1920,"height":1920},{"src":"/galleries/deep_fmi_2019_posters/Style Transfer.webp","width":1920,"height":2555},{"src":"/galleries/deep_fmi_2019_posters/poster.webp","width":1275,"height":1650}],"eeml2019_pictures":[{"src":"/galleries/eeml2019_pictures/andrei.jpg","width":1590,"height":1060},{"src":"/galleries/eeml2019_pictures/bitdefender_booth.jpg","width":1710,"height":1140},{"src":"/galleries/eeml2019_pictures/ema.jpg","width":1394,"height":930},{"src":"/galleries/eeml2019_pictures/florin.jpg","width":1152,"height":2048},{"src":"/galleries/eeml2019_pictures/florin_brad.jpg","width":928,"height":1392},{"src":"/galleries/eeml2019_pictures/full_house.jpg","width":1392,"height":928},{"src":"/galleries/eeml2019_pictures/group.jpg","width":1939,"height":1293},{"src":"/galleries/eeml2019_pictures/volunteers.jpg","width":1392,"height":928},{"src":"/galleries/eeml2019_pictures/z_ema_poster.png","width":1449,"height":2048},{"src":"/galleries/eeml2019_pictures/z_florin_poster.png","width":1698,"height":2400},{"src":"/galleries/eeml2019_pictures/z_iulia_andrei_poster.png","width":1772,"height":2496}],"eeml2019_posters":[{"src":"/galleries/eeml2019_posters/z_ema_poster.png","width":1449,"height":2048},{"src":"/galleries/eeml2019_posters/z_florin_poster.png","width":1698,"height":2400},{"src":"/galleries/eeml2019_posters/z_iulia_andrei_poster.png","width":1772,"height":2496}],"fused_swiglu2025":[{"src":"/galleries/fused_swiglu2025/A.png","width":1940,"height":1454},{"src":"/galleries/fused_swiglu2025/B.png","width":1982,"height":1242},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 405B kernels.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 405B.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 70B kernels.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 70B.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 8B kernels.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 8B.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Memory Usage for Llama 405B.svg","width":404,"height":325},{"src":"/galleries/fused_swiglu2025/Memory Usage for Llama 70B.svg","width":398,"height":325},{"src":"/galleries/fused_swiglu2025/Memory Usage for Llama 8B.svg","width":408,"height":325},{"src":"/galleries/fused_swiglu2025/chunk.png","width":1999,"height":1118},{"src":"/galleries/fused_swiglu2025/draft3.drawio.svg","width":591,"height":206},{"src":"/galleries/fused_swiglu2025/gpu.jpg","width":1024,"height":768},{"src":"/galleries/fused_swiglu2025/gpu2.jpeg","width":2048,"height":2048},{"src":"/galleries/fused_swiglu2025/kernel_look.png","width":2086,"height":1408},{"src":"/galleries/fused_swiglu2025/kernel_warp_diagram.svg","width":182761,"height":145554},{"src":"/galleries/fused_swiglu2025/savings.svg","width":1288,"height":423}],"homomorphic2020":[{"src":"/galleries/homomorphic2020/alice.png","width":1000,"height":339},{"src":"/galleries/homomorphic2020/arithmetic_circuit.png","width":408,"height":382},{"src":"/galleries/homomorphic2020/dec_details.png","width":676,"height":312},{"src":"/galleries/homomorphic2020/enigma_dec.jpg","width":800,"height":1200},{"src":"/galleries/homomorphic2020/eval_mul.png","width":569,"height":460},{"src":"/galleries/homomorphic2020/eval_sum.png","width":567,"height":441},{"src":"/galleries/homomorphic2020/he_scheme5.png","width":1354,"height":390},{"src":"/galleries/homomorphic2020/noisy_cyphertext.png","width":658,"height":449}],"private_set2021":[{"src":"/galleries/private_set2021/monolith.jpg","width":800,"height":1200}],"rstg_pictures":[{"src":"/galleries/rstg_pictures/RSTG_model.png","width":2347,"height":1031},{"src":"/galleries/rstg_pictures/chart.svg","width":600,"height":371},{"src":"/galleries/rstg_pictures/digits.gif","width":692,"height":337},{"src":"/galleries/rstg_pictures/featured_graph_nets.jpg","width":1455,"height":1200},{"src":"/galleries/rstg_pictures/featured_graph_nets_bk.jpg","width":2784,"height":1856},{"src":"/galleries/rstg_pictures/interaction_videos.gif","width":1079,"height":258},{"src":"/galleries/rstg_pictures/rstg.gif","width":1353,"height":517},{"src":"/galleries/rstg_pictures/rstg_stages.gif","width":600,"height":565},{"src":"/galleries/rstg_pictures/smt-smt2.gif","width":725,"height":251}],"tmlss2018_pictures":[{"src":"/galleries/tmlss2018_pictures/dana.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/elena.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/florin.jpg","width":810,"height":1080},{"src":"/galleries/tmlss2018_pictures/iulia.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/lecture_hall.jpg","width":1920,"height":992},{"src":"/galleries/tmlss2018_pictures/salina.jpg","width":1920,"height":1280},{"src":"/galleries/tmlss2018_pictures/salina_1.jpg","width":1920,"height":1280},{"src":"/galleries/tmlss2018_pictures/stefan.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/stefan_1.jpg","width":1599,"height":899}],"tmlss2018_posters":[{"src":"/galleries/tmlss2018_posters/Bitdefender+NLP2SQL_1440.jpg","width":1440,"height":3558},{"src":"/galleries/tmlss2018_posters/ElenaBurceanu_Tracking_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/EmanuelaHaller_Segmentation_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/IuliaDuta_AndreiNicolicioiu_Video2NLP_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/StefanPostavaru_RMSProp_1440.jpg","width":1440,"height":2036}]}},"siteData":{"title":"Bitdefender Machine Learning & Crypto Research Unit","description":"Bitdefender Machine Learning & Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&ML scene by supporting and participating in local conferences, lecture and research groups.","tagline":"Engaging with the broader Machine Learning Community.","tags":["machine-learning","research","bitdefender"]}};</script><script defer="" type="text/javascript" src="https://bit-ml.github.io/bootstrap.f160fe0b.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/templates/src/pages/Post.5cee3c5c.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/main.95926ac4.js"></script></body></html>