<!DOCTYPE html><html lang="en"><head><title data-react-helmet="true">Recurrent Space-time Graph Neural Network | Bitdefender Research</title><meta data-react-helmet="true" name="description" content="Recurrent Space-time Graph Neural NetworkWe introduce in this post our Recurrent Space-time Graph Neural Network (RSTG) architecture designed for learning video representation and  especially suited for tasks heavily relying on interactions.Let’s begin by considering the key components of video understanding that our method should include. Being..."/><meta data-react-helmet="true" name="keywords" content="blog"/><meta data-react-helmet="true" property="og:title" content="Recurrent Space-time Graph Neural Network | Bitdefender Research"/><meta data-react-helmet="true" property="og:description" content="Recurrent Space-time Graph Neural NetworkWe introduce in this post our Recurrent Space-time Graph Neural Network (RSTG) architecture designed for learning video representation and  especially suited for tasks heavily relying on interactions.Let’s begin by considering the key components of video understanding that our method should include. Being..."/><meta data-react-helmet="true" property="og:site_name" content="https://bit-ml.github.io"/><meta data-react-helmet="true" property="article:tag" content="blog"/><meta data-react-helmet="true" property="og:image" content="https://bit-ml.github.io/galleries/rstg_pictures/featured_graph_nets.jpg"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:site" content="@Bitdefender"/><meta data-react-helmet="true" name="twitter:title" content="Recurrent Space-time Graph Neural Network | Bitdefender Research"/><meta data-react-helmet="true" name="twitter:description" content="Recurrent Space-time Graph Neural NetworkWe introduce in this post our Recurrent Space-time Graph Neural Network (RSTG) architecture designed for learning video representation and  especially suited for tasks heavily relying on interactions.Let’s begin by considering the key components of video understanding that our method should include. Being..."/><meta data-react-helmet="true" name="twitter:image" content="https://bit-ml.github.io/galleries/rstg_pictures/featured_graph_nets.jpg"/><meta data-react-helmet="true" itemProp="description" content="Recurrent Space-time Graph Neural NetworkWe introduce in this post our Recurrent Space-time Graph Neural Network (RSTG) architecture designed for learning video representation and  especially suited for tasks heavily relying on interactions.Let’s begin by considering the key components of video understanding that our method should include. Being..."/><meta data-react-helmet="true" itemProp="keywords" content="blog"/><meta data-react-helmet="true" itemProp="image" content="https://bit-ml.github.io/galleries/rstg_pictures/featured_graph_nets.jpg"/><link rel="preload" as="script" href="https://bit-ml.github.io/bootstrap.f08b3b1c.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/templates/src/pages/Post.5cee3c5c.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/main.95926ac4.js"/><link rel="preload" as="style" href="https://bit-ml.github.io/styles.961ce4e9.css"/><link rel="stylesheet" href="https://bit-ml.github.io/styles.961ce4e9.css"/><meta charSet="UTF-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><link rel="apple-touch-icon" href="icon.png"/><style data-styled-components="goldxz gFasvY cSOdgJ jXIvvh fGODKX iRDKCt jSdCWo kAktca gMsmUp kSsCZe fJVWOL lhqqFe itWYlR">
/* sc-component-id: sc-global-16140688 */
html{line-height:1.15;-webkit-text-size-adjust:100%;} body{margin:0;} h1{font-size:2em;margin:0.67em 0;} hr{box-sizing:content-box;height:0;overflow:visible;} pre{font-family:monospace,monospace;font-size:1em;} a{background-color:transparent;-webkit-text-decoration:none;text-decoration:none;} abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;} b,strong{font-weight:bolder;} code,kbd,samp{font-family:monospace,monospace;font-size:1em;} small{font-size:80%;} sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;} sub{bottom:-0.25em;} sup{top:-0.5em;} figure{margin:0;} img{border-style:none;max-width:100%;} button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0;} button,input{overflow:visible;} button,select{text-transform:none;} button,[type="button"],[type="reset"],[type="submit"]{-webkit-appearance:button;} button::-moz-focus-inner,[type="button"]::-moz-focus-inner,[type="reset"]::-moz-focus-inner,[type="submit"]::-moz-focus-inner{border-style:none;padding:0;} button:-moz-focusring,[type="button"]:-moz-focusring,[type="reset"]:-moz-focusring,[type="submit"]:-moz-focusring{outline:1px dotted ButtonText;} fieldset{padding:0.35em 0.75em 0.625em;} legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;} progress{vertical-align:baseline;} textarea{overflow:auto;} [type="checkbox"],[type="radio"]{box-sizing:border-box;padding:0;} [type="number"]::-webkit-inner-spin-button,[type="number"]::-webkit-outer-spin-button{height:auto;} [type="search"]{-webkit-appearance:textfield;outline-offset:-2px;} [type="search"]::-webkit-search-decoration{-webkit-appearance:none;} ::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;} details{display:block;} summary{display:list-item;} template{display:none;} [hidden]{display:none;} html{font-size:19px;box-sizing:border-box;} *,*::before,*::after{box-sizing:inherit;} body{font-family:"Roboto",Helvetica,Arial,sans-serif;line-height:1.78rem;padding:0;background:#FFF;} h1{display:block;margin:0.67em 0;} .math-display{overflow-x:auto;} @media (min-width:74.6875em){.math-display{overflow-x:initial;}} .remark-highlight{font-size:14px;} @media (min-width:74.6875em){.remark-highlight{font-size:16px;}} p.hint.tip,p.hint.error,p.hint.warn{-webkit-letter-spacing:0;-moz-letter-spacing:0;-ms-letter-spacing:0;letter-spacing:0;box-sizing:border-box;font-size:inherit;line-height:1.6rem;word-spacing:0.05rem;background-color:rgba(238,238,238,0.5);border-bottom-right-radius:2px;border-top-right-radius:2px;padding:8px 12px 8px 24px;margin-bottom:16px;position:relative;} p.hint.tip:before,p.hint.error:before,p.hint.warn:before{border-radius:100%;color:#fff;content:'!';font-size:14px;font-weight:700;left:-12px;line-height:20px;position:absolute;height:20px;width:20px;text-align:center;top:12px;} p.hint.tip{border-left:4px solid #27ab83;} p.hint.tip:before{display:none;} p.hint.warn{border-left:4px solid #f0b429;} p.hint.warn:before{background-color:#f0b429;} p.hint.error{border-left:4px solid #ef4e4e;} p.hint.error:before{background-color:#ef4e4e;content:'×';} .content table{border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb;font-size:12px;line-height:1.0rem;} @media (min-width:74.6875em){.content table{font-size:19px;line-height:1.78rem;}} .content caption{color:#000;font:italic 85%/1 arial,sans-serif;padding:1em 0;text-align:center;} .content td,.content th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;font-size:inherit;margin:0;overflow:visible;padding:0.5em 1em;} .content thead{background-color:#e0e0e0;color:#000;text-align:left;vertical-align:bottom;} .content td{background-color:#f2f2f2;} .content tr:nth-child(2n-1) td{background-color:transparent;} .content td{border-bottom:1px solid #cbcbcb;} .content tbody > tr:last-child > td{border-bottom-width:0;} .content td,.content th{border-width:0 0 1px 0;border-bottom:1px solid #cbcbcb;} .content tbody > tr:last-child > td{border-bottom-width:0;}
/* sc-component-id: Navigation__Nav-qabwmo-0 */
.iRDKCt{width:100%;background:transparent;padding:0 1rem;font-family:"Roboto",Helvetica,Arial,sans-serif;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;} .iRDKCt a{padding:0.5rem 0 0.25rem 0;font-style:normal;font-weight:500;line-height:23px;font-size:14px;text-align:right;text-transform:uppercase;color:#333;} .iRDKCt a:last-child{padding:0.5rem;padding-right:0;} @media (min-width:74.6875em){.iRDKCt{padding:0 5rem;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}.iRDKCt a{padding:0.5rem 1rem 0.25rem 1rem;}}
/* sc-component-id: Featured__StyledButtonBack-xm4c49-5 */
.goldxz{position:absolute;top:50%;left:-5px;margin-left:10px;margin-top:-20px;width:40px;height:40px;padding:10px;background:none;border:none;outline:none;border-radius:40px;} @media (min-width:46.0625em){.goldxz{left:10px;width:70px;height:70px;padding:15px;}} @media (min-width:64.0625em){.goldxz{left:10px;width:70px;height:70px;padding:15px;}}
/* sc-component-id: Featured__StyledButtonNext-xm4c49-6 */
.gFasvY{position:absolute;top:50%;right:-5px;margin-right:10px;margin-top:-20px;width:40px;height:40px;padding:10px;background:none;border:none;outline:none;border-radius:40px;} @media (min-width:46.0625em){.gFasvY{right:10px;width:70px;height:70px;padding:15px;}} @media (min-width:64.0625em){.gFasvY{right:10px;width:70px;height:70px;padding:15px;}}
/* sc-component-id: Post__PageWithCoverImg-oyq0rs-0 */
@media (min-width:74.6875em){.cSOdgJ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}.cSOdgJ nav{padding:0;}} @media (min-width:46.0625em){.cSOdgJ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}}
/* sc-component-id: Post-oyq0rs-1 */
.fGODKX{margin:0 auto;padding:0 1rem;} @media (min-width:46.0625em){.fGODKX{max-width:62%;padding:0 2rem;}} @media (min-width:74.6875em){.fGODKX{max-width:62%;padding:0 5rem;}}
/* sc-component-id: Post__PostContent-oyq0rs-2 */
.jSdCWo{margin:0 auto;max-width:720px;} .jSdCWo > h1{font-family:"Exo 2",sans-serif;font-size:2.074rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:2.074rem;} .jSdCWo > h1 > span{display:block;font-size:1.20rem;line-height:2.074rem;} @media (min-width:74.6875em){.jSdCWo > h1{font-size:2.488rem;line-height:3.57895rem;}.jSdCWo > h1 > span{font-size:1.44rem;}} .jSdCWo > h2{font-family:"Exo 2",sans-serif;font-size:1.728rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:2.074rem;} @media (min-width:74.6875em){.jSdCWo > h2{font-size:2.074rem;line-height:3.57895rem;}} .jSdCWo > h3{font-family:"Exo 2",sans-serif;font-size:1.44rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:1.78947rem;} @media (min-width:74.6875em){.jSdCWo > h3{font-size:1.728rem;}} .jSdCWo > h4{font-family:"Exo 2",sans-serif;font-size:1.20rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:1.78947rem;} @media (min-width:74.6875em){.jSdCWo > h4{font-size:1.44rem;}} .jSdCWo > h5{font-family:"Exo 2",sans-serif;font-size:1.20rem;font-weight:600;margin-top:1.78947rem;margin-bottom:0;padding:0;line-height:1.78947rem;} .jSdCWo p{font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:1.00rem;font-weight:300;margin-top:0;margin-bottom:1.78947rem;padding:0;font-style:normal;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;color:#333;} .jSdCWo p > strong{font-weight:500;} .jSdCWo >table{font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:0.694rem;font-weight:300;margin-bottom:1.78947rem;} .jSdCWo >table > strong{font-weight:500;} @media (min-width:74.6875em){.jSdCWo div.wide-content+table{width:920px;margin-left:-100px;}} .jSdCWo a{-webkit-text-decoration:underline;text-decoration:underline;color:#333;} .jSdCWo a:hover{color:#e6212b;} .jSdCWo ul,.jSdCWo ol{margin-top:0;margin-bottom:1.78947rem;font-weight:300;line-height:1.78947rem;} .jSdCWo ul > li ul,.jSdCWo ol > li ul{margin-bottom:0;} .jSdCWo > img,.jSdCWo > p img{margin:0 auto;display:block;} .jSdCWo > blockquote{font-size:1.1rem;font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:italic;font-weight:300;line-height:1.8rem;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;text-align:right;margin-right:0;padding:24px;quotes:"\201E""\201C";} .jSdCWo > blockquote:before{display:inline-block;-webkit-transform:translate(-15px,-15px);-ms-transform:translate(-15px,-15px);transform:translate(-15px,-15px);content:open-quote;color:#edebeb;font-size:5rem;font-weight:400;} .jSdCWo > blockquote > footer{margin-top:10px;font-style:normal;font-weight:400;} .jSdCWo > details summary{cursor:pointer;}
/* sc-component-id: Post__PostHeader-oyq0rs-5 */
.kAktca{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:normal;font-weight:400;line-height:1.8rem;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;}
/* sc-component-id: Post__BackLink-oyq0rs-6 */
.gMsmUp{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;color:#828282;} .gMsmUp:hover{color:#e6212b;}
/* sc-component-id: Post__Date-oyq0rs-7 */
.kSsCZe{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;font-size:1rem;text-align:right;color:#828282;}
/* sc-component-id: Post__PostFooter-oyq0rs-8 */
.fJVWOL{display:inline-block;background:#edebeb;width:100%;}
/* sc-component-id: Post__PostFooterWraper-oyq0rs-9 */
.lhqqFe{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;max-width:720px;padding:0 1rem;margin:0 auto;} .lhqqFe > p{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;color:#828282;} @media (min-width:74.6875em){.lhqqFe{padding-left:0 0 0 100px;}}
/* sc-component-id: Post__CoverImg-oyq0rs-10 */
@media (min-width:46.0625em){.jXIvvh{background:#fff url(/galleries/rstg_pictures/featured_graph_nets.jpg) no-repeat bottom;background-size:cover;position:-webkit-sticky;position:sticky;top:0;left:0;height:100vh;width:38%;}} @media (min-width:74.6875em){.jXIvvh{background:#fff url(/galleries/rstg_pictures/featured_graph_nets.jpg) no-repeat bottom;background-size:cover;position:-webkit-sticky;position:sticky;top:0;left:0;height:100vh;width:38%;}}</style></head><body><div id="root"><div class="content" data-reactroot=""><div><div class="Post__PageWithCoverImg-oyq0rs-0 cSOdgJ"><div class="Post__CoverImg-oyq0rs-10 jXIvvh"></div><div class="Post-oyq0rs-1 fGODKX"><header><nav class="Navigation__Nav-qabwmo-0 iRDKCt"><a href="https://bit-ml.github.io/">Home</a><a href="https://bit-ml.github.io/#research">Research</a><a href="https://bit-ml.github.io/#teams">Team</a><a href="https://bit-ml.github.io/teaching/lectures-and-courses">Teaching</a></nav></header><section class="Post__PostContent-oyq0rs-2 jSdCWo"><div class="Post__PostHeader-oyq0rs-5 kAktca"><a class="Post__BackLink-oyq0rs-6 gMsmUp active" aria-current="page" href="https://bit-ml.github.io/">&lt;<!-- --> Back Home</a><small class="Post__Date-oyq0rs-7 kSsCZe">published on <!-- -->November 26, 2019</small></div><h1>Recurrent Space-time Graph Neural Network</h1>
<!-- -->
<p>We introduce in this post our <strong>Recurrent Space-time Graph Neural Network</strong> (RSTG) architecture designed for learning video representation and  especially suited for tasks heavily relying on interactions.</p>
<p>Let’s begin by considering the key components of video understanding that our method should include. Being able to detect and localise objects is the first crucial thing that we need to do, then we should combine them in various ways to create complex scenes. <em>Whole is greater than the sum of its parts</em>, but is it always true, and what are the conditions of this happening?</p>
<p>In order to form a greater whole, objects must have some kind of connections between them. A set of random objects that do not correlate in any way doesn’t bring much additional information.  Entities could form different types of connections, they could be semantically related or they could be related by their physical position in space and in time. These kinds of relations happen in both image and video, with the time dimension of the video adding more difficulty, greatly increasing the amount and complexity of interactions happening in the scene.</p>
<p>Let’s define the kind of interaction happening in the video by examining a few examples.</p>
<!-- -->
<p><img src="https://bit-ml.github.io/galleries/rstg_pictures/interaction_videos.gif" alt="eeml2019_group_photo" title="Temporal interaction."/></p>
<p>Above, the yellow car and the grass appear together during the whole video, but the event <em>the car crosses the line</em> is characterized by a specific interaction. We call those types of interactions happening at the frame level <strong>spatial</strong> interactions. On the other hand, the event <em>the yellow car overtakes the red car</em> can not be captured from a single frame as it only makes sense across time. We call this <strong>temporal</strong> interactions.</p>
<p>The entities that interact shouldn’t necessary be close to each other, either in time or in space, since there could also exist <strong>long-range</strong> interactions. In the above picture, the man and the moon are connected regardless of their distance in the image.</p>
<h2>Analysing videos with spatio-temporal graph models</h2>
<p>Models commonly used in Computer Vision, based on convolutional networks, implicitly capture interactions in both space and time dimension, but they are biased towards local, short-range relations.</p>
<p>We propose a method designed to explicitly model relations and that is capable of capturing long range connections. Our model fits into the broader category of <strong>graph neural networks</strong> (GNNs). We process the video imposing a graph structure in order to explicitly model interactions between different entities. GNNs usually send  messages between each pair of  nodes, thus easily modeling binary interactions. Higher order interactions could be achieved by multiple such message-passing iterations.</p>
<p>The animation below illustrates the main components of our model.</p>
<!-- -->
<img src="https://bit-ml.github.io/galleries/rstg_pictures/rstg.gif" alt="RSTG architecture." title="RSTG architecture." maxWidth="130%" width="3584"/>
<p>At each time step, we <strong>create nodes</strong> by extracting information from features given by a convolutional network. Each node corresponds to a different fixed region of the input and we connect them if they come from neighbouring regions or if they overlap, as shown in the figure above. Using multiple scales helps us to capture entities of different sizes  and also to connect distant regions more easily.</p>
<p>For the two types of interactions described above, we create two separate processing components. For each time step, we design a <strong>space processing</strong> stage to model the spatial interaction by iteratively  message-passing. This involves 3 steps: sending messages between each pair of connecting nodes, aggregating the messages received at each node by an attention mechanism and updating each node based on the current state and the aggregated message. We design a <strong>time processing</strong> stage to model temporal interactions by sending messages in time, only between the states of the same node, in a recurrent fashion. More specifically, at each time step, each node receives information only from its corresponding state from the previous time step.</p>
<p>To model more expressive spatio-temporal interactions and to give it the ability to reason about all the information in the scene, with knowledge about past states, we alternate the two processing stages, as shown in the animation below.</p>
<p><img src="https://bit-ml.github.io/galleries/rstg_pictures/rstg_stages.gif" alt="rstg_stages" title="Alternative Space and Time Processing Stages."/></p>
<p>By having multiple space iteration, we go from local to more global processing, modeling interactions happening between an increasing number of entities situated at increasingly longer distances. We want to have some temporal information at every such steps, in order to model interactions that takes into account the history.  In order to combine the same kind of features from different time steps, we only connect the k-th spatial stage with the k-th stage from the previous step as shown in the animation.</p>
<p>We can pool all the nodes into a vector representation used for the final prediction or we could project back each node into its initial corresponding region, forming a feature volume with the same size as the graph input so that our model could be used as a module inside any other architecture.</p>
<!-- -->
<!-- -->
<!-- -->
<h2>Synthetic Dataset</h2>
<p>Training on a large real world dataset takes a lot of time and computational resources and could also involves hidden biases that could mask the capabilities of a model. For example, because of an unbalanced dataset, the activity of skiing could be detected only from the context of a snowy scene.  Thus, we designed a synthetic dataset where the complexity comes from the necessity of explicitly modeling spatial and temporal interactions but in a cleaner, simpler environment.</p>
<img src="https://bit-ml.github.io/galleries/rstg_pictures/digits.gif" alt="SyncMNIST" title="SyncMNIST" width="600"/>
<p>Our <a href="https://github.com/IuliaDuta/RSTG">SyncMNIST</a> datasets involves videos where the goal is to detect a pair of digits that move synchronous among others that move randomly. On this dataset, we validate our key design choices by conducting ablation studies.</p>
<p>We show that it is important to have both processing stages, each having its own set of parameters and to have multiple alternating temporal and spatial stages. We note that our model is also improved by incorporating positional embeddings in the node features. Our final model, that includes all these elements  surpasses strong models such as I3D and Non-Local.</p>
<h2>Real world experiments</h2>
<p><img src="https://bit-ml.github.io/galleries/rstg_pictures/smt-smt2.gif" alt="smt_example" title="a) Failing to put something into something because something does not fit b) pretending to put something into something"/></p>
<p>To validate the capability of our model, we evaluate the RSTG model on a human-object interaction dataset - <a href="https://20bn.com/datasets/something-something/v1">Something-Something v1</a> . This dataset contains fine-grained actions, that can not be distinguished solely on their context, where the interactions between entities across the entire video are essential.  We compare against top-models in the literature and obtain state of the art results.</p>
<p><img src="https://bit-ml.github.io/galleries/rstg_pictures/chart.svg" alt="results" title="Something-Something Results"/></p>
<h2>Conclusion</h2>
<p>We hope that graph based methods would be more broadly  adopted in visual domain tasks, especially those where interactions play a crucial role and that our methods brings more evidence that such models could be successfully applied in such tasks.</p>
<p>Our proposed RSTG model, seen as a spatio-temporal processing module, could be used for other various problems. Key aspects proved by our experiments,  such as the creation of a graph structure from convolutional features or the coupled but factorised time and space processing could be integrated into other models.</p>
<p>We released the code of our models and SyncMNIST dataset
<a href="https://github.com/IuliaDuta/RSTG">here</a>.</p>
<p>More details  about our work could be found in our
<a href="https://papers.nips.cc/paper/9444-recurrent-space-time-graph-neural-networks">paper</a>:</p>
<p>Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu, <em>Recurrent Space-time Graph Neural Networks</em>,   In Advances in neural information processing systems (<em>NeurIPS 2019</em>).</p></section><div class="Post__PostFooter-oyq0rs-8 fJVWOL"><div class="Post__PostFooterWraper-oyq0rs-9 lhqqFe"><p class="Post__Author-oyq0rs-4 itWYlR">written by <!-- -->Iulia Duță, Andrei Nicolicioiu</p></div></div></div></div></div></div></div><script type="text/javascript">window.__CSS_CHUNKS__ = {"main":"https://bit-ml.github.io/styles.961ce4e9.css"}</script><script type="text/javascript">
    window.__routeInfo = {"path":"blog/post/recurrent-space-time-graph-neural-nets","templateID":2,"sharedPropsHashes":{"galleries":"Rq3k6"},"localProps":null,"allProps":{"post":{"data":{"slug":"recurrent-space-time-graph-neural-nets","authors":"Iulia Duță, Andrei Nicolicioiu","categories":"blog","featured_img":"/galleries/rstg_pictures/featured_graph_nets.jpg","date":"November-26-2019","title":"Recurrent Space-time Graph Neural Network","id":7},"messages":[],"history":["./content/collections/posts/rstg2019.md","content/collections/posts/rstg2019.html"],"cwd":"/Users/dtantaru/bit-ml","contents":"<h1>Recurrent Space-time Graph Neural Network</h1>\n<!-- featured_img: \"/galleries/rstg_pictures/side_banner5.png\" -->\n<p>We introduce in this post our <strong>Recurrent Space-time Graph Neural Network</strong> (RSTG) architecture designed for learning video representation and  especially suited for tasks heavily relying on interactions.</p>\n<p>Let’s begin by considering the key components of video understanding that our method should include. Being able to detect and localise objects is the first crucial thing that we need to do, then we should combine them in various ways to create complex scenes. <em>Whole is greater than the sum of its parts</em>, but is it always true, and what are the conditions of this happening?</p>\n<p>In order to form a greater whole, objects must have some kind of connections between them. A set of random objects that do not correlate in any way doesn’t bring much additional information.  Entities could form different types of connections, they could be semantically related or they could be related by their physical position in space and in time. These kinds of relations happen in both image and video, with the time dimension of the video adding more difficulty, greatly increasing the amount and complexity of interactions happening in the scene.</p>\n<p>Let’s define the kind of interaction happening in the video by examining a few examples.</p>\n<!-- ![eeml2019_group_photo](/galleries/rstg_pictures/rstg.gif \"RSTG architecture.\") -->\n<p><img src=\"/galleries/rstg_pictures/interaction_videos.gif\" alt=\"eeml2019_group_photo\" title=\"Temporal interaction.\"></p>\n<p>Above, the yellow car and the grass appear together during the whole video, but the event <em>the car crosses the line</em> is characterized by a specific interaction. We call those types of interactions happening at the frame level <strong>spatial</strong> interactions. On the other hand, the event <em>the yellow car overtakes the red car</em> can not be captured from a single frame as it only makes sense across time. We call this <strong>temporal</strong> interactions.</p>\n<p>The entities that interact shouldn’t necessary be close to each other, either in time or in space, since there could also exist <strong>long-range</strong> interactions. In the above picture, the man and the moon are connected regardless of their distance in the image.</p>\n<h2>Analysing videos with spatio-temporal graph models</h2>\n<p>Models commonly used in Computer Vision, based on convolutional networks, implicitly capture interactions in both space and time dimension, but they are biased towards local, short-range relations.</p>\n<p>We propose a method designed to explicitly model relations and that is capable of capturing long range connections. Our model fits into the broader category of <strong>graph neural networks</strong> (GNNs). We process the video imposing a graph structure in order to explicitly model interactions between different entities. GNNs usually send  messages between each pair of  nodes, thus easily modeling binary interactions. Higher order interactions could be achieved by multiple such message-passing iterations.</p>\n<p>The animation below illustrates the main components of our model.</p>\n<!-- ![eeml2019_group_photo](/galleries/rstg_pictures/rstg.gif \"RSTG architecture.\") -->\n<img src=\"/galleries/rstg_pictures/rstg.gif\" alt=\"RSTG architecture.\"\ttitle=\"RSTG architecture.\" max-width=130% width=3584   />\n<p>At each time step, we <strong>create nodes</strong> by extracting information from features given by a convolutional network. Each node corresponds to a different fixed region of the input and we connect them if they come from neighbouring regions or if they overlap, as shown in the figure above. Using multiple scales helps us to capture entities of different sizes  and also to connect distant regions more easily.</p>\n<p>For the two types of interactions described above, we create two separate processing components. For each time step, we design a <strong>space processing</strong> stage to model the spatial interaction by iteratively  message-passing. This involves 3 steps: sending messages between each pair of connecting nodes, aggregating the messages received at each node by an attention mechanism and updating each node based on the current state and the aggregated message. We design a <strong>time processing</strong> stage to model temporal interactions by sending messages in time, only between the states of the same node, in a recurrent fashion. More specifically, at each time step, each node receives information only from its corresponding state from the previous time step.</p>\n<p>To model more expressive spatio-temporal interactions and to give it the ability to reason about all the information in the scene, with knowledge about past states, we alternate the two processing stages, as shown in the animation below.</p>\n<p><img src=\"/galleries/rstg_pictures/rstg_stages.gif\" alt=\"rstg_stages\" title=\"Alternative Space and Time Processing Stages.\"></p>\n<p>By having multiple space iteration, we go from local to more global processing, modeling interactions happening between an increasing number of entities situated at increasingly longer distances. We want to have some temporal information at every such steps, in order to model interactions that takes into account the history.  In order to combine the same kind of features from different time steps, we only connect the k-th spatial stage with the k-th stage from the previous step as shown in the animation.</p>\n<p>We can pool all the nodes into a vector representation used for the final prediction or we could project back each node into its initial corresponding region, forming a feature volume with the same size as the graph input so that our model could be used as a module inside any other architecture.</p>\n<!-- ![florin_brad](/galleries/eeml2019_pictures/florin_brad.jpg \"Florin Brad giving the tour.\") -->\n<!-- <img src=\"/galleries/eeml2019_pictures/florin_brad.jpg\" alt=\"Florin Brad\"\ttitle=\"Florin Brad giving the tour.\" height=520 /> -->\n<!-- ![eeml_volunteers](/galleries/eeml2019_pictures/florin_brad.jpg \"EEML volunteers welcoming the participants.\") -->\n<h2>Synthetic Dataset</h2>\n<p>Training on a large real world dataset takes a lot of time and computational resources and could also involves hidden biases that could mask the capabilities of a model. For example, because of an unbalanced dataset, the activity of skiing could be detected only from the context of a snowy scene.  Thus, we designed a synthetic dataset where the complexity comes from the necessity of explicitly modeling spatial and temporal interactions but in a cleaner, simpler environment.</p>\n<img src=\"/galleries/rstg_pictures/digits.gif\" alt=\"SyncMNIST\"\ttitle=\"SyncMNIST\" width=600   />\n<p>Our <a href=\"https://github.com/IuliaDuta/RSTG\">SyncMNIST</a> datasets involves videos where the goal is to detect a pair of digits that move synchronous among others that move randomly. On this dataset, we validate our key design choices by conducting ablation studies.</p>\n<p>We show that it is important to have both processing stages, each having its own set of parameters and to have multiple alternating temporal and spatial stages. We note that our model is also improved by incorporating positional embeddings in the node features. Our final model, that includes all these elements  surpasses strong models such as I3D and Non-Local.</p>\n<h2>Real world experiments</h2>\n<p><img src=\"/galleries/rstg_pictures/smt-smt2.gif\" alt=\"smt_example\" title=\"a) Failing to put something into something because something does not fit b) pretending to put something into something\"></p>\n<p>To validate the capability of our model, we evaluate the RSTG model on a human-object interaction dataset - <a href=\"https://20bn.com/datasets/something-something/v1\">Something-Something v1</a> . This dataset contains fine-grained actions, that can not be distinguished solely on their context, where the interactions between entities across the entire video are essential.  We compare against top-models in the literature and obtain state of the art results.</p>\n<p><img src=\"/galleries/rstg_pictures/chart.svg\" alt=\"results\" title=\"Something-Something Results\"></p>\n<h2>Conclusion</h2>\n<p>We hope that graph based methods would be more broadly  adopted in visual domain tasks, especially those where interactions play a crucial role and that our methods brings more evidence that such models could be successfully applied in such tasks.</p>\n<p>Our proposed RSTG model, seen as a spatio-temporal processing module, could be used for other various problems. Key aspects proved by our experiments,  such as the creation of a graph structure from convolutional features or the coupled but factorised time and space processing could be integrated into other models.</p>\n<p>We released the code of our models and SyncMNIST dataset\n<a\nhref=\"https://github.com/IuliaDuta/RSTG\">here</a>.</p>\n<p>More details  about our work could be found in our\n<a\nhref=\"https://papers.nips.cc/paper/9444-recurrent-space-time-graph-neural-networks\">paper</a>:</p>\n<p>Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu, <em>Recurrent Space-time Graph Neural Networks</em>,   In Advances in neural information processing systems (<em>NeurIPS 2019</em>).</p>\n"},"galleries":{"asm_llm2024":[{"src":"/galleries/asm_llm2024/asmllm_code_repair.png","width":2570,"height":370},{"src":"/galleries/asm_llm2024/asmllm_downstream.png","width":1255,"height":769},{"src":"/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg","width":5760,"height":3840}],"atari_agents_2022":[{"src":"/galleries/atari_agents_2022/collage.png","width":960,"height":1300},{"src":"/galleries/atari_agents_2022/dqn_modern_pytorch_vs_dopamine.png","width":1022,"height":367},{"src":"/galleries/atari_agents_2022/octocat.svg","width":16,"height":16},{"src":"/galleries/atari_agents_2022/rliable_comparison.png","width":875,"height":763},{"src":"/galleries/atari_agents_2022/sampling.png","width":1348,"height":164}],"bgv_scheme2023":[{"src":"/galleries/bgv_scheme2023/fhe_add.drawio.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_add_tiny.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_mul.drawio.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_mul_tiny.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_noise.png","width":390,"height":412},{"src":"/galleries/bgv_scheme2023/fhe_noise_tiny.png","width":390,"height":412},{"src":"/galleries/bgv_scheme2023/fhe_simple.drawio.png","width":538,"height":161},{"src":"/galleries/bgv_scheme2023/fhe_simple_tiny.png","width":538,"height":161},{"src":"/galleries/bgv_scheme2023/lock.jpg","width":1200,"height":630},{"src":"/galleries/bgv_scheme2023/poly.drawio.png","width":342,"height":346},{"src":"/galleries/bgv_scheme2023/poly_tiny.png","width":342,"height":346},{"src":"/galleries/bgv_scheme2023/sec_game.drawio.png","width":350,"height":244},{"src":"/galleries/bgv_scheme2023/sec_game_tiny.png","width":350,"height":244}],"courses":[{"src":"/galleries/courses/atari_collage.png","width":960,"height":1300},{"src":"/galleries/courses/campus.jpg","width":725,"height":1080},{"src":"/galleries/courses/overview_bitdefender.jpg","width":1959,"height":2813},{"src":"/galleries/courses/overview_crypto.jpg","width":1920,"height":960},{"src":"/galleries/courses/overview_fmi.jpg","width":768,"height":600},{"src":"/galleries/courses/overview_pm.png","width":1194,"height":1190},{"src":"/galleries/courses/thumb_bitdefender.png","width":128,"height":128},{"src":"/galleries/courses/thumb_precis.jpg","width":128,"height":128},{"src":"/galleries/courses/thumb_rosedu.png","width":360,"height":360},{"src":"/galleries/courses/thumb_unibuc.png","width":512,"height":512}],"deep_bit_2019_pictures":[{"src":"/galleries/deep_bit_2019_pictures/20190731_150323.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_150331.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_153111.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_153118.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_164741.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_164747.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142509.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142514.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142521.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142523_1.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142925.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_144034.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145138.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145146.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145644.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145647.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145650.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145654.jpg","width":4160,"height":3120}],"deep_bit_2019_posters":[{"src":"/galleries/deep_bit_2019_posters/poster_alin_passwords.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_bogdan_train_with_private_data.webp","width":1920,"height":1440},{"src":"/galleries/deep_bit_2019_posters/poster_cristi_stegano.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_elena+adi_adversarial.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_horia+alin_homoglyph.webp","width":1920,"height":1486},{"src":"/galleries/deep_bit_2019_posters/poster_mada_crypto_nets.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_mircescu_MRI.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_novac_logs_anomalies.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_tudor_fake_news.webp","width":1920,"height":2715},{"src":"/galleries/deep_bit_2019_posters/poster_vlad_mouse_auth.webp","width":1920,"height":2715}],"deep_fmi_2019_posters":[{"src":"/galleries/deep_fmi_2019_posters/10. EmoContext_Poster.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/10. Image_Segmentation.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/10. Landmark Recognition _ Visualization.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/10. Political Orientation Classification.webp","width":1920,"height":1440},{"src":"/galleries/deep_fmi_2019_posters/10. Poster_VDSR.webp","width":1920,"height":2714},{"src":"/galleries/deep_fmi_2019_posters/2048_RL.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/A LEARNED REPRESENTATION FOR ARTISTIC STYLE.webp","width":1275,"height":1650},{"src":"/galleries/deep_fmi_2019_posters/Emotion_Detection.webp","width":1920,"height":2688},{"src":"/galleries/deep_fmi_2019_posters/Emotion_Detection_CNN.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/Google Landmark Recognition Challenge.webp","width":1755,"height":2480},{"src":"/galleries/deep_fmi_2019_posters/Image Classification.webp","width":1275,"height":1650},{"src":"/galleries/deep_fmi_2019_posters/Image Semantic Segmentation.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/Image_Colorization.webp","width":1920,"height":2717},{"src":"/galleries/deep_fmi_2019_posters/PosterRL.webp","width":1920,"height":2715},{"src":"/galleries/deep_fmi_2019_posters/Revising Bi-Axial LSTM for Music Generation.webp","width":1920,"height":2718},{"src":"/galleries/deep_fmi_2019_posters/Semantic Segmantation.webp","width":1920,"height":1920},{"src":"/galleries/deep_fmi_2019_posters/Style Transfer.webp","width":1920,"height":2555},{"src":"/galleries/deep_fmi_2019_posters/poster.webp","width":1275,"height":1650}],"eeml2019_pictures":[{"src":"/galleries/eeml2019_pictures/andrei.jpg","width":1590,"height":1060},{"src":"/galleries/eeml2019_pictures/bitdefender_booth.jpg","width":1710,"height":1140},{"src":"/galleries/eeml2019_pictures/ema.jpg","width":1394,"height":930},{"src":"/galleries/eeml2019_pictures/florin.jpg","width":1152,"height":2048},{"src":"/galleries/eeml2019_pictures/florin_brad.jpg","width":928,"height":1392},{"src":"/galleries/eeml2019_pictures/full_house.jpg","width":1392,"height":928},{"src":"/galleries/eeml2019_pictures/group.jpg","width":1939,"height":1293},{"src":"/galleries/eeml2019_pictures/volunteers.jpg","width":1392,"height":928},{"src":"/galleries/eeml2019_pictures/z_ema_poster.png","width":1449,"height":2048},{"src":"/galleries/eeml2019_pictures/z_florin_poster.png","width":1698,"height":2400},{"src":"/galleries/eeml2019_pictures/z_iulia_andrei_poster.png","width":1772,"height":2496}],"eeml2019_posters":[{"src":"/galleries/eeml2019_posters/z_ema_poster.png","width":1449,"height":2048},{"src":"/galleries/eeml2019_posters/z_florin_poster.png","width":1698,"height":2400},{"src":"/galleries/eeml2019_posters/z_iulia_andrei_poster.png","width":1772,"height":2496}],"fused_swiglu2025":[{"src":"/galleries/fused_swiglu2025/A.png","width":1940,"height":1454},{"src":"/galleries/fused_swiglu2025/B.png","width":1982,"height":1242},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 405B.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 70B.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Gated MLP for Llama 8B.svg","width":460,"height":345},{"src":"/galleries/fused_swiglu2025/Memory Usage for Llama 405B.svg","width":404,"height":325},{"src":"/galleries/fused_swiglu2025/Memory Usage for Llama 70B.svg","width":398,"height":325},{"src":"/galleries/fused_swiglu2025/Memory Usage for Llama 8B.svg","width":408,"height":325},{"src":"/galleries/fused_swiglu2025/chunk.png","width":1999,"height":1118},{"src":"/galleries/fused_swiglu2025/draft3.drawio.svg","width":591,"height":206},{"src":"/galleries/fused_swiglu2025/gpu.jpg","width":1024,"height":768},{"src":"/galleries/fused_swiglu2025/gpu2.jpeg","width":2048,"height":2048},{"src":"/galleries/fused_swiglu2025/kernel_look.png","width":2086,"height":1408},{"src":"/galleries/fused_swiglu2025/kernel_warp_diagram.svg","width":182761,"height":145554},{"src":"/galleries/fused_swiglu2025/savings.svg","width":1288,"height":423}],"homomorphic2020":[{"src":"/galleries/homomorphic2020/alice.png","width":1000,"height":339},{"src":"/galleries/homomorphic2020/arithmetic_circuit.png","width":408,"height":382},{"src":"/galleries/homomorphic2020/dec_details.png","width":676,"height":312},{"src":"/galleries/homomorphic2020/enigma_dec.jpg","width":800,"height":1200},{"src":"/galleries/homomorphic2020/eval_mul.png","width":569,"height":460},{"src":"/galleries/homomorphic2020/eval_sum.png","width":567,"height":441},{"src":"/galleries/homomorphic2020/he_scheme5.png","width":1354,"height":390},{"src":"/galleries/homomorphic2020/noisy_cyphertext.png","width":658,"height":449}],"private_set2021":[{"src":"/galleries/private_set2021/monolith.jpg","width":800,"height":1200}],"rstg_pictures":[{"src":"/galleries/rstg_pictures/RSTG_model.png","width":2347,"height":1031},{"src":"/galleries/rstg_pictures/chart.svg","width":600,"height":371},{"src":"/galleries/rstg_pictures/digits.gif","width":692,"height":337},{"src":"/galleries/rstg_pictures/featured_graph_nets.jpg","width":1455,"height":1200},{"src":"/galleries/rstg_pictures/featured_graph_nets_bk.jpg","width":2784,"height":1856},{"src":"/galleries/rstg_pictures/interaction_videos.gif","width":1079,"height":258},{"src":"/galleries/rstg_pictures/rstg.gif","width":1353,"height":517},{"src":"/galleries/rstg_pictures/rstg_stages.gif","width":600,"height":565},{"src":"/galleries/rstg_pictures/smt-smt2.gif","width":725,"height":251}],"tmlss2018_pictures":[{"src":"/galleries/tmlss2018_pictures/dana.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/elena.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/florin.jpg","width":810,"height":1080},{"src":"/galleries/tmlss2018_pictures/iulia.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/lecture_hall.jpg","width":1920,"height":992},{"src":"/galleries/tmlss2018_pictures/salina.jpg","width":1920,"height":1280},{"src":"/galleries/tmlss2018_pictures/salina_1.jpg","width":1920,"height":1280},{"src":"/galleries/tmlss2018_pictures/stefan.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/stefan_1.jpg","width":1599,"height":899}],"tmlss2018_posters":[{"src":"/galleries/tmlss2018_posters/Bitdefender+NLP2SQL_1440.jpg","width":1440,"height":3558},{"src":"/galleries/tmlss2018_posters/ElenaBurceanu_Tracking_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/EmanuelaHaller_Segmentation_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/IuliaDuta_AndreiNicolicioiu_Video2NLP_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/StefanPostavaru_RMSProp_1440.jpg","width":1440,"height":2036}]}},"siteData":{"title":"Bitdefender Machine Learning & Crypto Research Unit","description":"Bitdefender Machine Learning & Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&ML scene by supporting and participating in local conferences, lecture and research groups.","tagline":"Engaging with the broader Machine Learning Community.","tags":["machine-learning","research","bitdefender"]}};</script><script defer="" type="text/javascript" src="https://bit-ml.github.io/bootstrap.f08b3b1c.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/templates/src/pages/Post.5cee3c5c.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/main.95926ac4.js"></script></body></html>