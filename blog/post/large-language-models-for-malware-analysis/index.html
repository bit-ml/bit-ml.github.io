<!DOCTYPE html><html lang="en"><head><title data-react-helmet="true">Large Language Models for Malware Analysis | Bitdefender Research</title><meta data-react-helmet="true" name="description" content="Large Language Models for Malware AnalysisLarge Language Models (LLMs) took the world by storm in 2023, revolutionizing the way people search and generate text content. LLMs for code have also made inroads in helping people understand code or write code based on requests in natural language. For instance, translating requests to SQL queries has ra..."/><meta data-react-helmet="true" name="keywords" content="blog"/><meta data-react-helmet="true" property="og:title" content="Large Language Models for Malware Analysis | Bitdefender Research"/><meta data-react-helmet="true" property="og:description" content="Large Language Models for Malware AnalysisLarge Language Models (LLMs) took the world by storm in 2023, revolutionizing the way people search and generate text content. LLMs for code have also made inroads in helping people understand code or write code based on requests in natural language. For instance, translating requests to SQL queries has ra..."/><meta data-react-helmet="true" property="og:site_name" content="https://bit-ml.github.io"/><meta data-react-helmet="true" property="article:tag" content="blog"/><meta data-react-helmet="true" property="og:image" content="https://bit-ml.github.io/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:site" content="@Bitdefender"/><meta data-react-helmet="true" name="twitter:title" content="Large Language Models for Malware Analysis | Bitdefender Research"/><meta data-react-helmet="true" name="twitter:description" content="Large Language Models for Malware AnalysisLarge Language Models (LLMs) took the world by storm in 2023, revolutionizing the way people search and generate text content. LLMs for code have also made inroads in helping people understand code or write code based on requests in natural language. For instance, translating requests to SQL queries has ra..."/><meta data-react-helmet="true" name="twitter:image" content="https://bit-ml.github.io/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg"/><meta data-react-helmet="true" itemProp="description" content="Large Language Models for Malware AnalysisLarge Language Models (LLMs) took the world by storm in 2023, revolutionizing the way people search and generate text content. LLMs for code have also made inroads in helping people understand code or write code based on requests in natural language. For instance, translating requests to SQL queries has ra..."/><meta data-react-helmet="true" itemProp="keywords" content="blog"/><meta data-react-helmet="true" itemProp="image" content="https://bit-ml.github.io/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg"/><link rel="preload" as="script" href="https://bit-ml.github.io/bootstrap.70ac8375.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/templates/src/pages/Post.5cee3c5c.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/main.92808e2a.js"/><link rel="preload" as="style" href="https://bit-ml.github.io/styles.2986121e.css"/><link rel="stylesheet" href="https://bit-ml.github.io/styles.2986121e.css"/><meta charSet="UTF-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><link rel="apple-touch-icon" href="icon.png"/><style data-styled-components="goldxz gFasvY iRDKCt">
/* sc-component-id: sc-global-16140688 */
html{line-height:1.15;-webkit-text-size-adjust:100%;} body{margin:0;} h1{font-size:2em;margin:0.67em 0;} hr{box-sizing:content-box;height:0;overflow:visible;} pre{font-family:monospace,monospace;font-size:1em;} a{background-color:transparent;-webkit-text-decoration:none;text-decoration:none;} abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;} b,strong{font-weight:bolder;} code,kbd,samp{font-family:monospace,monospace;font-size:1em;} small{font-size:80%;} sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;} sub{bottom:-0.25em;} sup{top:-0.5em;} figure{margin:0;} img{border-style:none;max-width:100%;} button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0;} button,input{overflow:visible;} button,select{text-transform:none;} button,[type="button"],[type="reset"],[type="submit"]{-webkit-appearance:button;} button::-moz-focus-inner,[type="button"]::-moz-focus-inner,[type="reset"]::-moz-focus-inner,[type="submit"]::-moz-focus-inner{border-style:none;padding:0;} button:-moz-focusring,[type="button"]:-moz-focusring,[type="reset"]:-moz-focusring,[type="submit"]:-moz-focusring{outline:1px dotted ButtonText;} fieldset{padding:0.35em 0.75em 0.625em;} legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;} progress{vertical-align:baseline;} textarea{overflow:auto;} [type="checkbox"],[type="radio"]{box-sizing:border-box;padding:0;} [type="number"]::-webkit-inner-spin-button,[type="number"]::-webkit-outer-spin-button{height:auto;} [type="search"]{-webkit-appearance:textfield;outline-offset:-2px;} [type="search"]::-webkit-search-decoration{-webkit-appearance:none;} ::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;} details{display:block;} summary{display:list-item;} template{display:none;} [hidden]{display:none;} html{font-size:19px;box-sizing:border-box;} *,*::before,*::after{box-sizing:inherit;} body{font-family:"Roboto",Helvetica,Arial,sans-serif;line-height:1.78rem;padding:0;background:#FFF;} h1{display:block;margin:0.67em 0;} .math-display{overflow-x:auto;} @media (min-width:74.6875em){.math-display{overflow-x:initial;}} .remark-highlight{font-size:14px;} @media (min-width:74.6875em){.remark-highlight{font-size:16px;}} p.hint.tip,p.hint.error,p.hint.warn{-webkit-letter-spacing:0;-moz-letter-spacing:0;-ms-letter-spacing:0;letter-spacing:0;box-sizing:border-box;font-size:inherit;line-height:1.6rem;word-spacing:0.05rem;background-color:rgba(238,238,238,0.5);border-bottom-right-radius:2px;border-top-right-radius:2px;padding:8px 12px 8px 24px;margin-bottom:16px;position:relative;} p.hint.tip:before,p.hint.error:before,p.hint.warn:before{border-radius:100%;color:#fff;content:'!';font-size:14px;font-weight:700;left:-12px;line-height:20px;position:absolute;height:20px;width:20px;text-align:center;top:12px;} p.hint.tip{border-left:4px solid #27ab83;} p.hint.tip:before{display:none;} p.hint.warn{border-left:4px solid #f0b429;} p.hint.warn:before{background-color:#f0b429;} p.hint.error{border-left:4px solid #ef4e4e;} p.hint.error:before{background-color:#ef4e4e;content:'Ã—';} .content table{border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb;font-size:12px;line-height:1.0rem;} @media (min-width:74.6875em){.content table{font-size:19px;line-height:1.78rem;}} .content caption{color:#000;font:italic 85%/1 arial,sans-serif;padding:1em 0;text-align:center;} .content td,.content th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;font-size:inherit;margin:0;overflow:visible;padding:0.5em 1em;} .content thead{background-color:#e0e0e0;color:#000;text-align:left;vertical-align:bottom;} .content td{background-color:#f2f2f2;} .content tr:nth-child(2n-1) td{background-color:transparent;} .content td{border-bottom:1px solid #cbcbcb;} .content tbody > tr:last-child > td{border-bottom-width:0;} .content td,.content th{border-width:0 0 1px 0;border-bottom:1px solid #cbcbcb;} .content tbody > tr:last-child > td{border-bottom-width:0;}
/* sc-component-id: Navigation__Nav-qabwmo-0 */
.iRDKCt{width:100%;background:transparent;padding:0 1rem;font-family:"Roboto",Helvetica,Arial,sans-serif;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;} .iRDKCt a{padding:0.5rem 0 0.25rem 0;font-style:normal;font-weight:500;line-height:23px;font-size:14px;text-align:right;text-transform:uppercase;color:#333;} .iRDKCt a:last-child{padding:0.5rem;padding-right:0;} @media (min-width:74.6875em){.iRDKCt{padding:0 5rem;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}.iRDKCt a{padding:0.5rem 1rem 0.25rem 1rem;}}
/* sc-component-id: Featured__StyledButtonBack-xm4c49-5 */
.goldxz{position:absolute;top:50%;left:-5px;margin-left:10px;margin-top:-20px;width:40px;height:40px;padding:10px;background:none;border:none;outline:none;border-radius:40px;} @media (min-width:46.0625em){.goldxz{left:10px;width:70px;height:70px;padding:15px;}} @media (min-width:64.0625em){.goldxz{left:10px;width:70px;height:70px;padding:15px;}}
/* sc-component-id: Featured__StyledButtonNext-xm4c49-6 */
.gFasvY{position:absolute;top:50%;right:-5px;margin-right:10px;margin-top:-20px;width:40px;height:40px;padding:10px;background:none;border:none;outline:none;border-radius:40px;} @media (min-width:46.0625em){.gFasvY{right:10px;width:70px;height:70px;padding:15px;}} @media (min-width:64.0625em){.gFasvY{right:10px;width:70px;height:70px;padding:15px;}}</style><style data-styled-components="cSOdgJ cimTyS fGODKX jSdCWo kAktca gMsmUp kSsCZe fJVWOL lhqqFe itWYlR">
/* sc-component-id: Post__PageWithCoverImg-oyq0rs-0 */
@media (min-width:74.6875em){.cSOdgJ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}.cSOdgJ nav{padding:0;}} @media (min-width:46.0625em){.cSOdgJ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}}
/* sc-component-id: Post-oyq0rs-1 */
.fGODKX{margin:0 auto;padding:0 1rem;} @media (min-width:46.0625em){.fGODKX{max-width:62%;padding:0 2rem;}} @media (min-width:74.6875em){.fGODKX{max-width:62%;padding:0 5rem;}}
/* sc-component-id: Post__PostContent-oyq0rs-2 */
.jSdCWo{margin:0 auto;max-width:720px;} .jSdCWo > h1{font-family:"Exo 2",sans-serif;font-size:2.074rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:2.074rem;} .jSdCWo > h1 > span{display:block;font-size:1.20rem;line-height:2.074rem;} @media (min-width:74.6875em){.jSdCWo > h1{font-size:2.488rem;line-height:3.57895rem;}.jSdCWo > h1 > span{font-size:1.44rem;}} .jSdCWo > h2{font-family:"Exo 2",sans-serif;font-size:1.728rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:2.074rem;} @media (min-width:74.6875em){.jSdCWo > h2{font-size:2.074rem;line-height:3.57895rem;}} .jSdCWo > h3{font-family:"Exo 2",sans-serif;font-size:1.44rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:1.78947rem;} @media (min-width:74.6875em){.jSdCWo > h3{font-size:1.728rem;}} .jSdCWo > h4{font-family:"Exo 2",sans-serif;font-size:1.20rem;font-weight:600;margin-top:1.78947rem;margin-bottom:1.78947rem;padding:0;line-height:1.78947rem;} @media (min-width:74.6875em){.jSdCWo > h4{font-size:1.44rem;}} .jSdCWo > h5{font-family:"Exo 2",sans-serif;font-size:1.20rem;font-weight:600;margin-top:1.78947rem;margin-bottom:0;padding:0;line-height:1.78947rem;} .jSdCWo p{font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:1.00rem;font-weight:300;margin-top:0;margin-bottom:1.78947rem;padding:0;font-style:normal;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;color:#333;} .jSdCWo p > strong{font-weight:500;} .jSdCWo >table{font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:0.694rem;font-weight:300;margin-bottom:1.78947rem;} .jSdCWo >table > strong{font-weight:500;} @media (min-width:74.6875em){.jSdCWo div.wide-content+table{width:920px;margin-left:-100px;}} .jSdCWo a{-webkit-text-decoration:underline;text-decoration:underline;color:#333;} .jSdCWo a:hover{color:#e6212b;} .jSdCWo ul,.jSdCWo ol{margin-top:0;margin-bottom:1.78947rem;font-weight:300;line-height:1.78947rem;} .jSdCWo ul > li ul,.jSdCWo ol > li ul{margin-bottom:0;} .jSdCWo > img,.jSdCWo > p img{margin:0 auto;display:block;} .jSdCWo > blockquote{font-size:1.1rem;font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:italic;font-weight:300;line-height:1.8rem;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;text-align:right;margin-right:0;padding:24px;quotes:"\201E""\201C";} .jSdCWo > blockquote:before{display:inline-block;-webkit-transform:translate(-15px,-15px);-ms-transform:translate(-15px,-15px);transform:translate(-15px,-15px);content:open-quote;color:#edebeb;font-size:5rem;font-weight:400;} .jSdCWo > blockquote > footer{margin-top:10px;font-style:normal;font-weight:400;} .jSdCWo > details summary{cursor:pointer;}
/* sc-component-id: Post__PostHeader-oyq0rs-5 */
.kAktca{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:normal;font-weight:400;line-height:1.8rem;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;}
/* sc-component-id: Post__BackLink-oyq0rs-6 */
.gMsmUp{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;color:#828282;} .gMsmUp:hover{color:#e6212b;}
/* sc-component-id: Post__Date-oyq0rs-7 */
.kSsCZe{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;font-size:1rem;text-align:right;color:#828282;}
/* sc-component-id: Post__PostFooter-oyq0rs-8 */
.fJVWOL{display:inline-block;background:#edebeb;width:100%;}
/* sc-component-id: Post__PostFooterWraper-oyq0rs-9 */
.lhqqFe{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;max-width:720px;padding:0 1rem;margin:0 auto;} .lhqqFe > p{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;color:#828282;} @media (min-width:74.6875em){.lhqqFe{padding-left:0 0 0 100px;}}
/* sc-component-id: Post__CoverImg-oyq0rs-10 */
@media (min-width:46.0625em){.cimTyS{background:#fff url(/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg) no-repeat bottom;background-size:cover;position:-webkit-sticky;position:sticky;top:0;left:0;height:100vh;width:38%;}} @media (min-width:74.6875em){.cimTyS{background:#fff url(/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg) no-repeat bottom;background-size:cover;position:-webkit-sticky;position:sticky;top:0;left:0;height:100vh;width:38%;}}</style></head><body><div id="root"><div class="content" data-reactroot=""><div><div class="Post__PageWithCoverImg-oyq0rs-0 cSOdgJ"><div class="Post__CoverImg-oyq0rs-10 cimTyS"></div><div class="Post-oyq0rs-1 fGODKX"><header><nav class="Navigation__Nav-qabwmo-0 iRDKCt"><a href="https://bit-ml.github.io/">Home</a><a href="https://bit-ml.github.io/#research">Research</a><a href="https://bit-ml.github.io/#teams">Team</a><a href="https://bit-ml.github.io/teaching/lectures-and-courses">Teaching</a></nav></header><section class="Post__PostContent-oyq0rs-2 jSdCWo"><div class="Post__PostHeader-oyq0rs-5 kAktca"><a class="Post__BackLink-oyq0rs-6 gMsmUp active" aria-current="page" href="https://bit-ml.github.io/">&lt;<!-- --> Back Home</a><small class="Post__Date-oyq0rs-7 kSsCZe">published on <!-- -->January 12, 2024</small></div><h1>Large Language Models for Malware Analysis</h1>
<p>Large Language Models (LLMs) took the world by storm in 2023, revolutionizing the way people search and generate text content. LLMs for code have also made inroads in helping people understand code or write code based on requests in natural language. For instance, translating requests to <a href="https://yale-lily.github.io/spider">SQL queries</a> has rapidly advanced since the advent of GPT4.</p>
<p>Most popular code LLMs focus on generating or understanding high-level programming languages such as Python, C++, Java etc. <a href="https://arxiv.org/abs/2308.12950">[1]</a>,<a href="https://arxiv.org/abs/2305.06161">[2]</a>,<a href="https://arxiv.org/abs/2306.08568">[3]</a>,<a href="https://arxiv.org/abs/2308.07124">[4]</a>. However code LLMs tailored to malware analysis should be adapted to better understand assembly code as well. Recent works also explore this avenue <a href="https://arxiv.org/abs/2312.09601">[5]</a>,<a href="https://arxiv.org/abs/2310.16853">[6]</a>,<a href="https://arxiv.org/abs/2311.13721">[7]</a>.</p>
<p>To this end we start from existing general-purpose LLMs such as <a href="https://huggingface.co/mosaicml/mpt-7b">MPT7B</a>, which has been trained on both English text and code. We adapt them to x86-64 assembly data, by finetuning them using the Causal Language Modeling task and the <a href="https://arxiv.org/abs/2106.09685">LoRA</a> method. We then inspect these models to evaluate the quality of their embeddings and their generation capabilities. We call these models <strong>asmLLMs</strong>.</p>
<h2>asmLLMs for feature extraction</h2>
<p>Features learned by pretrained models are crucial for downstream tasks such as anomaly detection, search or classification. Most LLMs (either general purpose or code-based) have little assembly data in their pretraining corpus relative to the entire corpus. Finetuning LLMs on more assembly data should then result in better features. To test this, we developed several downstream classification tasks, where the input is assembly code. Classifiers trained on features from the <strong>asmLLM</strong> should then outperform classifiers trained on features from base LLMs.</p>
<p>To build examples for our tasks, we took C++ solutions from the code contest dataset <a href="https://arxiv.org/pdf/2105.12655v2.pdf">CodeNet 1000</a> and converted them to assembly. The three classification tasks are:</p>
<ul>
<li><em>Complexity prediction</em>, where the input is the assembly corresponding to a C++ function, and the label is the <a href="https://manpages.ubuntu.com/manpages/focal/man1/pmccabe.1.html">cyclomatic complexity</a> of that function (13 classes)</li>
<li><em>Problem identification</em>, where the input is the assembly corresponding to a C++ submission to a code contest, and the label is the ID of the contest problem (20 classes)</li>
<li><em>Accuracy prediction</em>, where the input is the assembly corresponding to a C++ submission to a code contest, and the label is the score achieved by the submission on the benchmark. Instead of predicting the score, the model predicts a class from a set of 5 classes, where each class corresponds to different bins of performance: [0-20) score, [20-40) score, ..., [80-100].</li>
</ul>
<p><img src="https://bit-ml.github.io/galleries/asm_llm2024/asmllm_downstream.png" alt="asm_down"/></p>
<p>The asmLLM in <a href="asm_down">Figure 1</a> is an MPT7B-base finetuned on assembly data using sequences of length 2K. To embed an input, it is first split into chunks of length 2K or 4K tokens. The input embedding is then computed as the average over these chunk embeddings. Results show that classifiers benefit from features from asmLLMs, especially when using larger contexts (4K tokens vs 2K tokens).</p>
<h2>asmLLMs for generative tasks</h2>
<h3>Code repair</h3>
<p>While <strong>asmLLMs</strong> can be used as feature extractors, we are also interested in generative settings such as code-to-code or code-to-text. A typical code-to-code task is code repair, where given a faulty piece of code, the correct version of the code is produced. To test this on assembly, we generate a synthetic dataset where we alter a percentage of the assembly tokens, by modifying instructions (e.g. <strong>mov</strong> -&gt; <strong>lea</strong>), registers (<strong>rax</strong> -&gt; <strong>rsp</strong>) or offsets. We then perform instruction tuning on the asmLLM using (altered assembly, correct assembly) pairs and a 4K token context.</p>
<p><img src="https://bit-ml.github.io/galleries/asm_llm2024/asmllm_code_repair.png" alt="asm_repair"/></p>
<p>We notice in Figure 2 that the instruction tuned asmLLM model can correctly predict altered instructions (<strong>movzx</strong> -&gt; <strong>mov</strong>) or registers (<strong>ch</strong>-&gt;<strong>rax</strong>).</p>
<h3>Code summarization</h3>
<p>To inspect the code of executable files, malware analysts use a wide array of tools, including powerful decompilers and disassemblers. While top performing LLMs such as GPT-4 are good at explaining high-level languages such as Python or C++, they are less likely to get the big picture of assembly code and instead describe it line by line usually. We are thus interested in the code summarization task, where explaining large chunks of assembly code can help analysts delve into binaries more efficiently.</p>
<p>For this task, we generate a synthetic instruction tuning dataset called OSS-ASM, which consists of (assembly code, explanation) pairs. We first use the OSS-instruct<a href="https://arxiv.org/pdf/2312.02120.pdf">[8]</a> method to produce (C++ code, explanation) pairs by seeding LLMs with assembly snippets from various technical resources. We then convert the C++ code to x86-64 assembly.</p>
<table><thead><tr><th>Model</th><th>Rouge-L</th><th>BLEU</th></tr></thead><tbody><tr><td>GPT-3.5 turbo</td><td>23.22</td><td>2.85</td></tr><tr><td>GPT-4</td><td>26.56</td><td>4.03</td></tr><tr><td><a href="https://huggingface.co/mosaicml/$mpt-7b">MPT7B-base</a> + instruction tuning OSS-ASM</td><td>33.22</td><td>11.89</td></tr><tr><td><a href="https://huggingface.co/mosaicml/mpt-7b-instruct">MPT7B-instruct</a> + instruction tuning OSS-ASM</td><td>33.92</td><td>12.29</td></tr><tr><td><strong>asmLLM</strong> + instruction tuning OSS-ASM</td><td><strong>35.72</strong></td><td><strong>13.65</strong></td></tr></tbody></table>
<p>The <strong>asmLLM</strong> model in the table above is a MPT7B-base, finetuned on a subset of 500M tokens of x86-64 assembly, with LoRA rank r=64 and context length L=4096 tokens. GPT-3.5 and GPT-4 are tested in a zero-shot setup, while the other three models are instruction tuned on the OSS-ASM dataset. Results show that adapting base LLMs on assembly data improves the performance on downstream generative tasks such as code summarization.</p>
<p>Along with results in Figure 1, this highlights the importance of pretraining LLMs on domain-specific data, particularly when dealing with languages that are less represented in large training sets, such as assembly.</p>
<p><strong>asmLLMs</strong> can become an essential item in a malware analysis toolkit, accelerating the inspection of binaries and improving the threat response time. Future work involves learning across different computer architectures, increasing the diversity of the instruction tuning data and tackling obfuscated code.</p>
<h2>References</h2>
<ol>
<li>Code Llama: Open Foundation Models for Code, <a href="https://arxiv.org/abs/2308.12950">https://arxiv.org/abs/2308.12950</a></li>
<li>StarCoder: may the source be with you!, <a href="https://arxiv.org/abs/2305.06161">https://arxiv.org/abs/2305.06161</a></li>
<li>WizardCoder: Empowering Code Large Language Models with Evol-Instruct, <a href="https://arxiv.org/abs/2306.08568">https://arxiv.org/abs/2306.08568</a></li>
<li>OctoPack: Instruction Tuning Code Large Language Models, <a href="https://arxiv.org/abs/2308.07124">https://arxiv.org/abs/2308.07124</a></li>
<li>Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models, <a href="https://arxiv.org/abs/2312.09601">https://arxiv.org/abs/2312.09601</a></li>
<li>CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code, <a href="https://arxiv.org/abs/2310.16853">https://arxiv.org/abs/2310.16853</a>,</li>
<li>Nova+: Generative Language Models for Binaries, <a href="https://arxiv.org/abs/2311.13721">https://arxiv.org/abs/2311.13721</a></li>
<li>Magicoder: Source Code Is All You Need, <a href="https://arxiv.org/abs/2312.02120">https://arxiv.org/abs/2312.02120</a></li>
</ol>
<h2>Credits</h2>
<p>Photo by <a href="https://unsplash.com/@markusspiske?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Markus Spiske</a> on <a href="https://unsplash.com/photos/turned-on-laptop-on-table-uPXs5Vx5bIg?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>.</p></section><div class="Post__PostFooter-oyq0rs-8 fJVWOL"><div class="Post__PostFooterWraper-oyq0rs-9 lhqqFe"><p class="Post__Author-oyq0rs-4 itWYlR">written by <!-- -->Florin Brad, Ioana Pintilie, Marius Dragoi, Dragos Tantaru</p></div></div></div></div></div></div></div><script type="text/javascript">window.__CSS_CHUNKS__ = {"main":"https://bit-ml.github.io/styles.2986121e.css"}</script><script type="text/javascript">
    window.__routeInfo = {"path":"blog/post/large-language-models-for-malware-analysis","templateID":2,"sharedPropsHashes":{"galleries":"ZHT1Kz"},"localProps":null,"allProps":{"post":{"data":{"slug":"large-language-models-for-malware-analysis","authors":"Florin Brad, Ioana Pintilie, Marius Dragoi, Dragos Tantaru","categories":"blog","featured_img":"/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg","date":"January-12-2024","title":"Large Language Models for Malware Analysis","id":0},"messages":[],"history":["./content/collections/posts/asm_llm2024.md","content/collections/posts/asm_llm2024.html"],"cwd":"/Users/fgogianu/Code/web/bit-ml","contents":"<h1>Large Language Models for Malware Analysis</h1>\n<p>Large Language Models (LLMs) took the world by storm in 2023, revolutionizing the way people search and generate text content. LLMs for code have also made inroads in helping people understand code or write code based on requests in natural language. For instance, translating requests to <a href=\"https://yale-lily.github.io/spider\">SQL queries</a> has rapidly advanced since the advent of GPT4.</p>\n<p>Most popular code LLMs focus on generating or understanding high-level programming languages such as Python, C++, Java etc. <a href=\"https://arxiv.org/abs/2308.12950\">[1]</a>,<a href=\"https://arxiv.org/abs/2305.06161\">[2]</a>,<a href=\"https://arxiv.org/abs/2306.08568\">[3]</a>,<a href=\"https://arxiv.org/abs/2308.07124\">[4]</a>. However code LLMs tailored to malware analysis should be adapted to better understand assembly code as well. Recent works also explore this avenue <a href=\"https://arxiv.org/abs/2312.09601\">[5]</a>,<a href=\"https://arxiv.org/abs/2310.16853\">[6]</a>,<a href=\"https://arxiv.org/abs/2311.13721\">[7]</a>.</p>\n<p>To this end we start from existing general-purpose LLMs such as <a href=\"https://huggingface.co/mosaicml/mpt-7b\">MPT7B</a>, which has been trained on both English text and code. We adapt them to x86-64 assembly data, by finetuning them using the Causal Language Modeling task and the <a href=\"https://arxiv.org/abs/2106.09685\">LoRA</a> method. We then inspect these models to evaluate the quality of their embeddings and their generation capabilities. We call these models <strong>asmLLMs</strong>.</p>\n<h2>asmLLMs for feature extraction</h2>\n<p>Features learned by pretrained models are crucial for downstream tasks such as anomaly detection, search or classification. Most LLMs (either general purpose or code-based) have little assembly data in their pretraining corpus relative to the entire corpus. Finetuning LLMs on more assembly data should then result in better features. To test this, we developed several downstream classification tasks, where the input is assembly code. Classifiers trained on features from the <strong>asmLLM</strong> should then outperform classifiers trained on features from base LLMs.</p>\n<p>To build examples for our tasks, we took C++ solutions from the code contest dataset <a href=\"https://arxiv.org/pdf/2105.12655v2.pdf\">CodeNet 1000</a> and converted them to assembly. The three classification tasks are:</p>\n<ul>\n<li><em>Complexity prediction</em>, where the input is the assembly corresponding to a C++ function, and the label is the <a href=\"https://manpages.ubuntu.com/manpages/focal/man1/pmccabe.1.html\">cyclomatic complexity</a> of that function (13 classes)</li>\n<li><em>Problem identification</em>, where the input is the assembly corresponding to a C++ submission to a code contest, and the label is the ID of the contest problem (20 classes)</li>\n<li><em>Accuracy prediction</em>, where the input is the assembly corresponding to a C++ submission to a code contest, and the label is the score achieved by the submission on the benchmark. Instead of predicting the score, the model predicts a class from a set of 5 classes, where each class corresponds to different bins of performance: [0-20) score, [20-40) score, ..., [80-100].</li>\n</ul>\n<p><img src=\"/galleries/asm_llm2024/asmllm_downstream.png\" alt=\"asm_down\"></p>\n<p>The asmLLM in <a href=\"asm_down\">Figure 1</a> is an MPT7B-base finetuned on assembly data using sequences of length 2K. To embed an input, it is first split into chunks of length 2K or 4K tokens. The input embedding is then computed as the average over these chunk embeddings. Results show that classifiers benefit from features from asmLLMs, especially when using larger contexts (4K tokens vs 2K tokens).</p>\n<h2>asmLLMs for generative tasks</h2>\n<h3>Code repair</h3>\n<p>While <strong>asmLLMs</strong> can be used as feature extractors, we are also interested in generative settings such as code-to-code or code-to-text. A typical code-to-code task is code repair, where given a faulty piece of code, the correct version of the code is produced. To test this on assembly, we generate a synthetic dataset where we alter a percentage of the assembly tokens, by modifying instructions (e.g. <strong>mov</strong> -> <strong>lea</strong>), registers (<strong>rax</strong> -> <strong>rsp</strong>) or offsets. We then perform instruction tuning on the asmLLM using (altered assembly, correct assembly) pairs and a 4K token context.</p>\n<p><img src=\"/galleries/asm_llm2024/asmllm_code_repair.png\" alt=\"asm_repair\"></p>\n<p>We notice in Figure 2 that the instruction tuned asmLLM model can correctly predict altered instructions (<strong>movzx</strong> -> <strong>mov</strong>) or registers (<strong>ch</strong>-><strong>rax</strong>).</p>\n<h3>Code summarization</h3>\n<p>To inspect the code of executable files, malware analysts use a wide array of tools, including powerful decompilers and disassemblers. While top performing LLMs such as GPT-4 are good at explaining high-level languages such as Python or C++, they are less likely to get the big picture of assembly code and instead describe it line by line usually. We are thus interested in the code summarization task, where explaining large chunks of assembly code can help analysts delve into binaries more efficiently.</p>\n<p>For this task, we generate a synthetic instruction tuning dataset called OSS-ASM, which consists of (assembly code, explanation) pairs. We first use the OSS-instruct<a href=\"https://arxiv.org/pdf/2312.02120.pdf\">[8]</a> method to produce (C++ code, explanation) pairs by seeding LLMs with assembly snippets from various technical resources. We then convert the C++ code to x86-64 assembly.</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Rouge-L</th>\n<th>BLEU</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GPT-3.5 turbo</td>\n<td>23.22</td>\n<td>2.85</td>\n</tr>\n<tr>\n<td>GPT-4</td>\n<td>26.56</td>\n<td>4.03</td>\n</tr>\n<tr>\n<td><a href=\"https://huggingface.co/mosaicml/$mpt-7b\">MPT7B-base</a> + instruction tuning OSS-ASM</td>\n<td>33.22</td>\n<td>11.89</td>\n</tr>\n<tr>\n<td><a href=\"https://huggingface.co/mosaicml/mpt-7b-instruct\">MPT7B-instruct</a> + instruction tuning OSS-ASM</td>\n<td>33.92</td>\n<td>12.29</td>\n</tr>\n<tr>\n<td><strong>asmLLM</strong> + instruction tuning OSS-ASM</td>\n<td><strong>35.72</strong></td>\n<td><strong>13.65</strong></td>\n</tr>\n</tbody>\n</table>\n<p>The <strong>asmLLM</strong> model in the table above is a MPT7B-base, finetuned on a subset of 500M tokens of x86-64 assembly, with LoRA rank r=64 and context length L=4096 tokens. GPT-3.5 and GPT-4 are tested in a zero-shot setup, while the other three models are instruction tuned on the OSS-ASM dataset. Results show that adapting base LLMs on assembly data improves the performance on downstream generative tasks such as code summarization.</p>\n<p>Along with results in Figure 1, this highlights the importance of pretraining LLMs on domain-specific data, particularly when dealing with languages that are less represented in large training sets, such as assembly.</p>\n<p><strong>asmLLMs</strong> can become an essential item in a malware analysis toolkit, accelerating the inspection of binaries and improving the threat response time. Future work involves learning across different computer architectures, increasing the diversity of the instruction tuning data and tackling obfuscated code.</p>\n<h2>References</h2>\n<ol>\n<li>Code Llama: Open Foundation Models for Code, <a href=\"https://arxiv.org/abs/2308.12950\">https://arxiv.org/abs/2308.12950</a></li>\n<li>StarCoder: may the source be with you!, <a href=\"https://arxiv.org/abs/2305.06161\">https://arxiv.org/abs/2305.06161</a></li>\n<li>WizardCoder: Empowering Code Large Language Models with Evol-Instruct, <a href=\"https://arxiv.org/abs/2306.08568\">https://arxiv.org/abs/2306.08568</a></li>\n<li>OctoPack: Instruction Tuning Code Large Language Models, <a href=\"https://arxiv.org/abs/2308.07124\">https://arxiv.org/abs/2308.07124</a></li>\n<li>Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models, <a href=\"https://arxiv.org/abs/2312.09601\">https://arxiv.org/abs/2312.09601</a></li>\n<li>CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code, <a href=\"https://arxiv.org/abs/2310.16853\">https://arxiv.org/abs/2310.16853</a>,</li>\n<li>Nova+: Generative Language Models for Binaries, <a href=\"https://arxiv.org/abs/2311.13721\">https://arxiv.org/abs/2311.13721</a></li>\n<li>Magicoder: Source Code Is All You Need, <a href=\"https://arxiv.org/abs/2312.02120\">https://arxiv.org/abs/2312.02120</a></li>\n</ol>\n<h2>Credits</h2>\n<p>Photo by <a href=\"https://unsplash.com/@markusspiske?utm_content=creditCopyText&#x26;utm_medium=referral&#x26;utm_source=unsplash\">Markus Spiske</a> on <a href=\"https://unsplash.com/photos/turned-on-laptop-on-table-uPXs5Vx5bIg?utm_content=creditCopyText&#x26;utm_medium=referral&#x26;utm_source=unsplash\">Unsplash</a>.</p>\n"},"galleries":{"asm_llm2024":[{"src":"/galleries/asm_llm2024/asmllm_code_repair.png","width":2570,"height":370},{"src":"/galleries/asm_llm2024/asmllm_downstream.png","width":1255,"height":769},{"src":"/galleries/asm_llm2024/markus-spiske-uPXs5Vx5bIg-unsplash-min.jpeg","width":5760,"height":3840}],"atari_agents_2022":[{"src":"/galleries/atari_agents_2022/collage.png","width":960,"height":1300},{"src":"/galleries/atari_agents_2022/dqn_modern_pytorch_vs_dopamine.png","width":1022,"height":367},{"src":"/galleries/atari_agents_2022/octocat.svg","width":16,"height":16},{"src":"/galleries/atari_agents_2022/rliable_comparison.png","width":875,"height":763},{"src":"/galleries/atari_agents_2022/sampling.png","width":1348,"height":164}],"bgv_scheme2023":[{"src":"/galleries/bgv_scheme2023/fhe_add.drawio.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_add_tiny.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_mul.drawio.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_mul_tiny.png","width":392,"height":307},{"src":"/galleries/bgv_scheme2023/fhe_noise.png","width":390,"height":412},{"src":"/galleries/bgv_scheme2023/fhe_noise_tiny.png","width":390,"height":412},{"src":"/galleries/bgv_scheme2023/fhe_simple.drawio.png","width":538,"height":161},{"src":"/galleries/bgv_scheme2023/fhe_simple_tiny.png","width":538,"height":161},{"src":"/galleries/bgv_scheme2023/lock.jpg","width":1200,"height":630},{"src":"/galleries/bgv_scheme2023/poly.drawio.png","width":342,"height":346},{"src":"/galleries/bgv_scheme2023/poly_tiny.png","width":342,"height":346},{"src":"/galleries/bgv_scheme2023/sec_game.drawio.png","width":350,"height":244},{"src":"/galleries/bgv_scheme2023/sec_game_tiny.png","width":350,"height":244}],"courses":[{"src":"/galleries/courses/atari_collage.png","width":960,"height":1300},{"src":"/galleries/courses/campus.jpg","width":725,"height":1080},{"src":"/galleries/courses/overview_bitdefender.jpg","width":1959,"height":2813},{"src":"/galleries/courses/overview_crypto.jpg","width":1920,"height":960},{"src":"/galleries/courses/overview_fmi.jpg","width":768,"height":600},{"src":"/galleries/courses/overview_pm.png","width":1194,"height":1190},{"src":"/galleries/courses/thumb_bitdefender.png","width":128,"height":128},{"src":"/galleries/courses/thumb_precis.jpg","width":128,"height":128},{"src":"/galleries/courses/thumb_rosedu.png","width":360,"height":360},{"src":"/galleries/courses/thumb_unibuc.png","width":512,"height":512}],"deep_bit_2019_pictures":[{"src":"/galleries/deep_bit_2019_pictures/20190731_150323.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_150331.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_153111.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_153118.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_164741.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/20190731_164747.jpg","width":4032,"height":1960},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142509.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142514.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142521.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142523_1.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_142925.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_144034.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145138.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145146.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145644.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145647.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145650.jpg","width":4160,"height":3120},{"src":"/galleries/deep_bit_2019_pictures/IMG_20190731_145654.jpg","width":4160,"height":3120}],"deep_bit_2019_posters":[{"src":"/galleries/deep_bit_2019_posters/poster_alin_passwords.png","width":4965,"height":7021},{"src":"/galleries/deep_bit_2019_posters/poster_bogdan_train_with_private_data.png","width":7200,"height":5400},{"src":"/galleries/deep_bit_2019_posters/poster_cristi_stegano.png","width":2480,"height":3507},{"src":"/galleries/deep_bit_2019_posters/poster_elena+adi_adversarial.png","width":4965,"height":7021},{"src":"/galleries/deep_bit_2019_posters/poster_horia+alin_homoglyph.png","width":6300,"height":4875},{"src":"/galleries/deep_bit_2019_posters/poster_mada_crypto_nets.png","width":4965,"height":7021},{"src":"/galleries/deep_bit_2019_posters/poster_mircescu_MRI.png","width":2480,"height":3507},{"src":"/galleries/deep_bit_2019_posters/poster_novac_logs_anomalies.png","width":4965,"height":7021},{"src":"/galleries/deep_bit_2019_posters/poster_tudor_fake_news.png","width":4965,"height":7021},{"src":"/galleries/deep_bit_2019_posters/poster_vlad_mouse_auth.png","width":4965,"height":7021}],"deep_fmi_2019_posters":[{"src":"/galleries/deep_fmi_2019_posters/10. EmoContext_Poster.png","width":4965,"height":7021},{"src":"/galleries/deep_fmi_2019_posters/10. Image_Segmentation.png","width":4965,"height":7021},{"src":"/galleries/deep_fmi_2019_posters/10. Landmark Recognition _ Visualization.png","width":4957,"height":7017},{"src":"/galleries/deep_fmi_2019_posters/10. Political Orientation Classification.png","width":7200,"height":5400},{"src":"/galleries/deep_fmi_2019_posters/10. Poster_VDSR.png","width":3507,"height":4957},{"src":"/galleries/deep_fmi_2019_posters/2048_RL.png","width":4957,"height":7017},{"src":"/galleries/deep_fmi_2019_posters/A LEARNED REPRESENTATION FOR ARTISTIC STYLE.png","width":1275,"height":1650},{"src":"/galleries/deep_fmi_2019_posters/Emotion_Detection.png","width":4500,"height":6300},{"src":"/galleries/deep_fmi_2019_posters/Emotion_Detection_CNN.png","width":4965,"height":7021},{"src":"/galleries/deep_fmi_2019_posters/Google Landmark Recognition Challenge.png","width":1755,"height":2480},{"src":"/galleries/deep_fmi_2019_posters/Image Classification.png","width":1275,"height":1650},{"src":"/galleries/deep_fmi_2019_posters/Image Semantic Segmentation.png","width":4957,"height":7017},{"src":"/galleries/deep_fmi_2019_posters/Image_Colorization.png","width":2478,"height":3507},{"src":"/galleries/deep_fmi_2019_posters/PosterRL.png","width":4965,"height":7021},{"src":"/galleries/deep_fmi_2019_posters/Revising Bi-Axial LSTM for Music Generation.png","width":4957,"height":7017},{"src":"/galleries/deep_fmi_2019_posters/Semantic Segmantation.png","width":8400,"height":8400},{"src":"/galleries/deep_fmi_2019_posters/Style Transfer.png","width":2480,"height":3300},{"src":"/galleries/deep_fmi_2019_posters/poster.png","width":1275,"height":1650}],"eeml2019_pictures":[{"src":"/galleries/eeml2019_pictures/andrei.jpg","width":1590,"height":1060},{"src":"/galleries/eeml2019_pictures/bitdefender_booth.jpg","width":1710,"height":1140},{"src":"/galleries/eeml2019_pictures/ema.jpg","width":1394,"height":930},{"src":"/galleries/eeml2019_pictures/florin.jpg","width":1152,"height":2048},{"src":"/galleries/eeml2019_pictures/florin_brad.jpg","width":928,"height":1392},{"src":"/galleries/eeml2019_pictures/full_house.jpg","width":1392,"height":928},{"src":"/galleries/eeml2019_pictures/group.jpg","width":1939,"height":1293},{"src":"/galleries/eeml2019_pictures/volunteers.jpg","width":1392,"height":928},{"src":"/galleries/eeml2019_pictures/z_ema_poster.png","width":1449,"height":2048},{"src":"/galleries/eeml2019_pictures/z_florin_poster.png","width":1698,"height":2400},{"src":"/galleries/eeml2019_pictures/z_iulia_andrei_poster.png","width":1772,"height":2496}],"eeml2019_posters":[{"src":"/galleries/eeml2019_posters/z_ema_poster.png","width":1449,"height":2048},{"src":"/galleries/eeml2019_posters/z_florin_poster.png","width":1698,"height":2400},{"src":"/galleries/eeml2019_posters/z_iulia_andrei_poster.png","width":1772,"height":2496}],"homomorphic2020":[{"src":"/galleries/homomorphic2020/alice.png","width":1000,"height":339},{"src":"/galleries/homomorphic2020/arithmetic_circuit.png","width":408,"height":382},{"src":"/galleries/homomorphic2020/dec_details.png","width":676,"height":312},{"src":"/galleries/homomorphic2020/enigma_dec.jpg","width":800,"height":1200},{"src":"/galleries/homomorphic2020/eval_mul.png","width":569,"height":460},{"src":"/galleries/homomorphic2020/eval_sum.png","width":567,"height":441},{"src":"/galleries/homomorphic2020/he_scheme5.png","width":1354,"height":390},{"src":"/galleries/homomorphic2020/noisy_cyphertext.png","width":658,"height":449}],"private_set2021":[{"src":"/galleries/private_set2021/monolith.jpg","width":800,"height":1200}],"rstg_pictures":[{"src":"/galleries/rstg_pictures/RSTG_model.png","width":2347,"height":1031},{"src":"/galleries/rstg_pictures/chart.svg","width":600,"height":371},{"src":"/galleries/rstg_pictures/digits.gif","width":692,"height":337},{"src":"/galleries/rstg_pictures/featured_graph_nets.jpg","width":1455,"height":1200},{"src":"/galleries/rstg_pictures/featured_graph_nets_bk.jpg","width":2784,"height":1856},{"src":"/galleries/rstg_pictures/interaction_videos.gif","width":1079,"height":258},{"src":"/galleries/rstg_pictures/rstg.gif","width":1353,"height":517},{"src":"/galleries/rstg_pictures/rstg_stages.gif","width":600,"height":565},{"src":"/galleries/rstg_pictures/smt-smt2.gif","width":725,"height":251}],"tmlss2018_pictures":[{"src":"/galleries/tmlss2018_pictures/dana.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/elena.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/florin.jpg","width":810,"height":1080},{"src":"/galleries/tmlss2018_pictures/iulia.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/lecture_hall.jpg","width":1920,"height":992},{"src":"/galleries/tmlss2018_pictures/salina.jpg","width":1920,"height":1280},{"src":"/galleries/tmlss2018_pictures/salina_1.jpg","width":1920,"height":1280},{"src":"/galleries/tmlss2018_pictures/stefan.jpg","width":1920,"height":1282},{"src":"/galleries/tmlss2018_pictures/stefan_1.jpg","width":1599,"height":899}],"tmlss2018_posters":[{"src":"/galleries/tmlss2018_posters/Bitdefender+NLP2SQL_1440.jpg","width":1440,"height":3558},{"src":"/galleries/tmlss2018_posters/ElenaBurceanu_Tracking_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/EmanuelaHaller_Segmentation_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/IuliaDuta_AndreiNicolicioiu_Video2NLP_1440.jpg","width":1440,"height":2036},{"src":"/galleries/tmlss2018_posters/StefanPostavaru_RMSProp_1440.jpg","width":1440,"height":2036}]}},"siteData":{"title":"Bitdefender Machine Learning & Crypto Research Unit","description":"Bitdefender Machine Learning & Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&ML scene by supporting and participating in local conferences, lecture and research groups.","tagline":"Engaging with the broader Machine Learning Community.","tags":["machine-learning","research","bitdefender"]}};</script><script defer="" type="text/javascript" src="https://bit-ml.github.io/bootstrap.70ac8375.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/templates/src/pages/Post.5cee3c5c.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/main.92808e2a.js"></script></body></html>