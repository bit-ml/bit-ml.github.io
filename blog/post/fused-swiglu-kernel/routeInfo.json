{"path":"blog/post/fused-swiglu-kernel","templateID":2,"sharedPropsHashes":{"galleries":"2gHC7O"},"localProps":{"post":{"data":{"slug":"fused-swiglu-kernel","authors":"Dragos Tantaru","categories":"blog","featured_img":"/galleries/fused_swiglu2025/gpu2.jpeg","date":"February-05-2025","title":"Towards Fused Kernels for Gated MLP","id":4},"messages":[],"history":["./content/collections/posts/fused_swiglu2025.md","content/collections/posts/fused_swiglu2025.html"],"cwd":"/Users/fgogianu/Code/web/bit-ml","contents":"<h1>Towards Fused Kernels for Gated MLP</h1>\n<p>The decoder block of a Transformer is the basic unit of all modern LLMs. Most of the compute used for it is spent on self-attention and the MLP, with self-attention in special being problematic on long sequences due to its quadratic compute and memory requirements. It is not surprising therefore that there's been a lot of progress towards increasing the performance of self-attention, such as FlashAttention [<a href=\"#fa\">1</a>], or algorithms and models that approximate full attention, like Window Attention [<a href=\"#wa\">2</a>], or State-Space Models [<a href=\"#mam\">3</a>, <a href=\"#lru\">4</a>, <a href=\"#mam2\">5</a>]. While efficient kernels for MLPs do exist, from what we could find they seem to be either tailored to very specific setups, or only partially solve some of the issues of MLPs, such as fusing the gating operation.</p>\n<p>We spent the last few weeks working on a kernel that computes the up-scaling and gating part of the MLP in a single (fused) call. In this blog post, we will explain our approach and dive into some low-level details of our implementation. While having some familiarity with GPU kernels will make reading this blog easier, we include some introductory sections that give a high-level overview of relevant concepts. The full implementation can be found at our GitHub <a href=\"https://github.com/bit-ml/Fused-SwiGLU\">repo</a>.</p>\n<h2>Gated MLPs</h2>\n<hr>\n<p>Gated MLPs, introduced in [<a href=\"#glu\">6</a>], changed the activation used in the traditional MLP block from an element-wise nonlinearity to a gated linear unit (GLU). As seen in the below image, this adds more computation and memory usage, as it requires an extra up-scaling of the input.</p>\n<div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/draft3.drawio.svg\" alt=\"gated mlp\" style=\"max-width: 600px; height: auto;\">\n    <p>\n        The thin rectangles are activation functions.\n    </p>\n</div>\n<p>This requires projecting the input tokens into a much larger dimension. For example, for Llama 405B the inputs are projected from <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi><mi>e</mi><mi>q</mi><mo>×</mo><mn>16384</mn></mrow><annotation encoding=\"application/x-tex\">Seq \\times 16384</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span><span class=\"mord\">3</span><span class=\"mord\">8</span><span class=\"mord\">4</span></span></span></span></span> to <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi><mi>e</mi><mi>q</mi><mo>×</mo><mn>53248</mn></mrow><annotation encoding=\"application/x-tex\">Seq \\times 53248</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">3</span><span class=\"mord\">2</span><span class=\"mord\">4</span><span class=\"mord\">8</span></span></span></span></span>, where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi><mi>e</mi><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">Seq</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span> is the sequence length. This in turn means that when training with sequences with less than <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>53248</mn></mrow><annotation encoding=\"application/x-tex\">53248</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">3</span><span class=\"mord\">2</span><span class=\"mord\">4</span><span class=\"mord\">8</span></span></span></span></span> context length, the MLP will dominate self-attention in terms of FLOPs, because the quadratic cost in terms of context length is actually lower. In general, this holds for most sizes of LLMs, as even smaller models in the 8B class usually upscale to the 14k-20k range, which exceeds most context sizes used for pretraining. When paired with efficient attention kernels, this can also lead to the MLP utilising most activation memory during training.</p>\n<p>For inference, the discussion is more nuanced. Inference engines tend to prioritize, among other things, storing as little memory as possible for activations to make room for larger models and KV caches. We will have a section towards the end of the post where we will discuss in more detail how exactly our kernel might fit in a modern inference engine, but briefly speaking, we expect the impact of our kernel to be modest in terms of memory, but still useful for further improving the throughput of model deployments.</p>\n<h2>GPU kernels</h2>\n<hr>\n<p>\"Kernels\" is the name given in GPU programming to functions that are executed in parallel on a GPU. More formally, \"in parallel\" refers to the <strong>S</strong>ingle <strong>I</strong>nstruction <strong>M</strong>ultiple <strong>T</strong>hreads (SIMT) model, in which groups of threads (called warps for Nvidia chips) execute the same instruction in parallel, while operating on separate slices of data.</p>\n<p>One classic example is to think of a function that grayscales an image. We can  write a kernel that computes the color for a single pixel, and then run it on the GPU to transform the entire image in parallel. A kernel will have an associated Cooperative Thread Array (CTA), also called threadblock, which generally consists of multiple warps. Each warp will handle a patch of the original image. Since all the threads in a warp run in parallel, even if one single thread isn't particularly fast, we can still obtain high throughput from the parallel nature of execution. More so, all warps are also launched in parallel, although the degree to which they can execute in parallel is a much more nuanced discussion than in the case of threads in a warp <sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup> (suffice to say there is still a very large amount of parallelism).</p>\n<p>While this explanation glosses over a lot of important details, we believe it is enough to follow the high-level overview of our kernel.</p>\n<h3>Kernel fusion</h3>\n<div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/kernel_look.png\" alt=\"fused kernel\" style=\"max-width: 600px; height: auto;\">\n    <p>\n        Each small rectangle is a patch handled by a launched kernel. For each kernel execution example, all patches are launched in parallel.\n    </p>\n</div>\n<p>Broadly speaking, kernel fusion refers to implementing kernels that do more work by \"fusing\" together multiple kernels. To expand on the grayscale example, imagine we would additionally have a kernel that thresholds all pixels with a value smaller than a constant. If we fused them, we would apply the thresholding on the grayscaled pixel in our code, and write the final result in one function call. If we launched the grayscale kernel, waited for it to finish, and then launched the threshold kernel on the grayscaled image, we would lose throughput for multiple reasons:</p>\n<ul>\n<li>We would load the entire image once for the grayscale kernel, store it, then load it again for the thresholding, and store the final result. The load/store operations are much slower on hardware than the actual computation that we do.</li>\n<li>We double the memory usage by storing the intermediate results for grayscaling.</li>\n<li>Kernel launches themselves add overhead. If your code has to launch many short kernels, this can impact performance.</li>\n</ul>\n<h3>Kernels and PyTorch</h3>\n<p>When writing PyTorch programs, the default mode of execution is <em>eager</em>, which means that all functions are immediately executed when they are encountered in code, with no automated optimizations. Consider this simple PyTorch script:</p>\n<div class=\"remark-highlight\"><pre class=\"language-python\"><code>x<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>torch<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x<span class=\"token space\"> </span><span class=\"token operator\">*</span><span class=\"token space\"> </span>scale<span class=\"token punctuation\">)</span><span class=\"token space\"> </span><span class=\"token operator\">**</span><span class=\"token space\"> </span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n</code></pre></div>\n<p>It might be surprising if you are not familiar with the details of <em>eager</em> mode to find out that this line of code will result in 3 memory allocations<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup> of the size of <code>x</code> and 3 kernel launches. The multiplication by <code>scale</code>, the squaring and the exponentiation will each result in one kernel call and memory allocation.</p>\n<h4>Torch.compile</h4>\n<p>While <em>eager</em> execution has its advantages, it is often wasteful in terms of both memory and compute. This is the reason tools like <code>torch.compile</code> have been developed, to offer an automated way to fuse kernels in PyTorch code. Torch compilation is definitely a powerful tool that can significantly speed up your code. That being said, certain optimizations are still out of reach for current compilers, such as FlashAttention. It seems that fused gated MLPs are currently also part of this class of optimizations, since we did not measure any improvements over the eager <code>cuBLAS+Unsloth</code> (we will clarify what this refers to in the Benchmarking section) approach when compiling the PyTorch gating module in our repo.</p>\n<h2>Our approach</h2>\n<hr>\n<p>In this section, we will give a high-level overview of the solution we came with to fuse some of the MLP computation. To recap, a Transformer MLP might look like this in PyTorch code:</p>\n<div class=\"remark-highlight\"><pre class=\"language-python\"><code><span class=\"token keyword\">def</span><span class=\"token space\"> </span><span class=\"token function\">mlp</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">:</span><span class=\"token space\"> </span>torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>weights_upscale<span class=\"token punctuation\">:</span><span class=\"token space\"> </span>torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>weights_downscale<span class=\"token punctuation\">:</span><span class=\"token space\"> </span>torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>weights_gate<span class=\"token punctuation\">:</span><span class=\"token space\"> </span>torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>act_fn<span class=\"token punctuation\">:</span><span class=\"token space\"> </span>Callable<span class=\"token punctuation\">)</span><span class=\"token space\"> </span><span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span><span class=\"token space\"> </span>torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">:</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>up_scaled_x<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>x<span class=\"token space\"> </span>@<span class=\"token space\"> </span>weights_upscale<span class=\"token space\"> </span><span class=\"token comment\">#<span class=\"token space\"> </span>linear<span class=\"token space\"> </span>projection</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>gate_values<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>x<span class=\"token space\"> </span>@<span class=\"token space\"> </span>weights_gate<span class=\"token space\"> </span><span class=\"token comment\">#<span class=\"token space\"> </span>linear<span class=\"token space\"> </span>projection</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>gated_up_scale<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>up_scaled_x<span class=\"token space\"> </span><span class=\"token operator\">*</span><span class=\"token space\"> </span>act_fn<span class=\"token punctuation\">(</span>gate_values<span class=\"token punctuation\">)</span><span class=\"token space\"> </span><span class=\"token comment\">#<span class=\"token space\"> </span>element-wise<span class=\"token space\"> </span>product<span class=\"token space\"> </span>with<span class=\"token space\"> </span>gate<span class=\"token space\"> </span>values<span class=\"token space\"> </span>passed<span class=\"token space\"> </span>through<span class=\"token space\"> </span>activation</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>result<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>gated_up_scale<span class=\"token space\"> </span>@<span class=\"token space\"> </span>weights_downscale<span class=\"token space\"> </span><span class=\"token comment\">#<span class=\"token space\"> </span>linear<span class=\"token space\"> </span>downscaling</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">return</span><span class=\"token space\"> </span>result\n</code></pre></div>\n<p>What we have currently worked on are the 2-4 lines of code. The algorithm can be summarized as follows:</p>\n<ol>\n<li>Concatenate the upscaling weights and gating weights on the second dimension (i.e. add extra columns), such that we can compute both the up scaled <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span> and the gating values in one matrix multiplication. This is a standard optimization used in many LLM projects. The main difference is that we interleave columns, with even positions corresponding to upscaling weights and odd positions to gate weights. This is helpful, because our kernel can only compute a small patch of both the up-scaled weights and the gating values. By grabbing the odd and even columns, we can compute the local gated patch.</li>\n<li>Compute the GEMM (General Matrix Multiplication). It is very important for our approach to be able to directly work on the result of the matrix multiplication and manipulate its shape, therefore we cannot use highly efficient, but closed source kernels like cuBLAS. For this purpose, we wrote a custom GEMM kernel.</li>\n<li>Each CTA now holds a patch of the final linear projection. To compute the gated linear unit (line 4), we take all odd columns from the patch, apply the activation function on them, and multiply them by the even columns.</li>\n<li>We write the result to <code>gated_up_scale</code>.</li>\n</ol>\n<p>By doing this, we only need enough memory to store a Tensor the size of <code>gated_up_scale</code>, and we fuse the inefficient gating multiplication in the GEMM kernel. This cuts the memory usage by more than two thirds compared to our PyTorch code above. Before looking at some benchmarks, let's quickly discuss other existing solution for efficient MLPs.</p>\n<h3>Triton kernels</h3>\n<p>Several Triton kernels for the fourth line of code in our <code>mlp</code> function are available, such as Liger kernels [<a href=\"#lig\">7</a>] or UnslothAI [<a href=\"#uns\">8</a>]. While certainly much more efficient than just eager PyTorch code, they still require extra memory for storing the <code>up_scaled_x</code> and <code>gate_values</code>. We can save some memory by writing <code>gated_up_scale</code> over either of these two Tensors, but that is still twice as much memory as our approach. Nevertheless, in our benchmarks we use a cuBLAS kernel for computing the matrix multiplications, and Unsloth kernels for the activation, as they are otherwise highly efficient.</p>\n<p>Furthermore, in <a href=\"https://github.com/fattorib/fusedswiglu/tree/main\">this repo</a> there are Triton fused kernels for both the forward and backward passes of the GEMM and gate operation.</p>\n<h3>CUDA kernels</h3>\n<p>A similar approach to our gated MLP kernel exists in the TensorRT-LLM repo <a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/d93a2dde84eada06ae2339b4fb4e6432167a1cfd/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/sm90_gemm_gated_tma_warpspecialized_pingpong.hpp\">here</a>. At a high-level the kernels are conceptually similar, in that they both fuse the gating computation with the GEMM. The similarities stop here, as the one from TensorRT-LLM is written and very optimized for the Hopper architecture, and used only for FP8 MLPs. The fusing itself also does not use the odd-even column scheme we employ, instead they do two separate GEMMs for the weights. In any case, we see this as a confirmation that fusing these kernels is a promising approach. We did not benchmark against their kernel, because our code is written for Ampere, since that is the hardware available for us (A100).</p>\n<p>Additionally, the <a href=\"https://github.com/facebookresearch/xformers\"><code>xformers</code></a> library includes a fused implementation that utilizes the <code>DualGemm</code> class from the Nvidia <a href=\"https://github.com/NVIDIA/cutlass\">CUTLASS</a> library.</p>\n<h2>Benchmarks</h2>\n<hr>\n<p>Before deep diving into low-level details, we wanted to show the performance of our kernel when compared to an optimized way of computing the MLP in PyTorch:</p>\n<style>\npre code {\n    white-space: pre-wrap !important;\n    word-break: break-word !important;\n}\n\n</style>\n<div class=\"remark-highlight\"><pre class=\"language-python\"><code><span class=\"token keyword\">def</span><span class=\"token space\"> </span><span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token comment\">#<span class=\"token space\"> </span>X<span class=\"token space\"> </span>and<span class=\"token space\"> </span>W<span class=\"token space\"> </span>are<span class=\"token space\"> </span>the<span class=\"token space\"> </span>inputs<span class=\"token space\"> </span>and<span class=\"token space\"> </span>concatenated<span class=\"token space\"> </span>weights</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>torch<span class=\"token punctuation\">.</span>mm<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>self<span class=\"token punctuation\">.</span>w<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>out<span class=\"token operator\">=</span>C<span class=\"token punctuation\">)</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token comment\">#<span class=\"token space\"> </span>self.N<span class=\"token space\"> </span>is<span class=\"token space\"> </span>2<span class=\"token space\"> </span>x<span class=\"token space\"> </span>D_up</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">return</span><span class=\"token space\"> </span>swiglu_fg_kernel<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token punctuation\">:</span>self<span class=\"token punctuation\">.</span>N<span class=\"token operator\">//</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>C<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>self<span class=\"token punctuation\">.</span>N<span class=\"token operator\">//</span><span class=\"token number\">2</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token space\"> </span>\n</code></pre></div>\n<p>The <code>torch.mm</code> call will call a <code>cuBLAS</code> optimized kernel, and <code>swiglu_fg_kernel</code> corresponds to the Unsloth kernel, with the modification that it overwrites the second argument with the result, to save on memory. We call this implementation <code>cuBLAS+Unsloth</code> throughout our blog, since it uses the <code>cuBLAS</code> GEMM kernel and the gate Triton kernel from UnslothAI. The code for the benchmarks is provided in the GitHub <a href=\"https://github.com/bit-ml/Fused-SwiGLU\">repo</a>. We use the Triton benchmarking module to measure the TFLOP/s achieved by the above code and our kernel. All benchmarks are run on an A100 80GB card, with bf16 precision for all tensors. We also report the results for the <a href=\"https://github.com/fattorib/fusedswiglu/tree/main\">Triton kernel</a> and <a href=\"https://github.com/facebookresearch/xformers\">xformers</a> implementations.</p>\n<div style=\"display: flex; justify-content: center; flex-wrap: wrap;\">\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Gated MLP for Llama 8B kernels.svg\" alt=\"Gated MLP for Llama 8B\" style=\"max-width: 330px; height: auto;\">\n  </div>\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Gated MLP for Llama 70B kernels.svg\" alt=\"Gated MLP for Llama 70B\" style=\"max-width: 330px; height: auto;\">\n  </div>\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Gated MLP for Llama 405B kernels.svg\" alt=\"Gated MLP for Llama 405B\" style=\"max-width: 330px; height: auto;\">\n  </div>\n    <p>\n        TFLOP/s for different sizes of Llama models and sequence lengths.\n    </p>\n</div>\n<div style=\"display: flex; justify-content: center; flex-wrap: wrap;\">\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Gated MLP for Llama 8B.svg\" alt=\"Gated MLP for Llama 8B\" style=\"max-width: 330px; height: auto;\">\n  </div>\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Gated MLP for Llama 70B.svg\" alt=\"Gated MLP for Llama 70B\" style=\"max-width: 330px; height: auto;\">\n  </div>\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Gated MLP for Llama 405B.svg\" alt=\"Gated MLP for Llama 405B\" style=\"max-width: 330px; height: auto;\">\n  </div>\n    <p>\n        TFLOP/s only for our kernel and cuBLAS+Unsloth.\n    </p>\n</div>\n<div style=\"display: flex; justify-content: space-around; flex-wrap: wrap;\">\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Memory Usage for Llama 8B.svg\" alt=\"Gated MLP for Llama 8B memory\" style=\"max-width: 300px; height: auto;\">\n  </div>\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Memory Usage for Llama 70B.svg\" alt=\"Gated MLP for Llama 70B memory\" style=\"max-width: 300px; height: auto;\">\n  </div>\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/Memory Usage for Llama 405B.svg\" alt=\"Gated MLP for Llama 405B memory\" style=\"max-width: 300px; height: auto;\">\n  </div>\n    <p>\n        Memory usage for different sizes of Llama models and sequence lengths.\n    </p>\n</div>\n<p>Our kernel underperforms in terms of TFLOP/s, achieving between 95-98% of the <code>cuBLAS+Unsloth</code> approach, but we can ease the memory pressure by half. This can result in elevated throughput for inference workloads, as the memory saved could be used for expanding the KV cache, for example.</p>\n<p>We also note that the difference in terms of compute is mostly explained by the difference in performance between our GEMM code and the extremely optimized <code>cuBLAS</code> kernel for the A100. Our claim is based on the observation that the performance gap between <code>cuBLAS</code> and our standalone GEMM code is generally larger than the gap between our complete kernel and the <code>cuBLAS+Unsloth</code> code. The table below shows the ratio between the TFLOP/s of our kernel GEMM code and <code>cuBLAS</code> for the GEMM column, and the Full column refers to the ratio of our complete kernel and <code>cuBLAS+Unsloth</code>. We see that for most model sizes and token counts the gap between the GEMM computations is higher, and in all cases the ratios are closely matched.</p>\n<div style=\"display: flex; justify-content: center; margin: 10px;\">\n  <table style=\"margin: auto; border-collapse: collapse;\">\n      <caption style=\"caption-side: top; font-weight: bold; margin-bottom: 8px; text-align: center;\">\n    Performance ratios across model sizes\n  </caption>\n  <thead>\n    <tr>\n      <th rowspan=\"2\" style=\"padding: 8px; border: 1px solid #000; vertical-align: middle;\">Tokens</th>\n      <th colspan=\"2\" style=\"padding: 8px; border: 1px solid #000; text-align: center;\">8B</th>\n      <th colspan=\"2\" style=\"padding: 8px; border: 1px solid #000; border-left: 3px solid #000; text-align: center;\">70B</th>\n      <th colspan=\"2\" style=\"padding: 8px; border: 1px solid #000; border-left: 3px solid #000; text-align: center;\">405B</th>\n    </tr>\n    <tr>\n      <th style=\"padding: 8px; border: 1px solid #000; text-align: center;\">Full</th>\n      <th style=\"padding: 8px; border: 1px solid #000; text-align: center;\">GEMM</th>\n      <th style=\"padding: 8px; border: 1px solid #000; text-align: center;border-left: 3px solid #000;\">Full</th>\n      <th style=\"padding: 8px; border: 1px solid #000; text-align: center;\">GEMM</th>\n      <th style=\"padding: 8px; border: 1px solid #000; text-align: center;border-left: 3px solid #000;\">Full</th>\n      <th style=\"padding: 8px; border: 1px solid #000; text-align: center;\">GEMM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">1024</td><td style=\"text-align: center;border: 1px solid #000;\">102.79%</td><td style=\"text-align: center;border: 1px solid #000;\">101.59%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">96.03%</td><td style=\"text-align: center;border: 1px solid #000;\">95.74%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.54%</td><td style=\"text-align: center;border: 1px solid #000;\">95.80%</td></tr>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">2048</td><td style=\"text-align: center;border: 1px solid #000;\">97.40%</td><td style=\"text-align: center;border: 1px solid #000;\">96.23%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">96.64%</td><td style=\"text-align: center;border: 1px solid #000;\">96.19%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.55%</td><td style=\"text-align: center;border: 1px solid #000;\">96.12%</td></tr>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">4096</td><td style=\"text-align: center;border: 1px solid #000;\">98.42%</td><td style=\"text-align: center;border: 1px solid #000;\">97.30%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">96.04%</td><td style=\"text-align: center;border: 1px solid #000;\">95.52%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.93%</td><td style=\"text-align: center;border: 1px solid #000;\">96.24%</td></tr>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">8192</td><td style=\"text-align: center;border: 1px solid #000;\">97.71%</td><td style=\"text-align: center;border: 1px solid #000;\">96.55%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.77%</td><td style=\"text-align: center;border: 1px solid #000;\">95.24%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.67%</td><td style=\"text-align: center;border: 1px solid #000;\">95.85%</td></tr>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">16384</td><td style=\"text-align: center;border: 1px solid #000;\">97.29%</td><td style=\"text-align: center;border: 1px solid #000;\">95.96%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.93%</td><td style=\"text-align: center;border: 1px solid #000;\">95.50%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.85%</td><td style=\"text-align: center;border: 1px solid #000;\">95.99%</td></tr>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">32768</td><td style=\"text-align: center;border: 1px solid #000;\">97.30%</td><td style=\"text-align: center;border: 1px solid #000;\">95.99%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">96.16%</td><td style=\"text-align: center;border: 1px solid #000;\">95.72%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.81%</td><td style=\"text-align: center;border: 1px solid #000;\">96.11%</td></tr>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">49152</td><td style=\"text-align: center;border: 1px solid #000;\">97.30%</td><td style=\"text-align: center;border: 1px solid #000;\">96.02%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">96.16%</td><td style=\"text-align: center;border: 1px solid #000;\">95.75%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.75%</td><td style=\"text-align: center;border: 1px solid #000;border: 1px solid #000;\">96.05%</td></tr>\n    <tr><td style=\"text-align: center;border: 1px solid #000;\">65536</td><td style=\"text-align: center;border: 1px solid #000;\">97.30%</td><td style=\"text-align: center;border: 1px solid #000;\">95.99%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">96.13%</td><td style=\"text-align: center;border: 1px solid #000;\">95.80%</td><td style=\"text-align: center;border: 1px solid #000; border-left: 3px solid #000;\">95.76%</td><td style=\"text-align: center;border: 1px solid #000;\">96.05%</td></tr>\n  </tbody>\n</table>\n</div>\n<p>In principle, any GEMM kernel for Ampere should be amenable to fuse the gated linear unit in the way we do in our code, since the <code>mma</code> instruction will result in similar shapes for the slices of data threads act on. Therefore, we see this as a positive result, in the fact that with more optimized GEMM code it should be possible to match or even exceed the performance of <code>cuBLAS+Unsloth</code> in terms of TFLOP/s. We will explain this in more detail in the low-level section of our blog.</p>\n<h3>Impact on model capabilities and correctness</h3>\n<p>To check the correctness of our model, we ran numerical tests where we measured the difference between the outputs of our kernel and the PyTorch code, using matrices initialized with Kaiming initialization, which is the default option for PyTorch weights. All of the tensors are kept in <code>bfloat16</code> precision. We use square matrices with the shape indicated by the <code>MNK</code> column. We run 100 iterations of random initializations to compute the mean and standard deviation of each statistic. We report the results in the following table:</p>\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center;border: 1px solid #000;\">MNK</th>\n      <th style=\"text-align: center;border: 1px solid #000;\">Max Abs Diff (Mean ± Std Dev)</th>\n      <th style=\"text-align: center;border: 1px solid #000;\" >Mean Abs Diff (Mean ± Std Dev)</th>\n      <th style=\"text-align: center;border: 1px solid #000;\">Relative Diff (Mean ± Std Dev)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">1024</td>\n      <td style=\"text-align: center;border: 1px solid #000;\">3.91e-06 ± 5.64e-07 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">8.08e-08 ± 2.96e-10 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">3.71e-03 ± 1.41e-05 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">2048</td>\n      <td style=\"text-align: center;border: 1px solid #000;\">2.02e-06 ± 4.11e-07 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">4.09e-08 ± 1.13e-10 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">3.74e-03 ± 6.35e-06 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">4096</td>\n      <td style=\"text-align: center;border: 1px solid #000;\">1.29e-06 ± 4.44e-07 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">2.08e-08 ± 1.16e-11 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">3.80e-03 ± 9.88e-06 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">8192</td>\n      <td style=\"text-align: center;border: 1px solid #000;\">7.25e-07 ± 2.13e-07 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">1.04e-08 ± 0.00e+00 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">3.86e-03 ± 4.34e-19 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">16384</td>\n      <td style=\"text-align: center;border: 1px solid #000;\">4.77e-07 ± 0.00e+00 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">4.98e-09 ± 0.00e+00 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">3.81e-03 ± 0.00e+00 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">32768</td>\n      <td style=\"text-align: center;border: 1px solid #000;\">2.72e-07 ± 6.10e-08 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">3.46e-09 ± 0.00e+00 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">4.34e-03 ± 0.00e+00 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">65536</td>\n      <td style=\"text-align: center;border: 1px solid #000;\">1.69e-07 ± 2.76e-08 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">1.68e-09 ± 0.00e+00 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\">4.22e-03 ± 0.00e+00 </td>\n    </tr>\n  </tbody>\n</table>\n<p>However, it is possible to degrade model performance even with small numerical errors, due to the compounding effects over the many layers models have. Therefore, we measure the performance of a Llama 8B model on three popular benchmarks, with and without our kernel.</p>\n<table style=\"margin: auto; border-collapse: collapse; margin-bottom: 10px;\">\n<caption style=\"caption-side: top; font-weight: bold; margin-bottom: 8px; text-align: center;\">\n    Benchmarks scores on a Llama 8B model\n  </caption>\n  <thead>\n    <tr>\n      <th style=\"text-align: center;border: 1px solid #000;\">Benchmark</th>\n      <th style=\"text-align: center;border: 1px solid #000;\">MLP kernel</th>\n      <th style=\"text-align: center;border: 1px solid #000;\">MLP eager</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\">MMLU-Pro (0-shot)</td>\n      <td style=\"text-align: center;border: 1px solid #000;\"> 44.37 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\"> 44.24 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\"> EvalPlus (0-shot)</td>\n      <td style=\"text-align: center;border: 1px solid #000;\"> 61.6 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\"> 60.4 </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;border: 1px solid #000;\"> GPQA (0-shot)</td>\n      <td style=\"text-align: center;border: 1px solid #000;\"> 33.71 </td>\n      <td style=\"text-align: center;border: 1px solid #000;\"> 31.92 </td>\n    </tr>\n  </tbody>\n</table>\n<p>We observe no degradation in terms of score across reasoning tasks, code and math problems, or the general knowledge required for some of the MMLU-Pro categories.</p>\n<h2>Inference</h2>\n<hr>\n<p>First of all, inference engines usually split the process of generating a response to a given request in 2 phases: <strong>prefilling</strong> and <strong>decoding</strong>. <strong>Prefilling</strong> is the first phase, in which the model receives the full user prompt. The prompt is passed through the entire model, and at each layer, all the keys and values are saved in the <em>KV cache</em>. The <strong>decoding</strong> phase refers to the process in which the model generates new tokens. After the first sampled token in the <strong>prefill</strong> phase, the model will only pass the currently generated token through the query projection and the MLP for each layer, as it can fetch the keys and values from the <em>KV cache</em> to perform self-attention.</p>\n<p>This would hint that our kernel would mostly be useful for the <strong>prefill</strong> phase, as that's when we can expect the peak memory usage for MLP activations. Considering <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi><mi>e</mi><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">Seq</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span> the sequence length of a prompt, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span> the dimension used for queries and keys, and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>D</mi><mrow><mi>u</mi><mi>p</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">D_{up}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span><span class=\"mord mathnormal mtight\">p</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span></span> the up-scaling dimension, a full decoder layer uses <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>⋅</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>×</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">4 \\cdot Seq \\times D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span> memory for self-attention (queries, keys, values and output) and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn><mo>⋅</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>×</mo><msub><mi>D</mi><mrow><mi>u</mi><mi>p</mi></mrow></msub><mo>+</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>×</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">3 \\cdot Seq \\times D_{up} + Seq \\times D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span><span class=\"mord mathnormal mtight\">p</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span> memory for the MLP (up-scale, gate values up-scale, gated result, down-scaled output). By using only <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi><mi>e</mi><mi>q</mi><mo>×</mo><msub><mi>D</mi><mrow><mi>u</mi><mi>p</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">Seq \\times D_{up}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span><span class=\"mord mathnormal mtight\">p</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span></span> memory for the first half of the MLP, we would reduce memory usage per-layer by a factor of around <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>25</mn><mi mathvariant=\"normal\">%</mi><mo>−</mo><mn>35</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">25\\%-35\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.08333em;\"></span><span class=\"mord\">2</span><span class=\"mord\">5</span><span class=\"mord\">%</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mord\">3</span><span class=\"mord\">5</span><span class=\"mord\">%</span></span></span></span></span><sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup> for activations, assuming an efficient attention kernel, such as FlashAttention. The saved memory could then be used for larger KV caches or models.</p>\n<p>On the other hand, modern inference engines also use <strong>chunked prefill</strong> to further improve latency. During chunked prefill, instead of splitting the computation in two phases, the model will always be served a chunk of tokens that can be either part of an user prompt, or used for decoding other requests. The advantage of this technique is that decoding is no longer bottlenecked by potentially very long prefills, as can be seen in the picture below, taken from an <a href=\"https://developer.nvidia.com/blog/streamlining-ai-inference-performance-and-deployment-with-nvidia-tensorrt-llm-chunked-prefill/\">Nvidia</a> blog.</p>\n<div style=\"display: flex; justify-content: space-around; flex-wrap: wrap;\">\n  <div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/chunk.png\" alt=\"chunked prefill\" style=\"max-width: 80%; height: auto;\">\n  </div>\n    <p>\n        Chunked prefill illustrated, from <a href=\"https://developer.nvidia.com/blog/streamlining-ai-inference-performance-and-deployment-with-nvidia-tensorrt-llm-chunked-prefill/\">Nvidia</a>.\n    </p>\n</div>\n<p>A side-effect of chunked prefill is that the memory used by activations drops even lower than in \"classic\" prefill-decode inference. As now the model will only be fed one chunk at a time, we only need activation memory for one chunk. Inference engines that implement chunked prefill use <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>256</mn></mrow><annotation encoding=\"application/x-tex\">256</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord\">5</span><span class=\"mord\">6</span></span></span></span></span> or <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">512</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">1</span><span class=\"mord\">2</span></span></span></span></span> tokens for a chunk, so we can actually exactly compute the amount of memory we save for a given model, per deployment.</p>\n<div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/savings.svg\" alt=\"kv cache savings\" style=\"max-width: 600px; height: auto;\">\n    <p>\n        The extra tokens are estimated by dividing the total saved memory by the required memory of a key and value for all layers of a model.\n    </p>\n</div>\n<p>We plot in these graphs the amount of tokens that we can further add to the <em>KV cache</em> when reducing the activation memory per chunk with a chunk size of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">512</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">1</span><span class=\"mord\">2</span></span></span></span></span>. This value is fixed per model deployment, since it only depends on the chunk size and model configuration. The stronger dependence on model configuration is also the reason why the graphs have irregular patterns. The gains are more modest in this scenario, given the small chunk sizes. Nonetheless, they can still help squeeze more value out of existing deployments. Notably, Gemma 2-27B has the highest up-scaling factor of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">8</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">8</span></span></span></span></span>, and we can see it leads to the largest potential increase in KV cache memory.</p>\n<h2>A closer look <a href id=\"close\"></a></h2>\n<hr>\n<p>So far, we kept the blog post at a reasonably high-level in terms of details. The next parts of the blog post are all dedicated to low-level details of our kernel that we think might be relevant, and at the very least interesting.</p>\n<h3>CuTe</h3>\n<p>CuTe is a collection of low-level templates from the Nvidia <a href=\"https://github.com/NVIDIA/cutlass\">CUTLASS</a> library that is very handy for writing kernels. We extensively use CuTe throughout our kernel, since it's very helpful with computing the shapes of various tensors used throughout a GEMM kernel.</p>\n<!-- There is some official documentation on the CUTLASS repo, but the resources to actually write kernels with acceptable performance are more scarce. For this reason, we\n -->\n<h4>Tiling with CuTe</h4>\n<p>One of the main things that CuTe simplifies is tiling tensors. For example, to obtain the tile of the A matrix that the current CTA (Cooperative Thread Array, also known as threadblock) acts on, one can write:</p>\n<div class=\"remark-highlight\"><pre class=\"language-cpp\"><code><span class=\"token comment\">//<span class=\"token space\"> </span>(M,K)<span class=\"token space\"> </span>row-major<span class=\"token space\"> </span>matrix<span class=\"token space\"> </span>in<span class=\"token space\"> </span>global<span class=\"token space\"> </span>memory</span><span class=\"token lf\">\n</span>Tensor<span class=\"token space\"> </span>mA<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token function\">make_tensor</span><span class=\"token punctuation\">(</span><span class=\"token function\">make_gmem_ptr</span><span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token function\">make_shape</span><span class=\"token punctuation\">(</span>M<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>K<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token function\">make_stride</span><span class=\"token punctuation\">(</span>K<span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token comment\">//<span class=\"token space\"> </span>_<span class=\"token space\"> </span>refers<span class=\"token space\"> </span>to<span class=\"token space\"> </span>selecting<span class=\"token space\"> </span>all<span class=\"token space\"> </span>indexes<span class=\"token space\"> </span>on<span class=\"token space\"> </span>the<span class=\"token space\"> </span>K<span class=\"token space\"> </span>dimension</span><span class=\"token lf\">\n</span><span class=\"token keyword\">auto</span><span class=\"token space\"> </span>cta_coord<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span>blockIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>blockIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token comment\">//<span class=\"token space\"> </span>tile<span class=\"token space\"> </span>shapes<span class=\"token space\"> </span></span><span class=\"token lf\">\n</span><span class=\"token keyword\">auto</span><span class=\"token space\"> </span>cta_tiler<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token function\">make_shape</span><span class=\"token punctuation\">(</span>bM<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>bN<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>bK<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token comment\">//<span class=\"token space\"> </span>selects<span class=\"token space\"> </span>bM<span class=\"token space\"> </span>and<span class=\"token space\"> </span>bK<span class=\"token space\"> </span>as<span class=\"token space\"> </span>tile<span class=\"token space\"> </span>dimensions,<span class=\"token space\"> </span>and<span class=\"token space\"> </span>uses<span class=\"token space\"> </span>cta_coord<span class=\"token space\"> </span>to<span class=\"token space\"> </span>select<span class=\"token space\"> </span>the<span class=\"token space\"> </span>rows<span class=\"token space\"> </span>and<span class=\"token space\"> </span>columns<span class=\"token space\"> </span>corresponding<span class=\"token space\"> </span>to<span class=\"token space\"> </span>this<span class=\"token space\"> </span>CTA</span><span class=\"token lf\">\n</span>Tensor<span class=\"token space\"> </span>gA<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token function\">local_tile</span><span class=\"token punctuation\">(</span>mA<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>cta_tiler<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>cta_coord<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>Step<span class=\"token operator\">&lt;</span>_1<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>X<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>_1<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token space\"> </span>\n</code></pre></div>\n<p>In the above example, <code>mA</code> corresponds to the large <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span></span> matrix and is initialized with a GMEM pointer and the appropriate shape and stride. The CTA-level tile <code>gA</code> will have shape <code>(bM, bK, k)</code>, where <code>bM</code> and <code>bK</code> are the dimensions used for a tile, and <code>k</code> is the amount of times the tile has to be repeated in the <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span> dimension to completely fill a row. Furthermore, the offset with regards to the global memory pointer is automatically computed by CuTe. This makes indexing easier to follow, especially when working with thread-level Tensors, as their shapes can get convoluted even with the simplifications provided by CuTe. For example, the thread-level tensor used for indexing into shared memory when copying the final gated result is a rank-3 tensor of shape <code>(2, 4, (2, 2, 2))</code> <sup id=\"fnref-4\"><a href=\"#fn-4\" class=\"footnote-ref\">4</a></sup> with stride <code>(512, 2048, (8, 16, 32))</code>.</p>\n<p>However, the CuTe approach can have some drawbacks once we start trying to do some more unusual manipulation on the shapes. In particular, when we write our shapes for the GEMM part, the final result of the GEMM will be a tile of shape <code>(bM, bN)</code>. After the gating, we will need to reduce this to a <code>(bM, bN/2)</code> tile, with the appropriate changes to the strides. In principle, it is possible to do this by carefully constructing new Tensors in code from the original tile returned by CuTe. This is the first solution we tried, but we found it to be very difficult to scale to all the Tensors used throughout the kernel, besides also being an unflexible approach in terms of varying the tile shapes and Atoms used. We therefore sought to find an approach where we get the correct Tensor shapes by using the same pipeline we apply on the GEMM Tensors. This would have the advantage of allowing us to reuse the GEMM tiling code to obtain the gating tile, and it would also be more robust, since in this case we would rely on the same CuTe functions used throughout the kernel.</p>\n<h4>CuTe Atoms</h4>\n<p>Atoms in CuTe can be thought of as the basic building block for constructing thread-level Tensors and calling the relevant PTX instructions. They are mostly defined by an <strong>Operation</strong> and <strong>Traits</strong> struct. <strong>Operation</strong> handles calling the relevant PTX instruction and directly receives the raw pointers for the registers, while <strong>Traits</strong> contains information such as the Layout of the Tensor or the data type used. The common practice for CuTe is to build thread-level Tensors that usually have the shape <code>(ATOM, ATOM_M, ATOM_N)</code>, where <code>ATOM</code> is the shape on which the PTX code will be called, and <code>ATOM_M</code> refers to how the Atom is tiled on the M dimension (similarly for N).</p>\n<p>For example, if we would use a vectorized load copy instruction on half precision data, we would expect a shape like <code>((8, 1), tiled_m, tiled_n):((1, 0), stride_m, stride_n)</code>. The <code>(8, 1):(1,0)</code> Atom means that we will load 8 numbers across whichever is the major dimension, which is exactly 128 bits.</p>\n<p>In our case, we are interested in changing the shape of the MMA Atom to reflect the halved column dimension after the gating operation. Since we only need to change the shapes and do not care about the PTX code, we will skip inspecting the Operation struct. So, let's take a look at the code for the Atom Traits that we use in our GEMM:</p>\n<div class=\"remark-highlight\"><pre class=\"language-cpp\"><code><span class=\"token keyword\">template</span><span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token operator\">&gt;</span><span class=\"token lf\">\n</span><span class=\"token keyword\">struct</span><span class=\"token space\"> </span><span class=\"token class-name\">MMA_Traits</span><span class=\"token operator\">&lt;</span>SM80_16x8x16_F32BF16BF16F32_TN<span class=\"token operator\">&gt;</span><span class=\"token lf\">\n</span><span class=\"token punctuation\">{</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>ValTypeD<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token keyword\">float</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>ValTypeA<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>bfloat16_t<span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>ValTypeB<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>bfloat16_t<span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>ValTypeC<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token keyword\">float</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>Shape_MNK<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Shape<span class=\"token operator\">&lt;</span>_16<span class=\"token punctuation\">,</span>_8<span class=\"token punctuation\">,</span>_16<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>ThrID<span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Layout<span class=\"token operator\">&lt;</span>_32<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>ALayout<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Layout<span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>_4<span class=\"token punctuation\">,</span>_8<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>_2<span class=\"token punctuation\">,</span>_2<span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token space\"> </span>_2<span class=\"token operator\">&gt;&gt;</span><span class=\"token punctuation\">,</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>Stride<span class=\"token operator\">&lt;</span>Stride<span class=\"token operator\">&lt;</span>_32<span class=\"token punctuation\">,</span>_1<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span>Stride<span class=\"token operator\">&lt;</span>_16<span class=\"token punctuation\">,</span>_8<span class=\"token punctuation\">,</span>_128<span class=\"token operator\">&gt;&gt;</span><span class=\"token operator\">&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>BLayout<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Layout<span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>_4<span class=\"token punctuation\">,</span>_8<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span>_2<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>_2<span class=\"token operator\">&gt;&gt;</span><span class=\"token punctuation\">,</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>Stride<span class=\"token operator\">&lt;</span>Stride<span class=\"token operator\">&lt;</span>_16<span class=\"token punctuation\">,</span>_1<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span>Stride<span class=\"token operator\">&lt;</span>_8<span class=\"token punctuation\">,</span>_64<span class=\"token operator\">&gt;&gt;</span><span class=\"token operator\">&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>CLayout<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Layout<span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>_4<span class=\"token punctuation\">,</span>_8<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>_2<span class=\"token punctuation\">,</span>_2<span class=\"token operator\">&gt;&gt;</span><span class=\"token punctuation\">,</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>Stride<span class=\"token operator\">&lt;</span>Stride<span class=\"token operator\">&lt;</span>_32<span class=\"token punctuation\">,</span>_1<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span>Stride<span class=\"token operator\">&lt;</span>_16<span class=\"token punctuation\">,</span>_8<span class=\"token operator\">&gt;&gt;</span><span class=\"token operator\">&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>This corresponds to a warp-level <code>mma</code> instructions for the Ampere architecture, that computes the product of a <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">16 \\times 16</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span></span></span></span></span> tile from <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span></span> with a <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">8 \\times 16</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">8</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span></span></span></span></span> tile from B, and holds the layouts for the entire warp. The way to interpret these layouts is that the second rank corresponds to the elements each thread will provide for the <code>mma</code>, and the first rank represents the thread layout (note that it is always exactly <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>⋅</mo><mn>8</mn><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">4\\cdot8=32</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">8</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span><span class=\"mord\">2</span></span></span></span></span> in size, which is a warp). Taken together, the layouts represent a mapping from the <em>(Thread, Value)</em> space to the <em>Tile indices</em>.</p>\n<p>To get a clearer picture of what this means, let's inspect <code>ALayout</code>. First, we can see that each thread will use a value of shape <code>(2, 2, 2)</code> and size <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mo>⋅</mo><mn>2</mn><mo>⋅</mo><mn>2</mn><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">2\\cdot2\\cdot2=8</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">8</span></span></span></span></span>. This makes sense if we take this in the context of the A tile, which is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">16 \\times 16</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span></span></span></span></span>, and the fact that this is executed at warp-level, which has <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">32</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span><span class=\"mord\">2</span></span></span></span></span> threads. That would imply each thread needs to provide <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mn>16</mn><mo>⋅</mo><mn>16</mn></mrow><mn>32</mn></mfrac><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\frac{16\\cdot16}{32}=8</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">3</span><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mord mtight\">6</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mtight\">1</span><span class=\"mord mtight\">6</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">8</span></span></span></span></span> values from the A tile. Furthermore, we can look at the <a href=\"https://docs.nvidia.com/cuda/parallel-thread-execution/#matrix-fragments-for-mma-m16n8k16-with-floating-point-type\">PTX documentation</a> to get a visual intuition of how the threads are mapped to their respective fragments of the tile:</p>\n<div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/A.png\" alt=\"ptx fragment\" style=\"max-width: 600px; height: auto;\">\n    <p>Image from <a href=\"https://docs.nvidia.com/cuda/parallel-thread-execution/#matrix-fragments-for-mma-m16n8k16-with-floating-point-type\">PTX documentation.</a></p>\n</div>\n<p>Let's focus on thread <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span></span>, we can see that it handles the upper left corner of each <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn><mo>×</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">8 \\times 8</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">8</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">8</span></span></span></span></span> tile in the matrix fragment. Each such corner corresponds to a pair of values, and this adds up to the total of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">8</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">8</span></span></span></span></span> values it provides to the <code>mma</code> instruction.</p>\n<p>Next, let's understand how we can use Atoms in CuTe kernels to build larger tiles. The above Atom only works at the level of one warp and with only small tiles, but that is very unlikely to be sufficient to achieve good performance. In this case, there are 2 ways we can increase the amount of work a kernel does: (1) using more threads in CTAs and (2) increasing the amount of values each thread handles (thread coarsening). CuTe provides a very straightforward way to do both:</p>\n<div class=\"remark-highlight\"><pre class=\"language-cpp\"><code><span class=\"token comment\">//<span class=\"token space\"> </span>16x8x16<span class=\"token space\"> </span>mma<span class=\"token space\"> </span>from<span class=\"token space\"> </span>above</span><span class=\"token lf\">\n</span>TiledMMA<span class=\"token space\"> </span>mma<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token function\">make_tiled_mma</span><span class=\"token punctuation\">(</span>SM80_16x8x16_F32BF16BF16F32_TN<span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>Layout<span class=\"token operator\">&lt;</span>Shape<span class=\"token operator\">&lt;</span>_2<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>_2<span class=\"token operator\">&gt;&gt;</span><span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token comment\">//<span class=\"token space\"> </span>(1)<span class=\"token space\"> </span>larger<span class=\"token space\"> </span>CTA</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>Tile<span class=\"token operator\">&lt;</span>_64<span class=\"token punctuation\">,</span>_32<span class=\"token punctuation\">,</span>_32<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token comment\">//<span class=\"token space\"> </span>(2)<span class=\"token space\"> </span>thread<span class=\"token space\"> </span>coarsening,<span class=\"token space\"> </span>MxNxK<span class=\"token space\"> </span>shape</span>\n</code></pre></div>\n<p>In line 2, we tile two times across both <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> dimensions by increasing the number of warps in our CTA to a total of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">4</span></span></span></span></span>, which corresponds to <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">128</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">2</span><span class=\"mord\">8</span></span></span></span></span> threads. This also means that so far we can compute a <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>32</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">32 \\times 16</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">3</span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span></span></span></span></span> final result, since we repeat the tile twice. On line 3, we coarsen each thread, by doubling how many elements each thread has to handle. Importantly, CuTe <strong>does not always throw a compile error</strong> for incorrect values in the tile. For example, using <code>Tile&#x3C;_64, _8, _32></code> would work, even if it doesn't make sense. This detail can lead to confusing runtime errors, and is especially important for the way we will solve the gate tile issue.</p>\n<h4>Computing the gated result</h4>\n<p>Before showing how we can obtain the required Tensor shapes, let's quickly take a look at how we compute the gated result. Remember that we store our weights by using the odd and even columns for the two up-scaling matrices. In particular, we make sure that <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">⌊</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>l</mi></mrow><mn>2</mn></mfrac><mo stretchy=\"false\">⌋</mo></mrow><annotation encoding=\"application/x-tex\">\\lfloor \\frac{col}{2} \\rfloor</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2251079999999999em;vertical-align:-0.345em;\"></span><span class=\"mopen\">⌊</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">c</span><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">⌋</span></span></span></span></span> corresponds to the same column in the original weight matrix, e.g. column 1 is equivalent to the first column of the gate weights, and column 0 is the first column of the up-proj weights. Using this, we simply write:</p>\n<div class=\"remark-highlight\"><pre class=\"language-cpp\"><code>CUTE_UNROLL<span class=\"token lf\">\n</span><span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span><span class=\"token space\"> </span>j<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span><span class=\"token space\"> </span>j<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>MMA_N<span class=\"token punctuation\">;</span><span class=\"token space\"> </span><span class=\"token operator\">++</span>j<span class=\"token punctuation\">)</span><span class=\"token space\"> </span><span class=\"token punctuation\">{</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span>CUTE_UNROLL<span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span><span class=\"token space\"> </span>i<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span><span class=\"token space\"> </span>i<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>MMA_M<span class=\"token punctuation\">;</span><span class=\"token space\"> </span><span class=\"token operator\">++</span>i<span class=\"token punctuation\">)</span><span class=\"token space\"> </span><span class=\"token punctuation\">{</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>tCrC_gate<span class=\"token punctuation\">[</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>i<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token generic-function\"><span class=\"token function\">cast_mmaresult</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span>TC<span class=\"token operator\">&gt;</span></span></span><span class=\"token punctuation\">(</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>tCrC<span class=\"token punctuation\">[</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>i<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token space\"> </span><span class=\"token operator\">*</span><span class=\"token space\"> </span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token function\">silu</span><span class=\"token punctuation\">(</span>tCrC<span class=\"token punctuation\">[</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>i<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>tCrC_gate<span class=\"token punctuation\">[</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>i<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span><span class=\"token generic-function\"><span class=\"token function\">cast_mmaresult</span><span class=\"token generic class-name\"><span class=\"token operator\">&lt;</span>TC<span class=\"token operator\">&gt;</span></span></span><span class=\"token punctuation\">(</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>tCrC<span class=\"token punctuation\">[</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>i<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token space\"> </span><span class=\"token operator\">*</span><span class=\"token space\"> </span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token function\">silu</span><span class=\"token punctuation\">(</span>tCrC<span class=\"token punctuation\">[</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token function\">make_coord</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>i<span class=\"token punctuation\">,</span><span class=\"token space\"> </span>j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token space\"> </span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token punctuation\">}</span><span class=\"token lf\">\n</span><span class=\"token punctuation\">}</span>\n</code></pre></div>\n<p>Let's also take a look at the mapping of threads to values used by <code>mma</code> for the <code>C</code> matrix:</p>\n<div style=\"text-align: center; margin: 10px;\">\n    <img src=\"/galleries/fused_swiglu2025/B.png\" alt=\"ptx C fragment\" style=\"max-width: 600px; height: auto;\">\n</div>\n<p>As we can see, each thread will always hold a pair of two elements. Furthermore, we know that from the way we do tiling that <code>c0</code> will always be an even column and <code>c1</code> an odd column. This is the key reason we can be sure our algorithm is correct, as this structure generalizes to all warps. It only remains to see how we can construct the <code>tCrC_gate</code> Tensor in a clean fashion.</p>\n<h3>Custom MMA Atom</h3>\n<hr>\n<p>Finally, we can explain our solution. What we did is to define this <strong>Traits</strong> Atom for an imaginary <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>16</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">16 \\times 4 \\times 16</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span></span></span></span></span> MMA:</p>\n<div class=\"remark-highlight\"><pre class=\"language-cpp\"><code><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">template</span><span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token operator\">&gt;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">struct</span><span class=\"token space\"> </span><span class=\"token class-name\">MMA_Traits</span><span class=\"token operator\">&lt;</span>SM80_16x4x16_F32BF16BF16F32_TN_GATED<span class=\"token operator\">&gt;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token operator\">:</span><span class=\"token space\"> </span>MMA_Traits<span class=\"token operator\">&lt;</span>SM80_16x8x8_F16F16F16F16_TN<span class=\"token operator\">&gt;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token punctuation\">{</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token comment\">//<span class=\"token space\"> </span>4<span class=\"token space\"> </span>is<span class=\"token space\"> </span>very<span class=\"token space\"> </span>important<span class=\"token space\"> </span>here,<span class=\"token space\"> </span>since<span class=\"token space\"> </span>we<span class=\"token space\"> </span>reduce<span class=\"token space\"> </span>the<span class=\"token space\"> </span>atoms<span class=\"token space\"> </span>on<span class=\"token space\"> </span>the<span class=\"token space\"> </span>feature<span class=\"token space\"> </span>(N)<span class=\"token space\"> </span>dimension</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>Shape_MNK<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Shape<span class=\"token operator\">&lt;</span>_16<span class=\"token punctuation\">,</span>_4<span class=\"token punctuation\">,</span>_16<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>ThrID<span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Layout<span class=\"token operator\">&lt;</span>_32<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token comment\">//<span class=\"token space\"> </span>A<span class=\"token space\"> </span>and<span class=\"token space\"> </span>B<span class=\"token space\"> </span>don't<span class=\"token space\"> </span>really<span class=\"token space\"> </span>matter,<span class=\"token space\"> </span>this<span class=\"token space\"> </span>atom<span class=\"token space\"> </span>is<span class=\"token space\"> </span>useful<span class=\"token space\"> </span>only<span class=\"token space\"> </span>for<span class=\"token space\"> </span>creating<span class=\"token space\"> </span>the<span class=\"token space\"> </span>right<span class=\"token space\"> </span>shapes<span class=\"token space\"> </span>for<span class=\"token space\"> </span>the<span class=\"token space\"> </span>C<span class=\"token space\"> </span>layout</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token comment\">//<span class=\"token space\"> </span>the<span class=\"token space\"> </span>C<span class=\"token space\"> </span>layout<span class=\"token space\"> </span>suggest<span class=\"token space\"> </span>the<span class=\"token space\"> </span>(2)<span class=\"token space\"> </span>atoms<span class=\"token space\"> </span>used<span class=\"token space\"> </span>for<span class=\"token space\"> </span>the<span class=\"token space\"> </span>merging</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token comment\">//<span class=\"token space\"> </span>and<span class=\"token space\"> </span>the<span class=\"token space\"> </span>strides<span class=\"token space\"> </span>are<span class=\"token space\"> </span>changed<span class=\"token space\"> </span>to<span class=\"token space\"> </span>make<span class=\"token space\"> </span>sense<span class=\"token space\"> </span>in<span class=\"token space\"> </span>terms<span class=\"token space\"> </span>of<span class=\"token space\"> </span>the<span class=\"token space\"> </span>new<span class=\"token space\"> </span>numel</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token keyword\">using</span><span class=\"token space\"> </span>CLayout<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>Layout<span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span>Shape<span class=\"token space\"> </span><span class=\"token operator\">&lt;</span><span class=\"token space\"> </span>_4<span class=\"token punctuation\">,</span>_8<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span><span class=\"token space\"> </span>_2<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token space\"> </span>Stride<span class=\"token operator\">&lt;</span>Stride<span class=\"token operator\">&lt;</span>_16<span class=\"token punctuation\">,</span>_1<span class=\"token operator\">&gt;</span><span class=\"token punctuation\">,</span>_8<span class=\"token operator\">&gt;&gt;</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token space\"> </span><span class=\"token space\"> </span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>Because we are only interested in using this Atom to work on the result of the GEMM, we can leave the <code>A</code> and <code>B</code> layouts unchanged from the earlier struct. The change is relatively easy to understand, we reduce the value for each thread from <code>(2, 2)</code> to <code>(2)</code>, as we will multiply each odd-even pair to do our gating. We also have to adapt the stride values to our smaller layout. Relating to the earlier figure of the <code>C</code> matrix, this is equivalent to reducing each pair to just one element, through the gating operation.</p>\n<p>We can now keep the code almost completely unchanged and apply the same transformations we do to obtain the thread values for the <code>C</code> matrix during the GEMM, as they are guaranteed to be applied in the same way, but with our corrected <code>CLayout</code>. For example, the fragments are obtained in essentially the same way:</p>\n<div class=\"remark-highlight\"><pre class=\"language-cpp\"><code><span class=\"token comment\">//<span class=\"token space\"> </span>for<span class=\"token space\"> </span>the<span class=\"token space\"> </span>GEMM</span><span class=\"token lf\">\n</span>Tensor<span class=\"token space\"> </span>tCgC<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>thr_mma<span class=\"token punctuation\">.</span><span class=\"token function\">partition_C</span><span class=\"token punctuation\">(</span>gC<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span>Tensor<span class=\"token space\"> </span>tCrC<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>thr_mma<span class=\"token punctuation\">.</span><span class=\"token function\">make_fragment_C</span><span class=\"token punctuation\">(</span>tCgC<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token comment\">//<span class=\"token space\"> </span>for<span class=\"token space\"> </span>the<span class=\"token space\"> </span>gating</span><span class=\"token lf\">\n</span>Tensor<span class=\"token space\"> </span>tCgC_gate<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>thr_mma_gate<span class=\"token punctuation\">.</span><span class=\"token function\">partition_C</span><span class=\"token punctuation\">(</span>gC_half<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span>Tensor<span class=\"token space\"> </span>tCrC_gate<span class=\"token space\"> </span><span class=\"token operator\">=</span><span class=\"token space\"> </span>thr_mma_gate<span class=\"token punctuation\">.</span><span class=\"token function\">make_fragment_C</span><span class=\"token punctuation\">(</span>tCgC_gate<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token lf\">\n</span><span class=\"token comment\">//<span class=\"token space\"> </span>and<span class=\"token space\"> </span>so<span class=\"token space\"> </span>on</span>\n</code></pre></div>\n<p>As can be seen in the code snippet, there are still some small changes we need to make. In particular, we need to ensure we adjust some strides before calling the kernel, since our rows are shorter than what a GEMM kernel would expect. If we use thread coarsening, we also must be careful in the final <code>Tile&#x3C;></code> definition to account for the halved <code>N</code> dimension. Regardless, we find that handling these details outside of the kernel code is significantly easier to follow than directly manipulating Tensor shapes and strides in the kernel.</p>\n<h2>Closing thoughts</h2>\n<hr>\n<h3>Can this be extended to any GEMM kernel?</h3>\n<p>We earlier claimed that any efficient Ampere GEMM kernel should be amenable to fuse gating in a similar manner. The reason for that is the <code>mma</code> instruction, more exactly the way the <code>C</code> matrix is held by each thread. As we have mentioned in the <a href=\"#close\">low-level section</a>, all threads in a warp will hold pairs of elements. If it's possible to determine the parity of each element of the pair, a similar algorithm to what we propose should be achievable, which we think is the case for most (if not all) efficient kernels.</p>\n<p>The reason we specify Ampere is that the algorithm hinges on the particular structure of the <code>C</code> matrix. However, in general <code>mma</code>-like instructions tend to hold multiple adjacent values per thread, which suggest a similar odd-even scheme should be possible for other architectures. For warp-level instructions that hold just one value per thread, warp shuffling could be used to exchange data intra-warp.</p>\n<h3>Future directions</h3>\n<p>So far, we have focused only on the first half of the MLP computation. More so, our optimization concerns just the forward pass, as for a backward pass it's generally more efficient to store intermediary results anyway, and therefore our kernel would achieve lower throughput. We are interested in studying the possiblity of fusing the entire MLP in a single kernel. A more immediate goal would be to improve our current kernel, mainly to further bridge the gap to cuBLAS performance.</p>\n<h2>References</h2>\n<hr>\n<p>[1]. <a href id=\"fa\" target=\"_blank\"></a> Flash Attention, Dao et. al: <a href=\"https://arxiv.org/abs/2205.14135\">https://arxiv.org/abs/2205.14135</a></p>\n<p>[2]. <a href id=\"wa\" target=\"_blank\"></a> Longformer: The Long-Document Transformer, Beltagy et. al: <a href=\"https://arxiv.org/abs/2004.05150\">https://arxiv.org/abs/2004.05150</a></p>\n<p>[3]. <a href id=\"mam\" target=\"_blank\"></a> Mamba: Linear-Time Sequence Modeling with Selective State Spaces, Gu et. al: <a href=\"https://arxiv.org/abs/2312.00752\">https://arxiv.org/abs/2312.00752</a></p>\n<p>[4]. <a href id=\"lru\" target=\"_blank\"></a> Resurrecting Recurrent Neural Networks for Long Sequences, Orvieto et. al: <a href=\"https://arxiv.org/abs/2303.06349\">https://arxiv.org/abs/2303.06349</a></p>\n<p>[5]. <a href id=\"mam2\" target=\"_blank\"></a> Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality, Dao et. al: <a href=\"https://arxiv.org/abs/2405.21060\">https://arxiv.org/abs/2405.21060</a></p>\n<p>[6]. <a href id=\"glu\" target=\"_blank\"></a> GLU Variants Improve Transformer, Noam Shazeer: <a href=\"https://arxiv.org/abs/2002.05202\">https://arxiv.org/abs/2002.05202</a></p>\n<p>[7]. <a href id=\"lig\" target=\"_blank\"></a> Liger Kernel: Efficient Triton Kernels for LLM Training, Hsu et. al <a href=\"https://arxiv.org/abs/2410.10989\">https://arxiv.org/abs/2410.10989</a></p>\n<p>[8]. <a href id=\"uns\" target=\"_blank\"></a> UnslothAI, Han et. al: <a href=\"https://github.com/unslothai/unsloth\">https://github.com/unslothai/unsloth</a></p>\n<h2>Credits</h2>\n<p>Blog background image generated with DALL·E 3.</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">Here we should also note that the discussion for warps is also more nuanced, since we skipped talking about warp divergence.<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\">In practice, PyTorch uses its own CUDA memory allocator. A simplified way to look at it is that when a tensor frees memory, PyTorch doesn’t immediately return it to the GPU. Instead, it holds onto the memory for reuse, reducing the overhead associated with <code>cudaMalloc</code> and <code>cudaFree</code> calls. Therefore, scripts similar to this one might not actually result in increased total memory usage, and will (probably) always use less than <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span></span> times the memory of <code>x</code>, since the <code>x * scaling</code> term can be freed after the squaring operation.<a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-3\">This is computed as the ratio <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mn>4</mn><mo>⋅</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>D</mi><mo>+</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>U</mi><mo>+</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>D</mi></mrow><mrow><mn>4</mn><mo>⋅</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>D</mi><mo>+</mo><mn>3</mn><mo>⋅</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>U</mi><mo>+</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>D</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{4\\cdot Seq \\cdot D + Seq \\cdot U + Seq \\cdot D}{4\\cdot Seq \\cdot D + 3 \\cdot Seq \\cdot U + Seq \\cdot D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.4055469999999999em;vertical-align:-0.481108em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.924439em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">4</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">3</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">U</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.446108em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">4</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">U</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.481108em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>, where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">U</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span></span></span></span></span> is short-hand for the up-scaling dimension. This further reduces to <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mn>5</mn><mi>D</mi><mo>+</mo><mi>U</mi></mrow><mrow><mn>5</mn><mi>D</mi><mo>+</mo><mn>3</mn><mi>U</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{5D + U}{5D + 3U}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.275662em;vertical-align:-0.403331em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.872331em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">5</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">3</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">U</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">5</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">U</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>. Since we have <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi><mo>=</mo><mi>c</mi><mo>⋅</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">U=c\\cdot D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.44445em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span> with <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi><mo>></mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">c>2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span></span> in practice, we can simplify this to <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mn>5</mn><mo>+</mo><mi>c</mi></mrow><mrow><mn>5</mn><mo>+</mo><mn>3</mn><mi>c</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{5+c}{5+3c}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2484389999999999em;vertical-align:-0.403331em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">5</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">3</span><span class=\"mord mathnormal mtight\">c</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">5</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\">c</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403331em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>, which is lower bounded by <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{1}{3}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">3</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span> in the limit, and upper bounded by <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext> </mtext><mn>64</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">~64\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mspace nobreak\"> </span><span class=\"mord\">6</span><span class=\"mord\">4</span><span class=\"mord\">%</span></span></span></span></span> for <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">c=2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span></span>. This translates for <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>35</mn><mo>−</mo><mn>70</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">35-70\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">3</span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mord\">7</span><span class=\"mord\">0</span><span class=\"mord\">%</span></span></span></span></span> gains per layer as the up-proj dimension gets larger. If we assume some more memory optimizations, like overwriting the queries with the outputs and the up-scaled MLP inputs with the gating result (for example), we can reduce the memory requirement to <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn><mo>⋅</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>D</mi><mo>+</mo><mn>2</mn><mo>⋅</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>U</mi><mo>+</mo><mi>S</mi><mi>e</mi><mi>q</mi><mo>⋅</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">3 \\cdot Seq \\cdot D + 2 \\cdot Seq \\cdot U + Seq \\cdot D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span>. Using the same series of calculations we would arrive at a <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>25</mn><mo>−</mo><mn>50</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">25-50\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">2</span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mord\">5</span><span class=\"mord\">0</span><span class=\"mord\">%</span></span></span></span></span> gain.<a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-4\">We are not entirely sure why CuTe doesn't reduce this to a <code>(2, 4, 8): (512, 2048, 8)</code> layout, but this further reinforces why trying to directly operate on the raw layouts is a bad idea.<a href=\"#fnref-4\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>\n"}}}
