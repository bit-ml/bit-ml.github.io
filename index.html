<!DOCTYPE html><html lang="en"><head><title data-react-helmet="true">Bitdefender Machine Learning &amp; Crypto Research Unit | Engaging with the broader Machine Learning Community.</title><meta data-react-helmet="true" name="description" content="Bitdefender Machine Learning &amp; Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&amp;ML scene by supporting and participating in local conferences, lecture and research groups."/><meta data-react-helmet="true" name="keywords" content="machine-learning,research,bitdefender"/><meta data-react-helmet="true" property="og:title" content="Bitdefender Machine Learning &amp; Crypto Research Unit | Engaging with the broader Machine Learning Community."/><meta data-react-helmet="true" property="og:description" content="Bitdefender Machine Learning &amp; Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&amp;ML scene by supporting and participating in local conferences, lecture and research groups."/><meta data-react-helmet="true" property="og:site_name" content="https://bit-ml.github.io/"/><meta data-react-helmet="true" property="article:tag" content="machine-learning"/><meta data-react-helmet="true" property="article:tag" content="research"/><meta data-react-helmet="true" property="article:tag" content="bitdefender"/><meta data-react-helmet="true" property="og:image" content="https://bit-ml.github.io/tile.png"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:title" content="Bitdefender Machine Learning &amp; Crypto Research Unit | Engaging with the broader Machine Learning Community."/><meta data-react-helmet="true" name="twitter:description" content="Bitdefender Machine Learning &amp; Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&amp;ML scene by supporting and participating in local conferences, lecture and research groups."/><meta data-react-helmet="true" name="twitter:image" content="https://bit-ml.github.io/tile.png"/><meta data-react-helmet="true" itemProp="description" content="Bitdefender Machine Learning &amp; Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&amp;ML scene by supporting and participating in local conferences, lecture and research groups."/><meta data-react-helmet="true" itemProp="keywords" content="machine-learning,research,bitdefender"/><meta data-react-helmet="true" itemProp="image" content="https://bit-ml.github.io/tile.png"/><link rel="preload" as="script" href="https://bit-ml.github.io/bootstrap.fdc7d33e.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/templates/src/pages/Home.09cabfc6.js"/><link rel="preload" as="script" href="https://bit-ml.github.io/main.b0acd7e8.js"/><meta charSet="UTF-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><link rel="apple-touch-icon" href="icon.png"/><style data-styled-components="">
/* sc-component-id: sc-global-3682090152 */
html{line-height:1.15;-webkit-text-size-adjust:100%;} body{margin:0;} h1{font-size:2em;margin:0.67em 0;} hr{box-sizing:content-box;height:0;overflow:visible;} pre{font-family:monospace,monospace;font-size:1em;} a{background-color:transparent;-webkit-text-decoration:none;text-decoration:none;} abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;} b,strong{font-weight:bolder;} code,kbd,samp{font-family:monospace,monospace;font-size:1em;} small{font-size:80%;} sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;} sub{bottom:-0.25em;} sup{top:-0.5em;} figure{margin:0;} img{border-style:none;max-width:100%;} button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0;} button,input{overflow:visible;} button,select{text-transform:none;} button,[type="button"],[type="reset"],[type="submit"]{-webkit-appearance:button;} button::-moz-focus-inner,[type="button"]::-moz-focus-inner,[type="reset"]::-moz-focus-inner,[type="submit"]::-moz-focus-inner{border-style:none;padding:0;} button:-moz-focusring,[type="button"]:-moz-focusring,[type="reset"]:-moz-focusring,[type="submit"]:-moz-focusring{outline:1px dotted ButtonText;} fieldset{padding:0.35em 0.75em 0.625em;} legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;} progress{vertical-align:baseline;} textarea{overflow:auto;} [type="checkbox"],[type="radio"]{box-sizing:border-box;padding:0;} [type="number"]::-webkit-inner-spin-button,[type="number"]::-webkit-outer-spin-button{height:auto;} [type="search"]{-webkit-appearance:textfield;outline-offset:-2px;} [type="search"]::-webkit-search-decoration{-webkit-appearance:none;} ::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;} details{display:block;} summary{display:list-item;} template{display:none;} [hidden]{display:none;} html{box-sizing:border-box;font-size:16px;} *,*::before,*::after{box-sizing:inherit;} body{font-family:"Roboto",Helvetica,Arial,sans-serif;font-weight:400;font-size:16px;padding:0;background:#FFF;} h1{display:block;margin:0.67em 0;}</style><style data-styled-components="cMyULX igvuDy iNYyOL iAdFiH khWPBl kjUrxn fUYniY jYpeLT bKCobW gZUyBX eOWfHV iuMdWb kDQhEd eVvUPv fHjLkB iDbEyj cHvJpV kiWNqH gjaDKl dziJzz Ouvyh xFCli kdLuCp fQZxgf aGFSx OMggx hCNZb kVaVTK fmlbnD fGVhKG gpzdBf gidFlV dibxEQ RLPQv ezUVjM">
/* sc-component-id: Navigation__Nav-qabwmo-0 */
.cMyULX{width:100%;background:#020100;padding:0 1rem;font-family:"Roboto",Helvetica,Arial,sans-serif;text-align:right;} .cMyULX a{padding:1rem;display:inline-block;font-style:normal;font-weight:500;line-height:23px;font-size:14px;text-align:right;text-transform:uppercase;color:#A8A8A8;} .cMyULX a:last-child{padding-right:0;} @media (min-width:64.0625em){.cMyULX{padding:0 5rem;}}
/* sc-component-id: Featured__FeaturedContainer-xm4c49-0 */
.iAdFiH{background:rgba(24,27,42,0.8);left:0;bottom:-5px;position:absolute;width:100%;} @media (min-width:64.0625em){.iAdFiH{bottom:-5px;}}
/* sc-component-id: Featured__FeaturedWrapper-xm4c49-1 */
.khWPBl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:0 auto;padding:0 1rem;color:#fff;} @media (min-width:46.0625em){.khWPBl{padding:0 5rem;}} @media (min-width:64.0625em){.khWPBl{padding:0 5rem;}}
/* sc-component-id: Featured__FeaturedLink-xm4c49-2 */
.kjUrxn{display:none;background:rgba(98,118,207,0);-webkit-transition:all 0.2s cubic-bezier(0.25,0.46,0.45,0.94);transition:all 0.2s cubic-bezier(0.25,0.46,0.45,0.94);color:#E4E4E4;} .kjUrxn:first-child{display:inline;} .kjUrxn:hover{color:#fff;background:rgba(98,118,207,0.3);} @media (min-width:46.0625em){.kjUrxn{display:inline;-webkit-flex-basis:50%;-ms-flex-preferred-size:50%;flex-basis:50%;padding:1rem;}} @media (min-width:64.0625em){.kjUrxn{display:inline;-webkit-flex-basis:50%;-ms-flex-preferred-size:50%;flex-basis:50%;padding:1rem;}}
/* sc-component-id: Featured__FeaturedHeading-xm4c49-3 */
.fUYniY{font-family:'Exo 2',sans-serif;font-style:normal;font-weight:500;font-size:1.4rem;line-height:2rem;color:#89cdf0;}
/* sc-component-id: Featured__FeaturedSynopsis-xm4c49-4 */
.jYpeLT{line-height:1.4;}
/* sc-component-id: Hero-ajom9h-0 */
.igvuDy{display:inline-block;width:100%;margin:0 auto;height:92vh;padding:0 1rem;background:#020100 url('/hero_avatar.jpg') no-repeat bottom;background-size:1024px;background-position:-600px bottom;} @media (min-width:46.0625em){.igvuDy{padding:0 5rem;height:95vh;background-size:contain;background-position:bottom;}@media screen and (orientation:portrait){.igvuDy{background-size:1512px;background-position:-700px bottom;}}} @media (min-width:64.0625em){.igvuDy{padding:0 5rem;height:95vh;background-size:contain;background-position:bottom;}@media screen and (orientation:portrait){.igvuDy{background-size:2048px;background-position:-700px bottom;}}}
/* sc-component-id: Hero__Heading-ajom9h-1 */
.iNYyOL{max-width:800px;font-family:'Exo 2',sans-serif;font-style:normal;font-weight:500;font-size:36px;line-height:3rem;color:#E4E4E4;} @media (min-width:46.0625em){.iNYyOL{font-size:64px;line-height:92px;}} @media (min-width:64.0625em){.iNYyOL{font-size:64px;line-height:92px;}}
/* sc-component-id: Specialty__SpecialtyWrapper-pq5osx-0 */
.gZUyBX{position:relative;background:#EDEBEB;} @media (min-width:64.0625em){.gZUyBX{max-width:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}} @media (min-width:46.0625em){.gZUyBX{max-width:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}}
/* sc-component-id: Specialty__SpecialtyPanel-pq5osx-1 */
.eOWfHV{background:#E6212B;overflow:hidden;} @media (min-width:64.0625em){.eOWfHV{position:-webkit-sticky;position:sticky;top:0;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;height:100vh;width:38%;overflow:hidden;}.eOWfHV .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.eOWfHV .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}} @media (min-width:46.0625em){.eOWfHV{position:-webkit-sticky;position:sticky;top:0;height:100vh;width:38%;overflow:hidden;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.eOWfHV .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.eOWfHV .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}}.iuMdWb{background:#00B2CB;overflow:hidden;} @media (min-width:64.0625em){.iuMdWb{position:-webkit-sticky;position:sticky;top:0;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;height:100vh;width:38%;overflow:hidden;}.iuMdWb .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.iuMdWb .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}} @media (min-width:46.0625em){.iuMdWb{position:-webkit-sticky;position:sticky;top:0;height:100vh;width:38%;overflow:hidden;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.iuMdWb .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.iuMdWb .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}}.kDQhEd{background:#12161E;overflow:hidden;} @media (min-width:64.0625em){.kDQhEd{position:-webkit-sticky;position:sticky;top:0;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;height:100vh;width:38%;overflow:hidden;}.kDQhEd .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.kDQhEd .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}} @media (min-width:46.0625em){.kDQhEd{position:-webkit-sticky;position:sticky;top:0;height:100vh;width:38%;overflow:hidden;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.kDQhEd .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.kDQhEd .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}}.eVvUPv{background:#C700CB;overflow:hidden;} @media (min-width:64.0625em){.eVvUPv{position:-webkit-sticky;position:sticky;top:0;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;height:100vh;width:38%;overflow:hidden;}.eVvUPv .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.eVvUPv .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}} @media (min-width:46.0625em){.eVvUPv{position:-webkit-sticky;position:sticky;top:0;height:100vh;width:38%;overflow:hidden;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.eVvUPv .specItem{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.eVvUPv .specItem.grows{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:20;-webkit-flex-grow:20;-ms-flex-positive:20;flex-grow:20;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}}
/* sc-component-id: Specialty__Heading-pq5osx-2 */
.fHjLkB{margin:0;font-family:'Exo 2',sans-serif;font-style:normal;font-weight:700;font-size:19.4vw;line-height:0.75;text-transform:uppercase;color:rgba(255,255,255,0.2);margin-top:-5px;} @media (min-width:46.0625em){.fHjLkB{font-size:7.2vw;margin-top:-3px;}@media screen and (orientation:landscape){.fHjLkB{margin-top:-5px;}}} @media (min-width:64.0625em){.fHjLkB{position:initial;top:initial;font-size:7.3vw;margin-top:-8px;}}
/* sc-component-id: Specialty__Description-pq5osx-3 */
.iDbEyj{font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:normal;font-weight:400;line-height:1.6rem;font-size:1.2rem;color:rgba(255,255,255,0.8);padding:3rem 1rem;} @media (min-width:46.0625em){.iDbEyj{font-size:2vh;padding:0 2rem;}@media screen and (orientation:landscape){.iDbEyj{font-size:2.4vh;}}} @media (min-width:64.0625em){.iDbEyj{font-size:2.8vh;padding:0 8vh;line-height:4vh;}}
/* sc-component-id: Specialty__Team-pq5osx-4 */
.cHvJpV{font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:normal;font-weight:400;line-height:1.4;font-size:1rem;color:rgba(255,255,255,0.5);padding:0 1rem;} @media (min-width:46.0625em){.cHvJpV{padding:0 2rem;}} @media (min-width:64.0625em){.cHvJpV{padding:0 8vh;}}
/* sc-component-id: Specialty__ProjectsWrapper-pq5osx-5 */
@media (min-width:46.0625em){.kiWNqH{max-width:62%;}} @media (min-width:64.0625em){.kiWNqH{max-width:62%;}}
/* sc-component-id: Project-d9zx3a-0 */
@media (min-width:46.0625em){.gjaDKl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:100vh;}} @media (min-width:64.0625em){.gjaDKl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:100vh;}}
/* sc-component-id: Project__ProjectContent-d9zx3a-1 */
.dziJzz{display:inline-block;padding:0 1rem;} @media (min-width:46.0625em){.dziJzz{padding:0 5rem;}} @media (min-width:64.0625em){.dziJzz{padding:0 24vmin;}}
/* sc-component-id: Project__Heading-d9zx3a-2 */
.Ouvyh{font-family:'Exo 2',sans-serif;font-style:normal;font-weight:700;line-height:37px;font-size:22px;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;text-transform:uppercase;color:#333333;}
/* sc-component-id: Project__Text-d9zx3a-3 */
.xFCli{font-family:"Roboto",Helvetica,Arial,sans-serif;font-style:normal;font-weight:400;line-height:26px;font-size:1.1rem;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;color:#333333;}
/* sc-component-id: Project__BibList-d9zx3a-4 */
.kdLuCp{margin-top:3rem;padding-left:0;} .kdLuCp li>a{font-weight:normal;color:inherit;}
/* sc-component-id: Project__BibItem-d9zx3a-5 */
.fQZxgf{padding:.5rem;list-style:none;font-family:Roboto;font-style:italic;font-weight:400;line-height:20px;font-size:14px;-webkit-letter-spacing:0.03em;-moz-letter-spacing:0.03em;-ms-letter-spacing:0.03em;letter-spacing:0.03em;color:#828282;} .fQZxgf:hover{background-color:#e0dcdc;-webkit-transition:background-color 0.2s cubic-bezier(0.25,0.46,0.45,0.94);transition:background-color 0.2s cubic-bezier(0.25,0.46,0.45,0.94);}
/* sc-component-id: Project__BibAuthors-d9zx3a-7 */
.OMggx{font-style:normal;color:#E6212B;}
/* sc-component-id: Project__BibYear-d9zx3a-8 */
.kVaVTK{font-style:normal;}
/* sc-component-id: Project__BibPublished-d9zx3a-9 */
.hCNZb{font-style:normal;}
/* sc-component-id: Bio__BioBox-sc-13auz32-0 */
.fGVhKG{margin-left:5px;margin-right:5px;-webkit-flex:1 1 350px;-ms-flex:1 1 350px;flex:1 1 350px;padding:0.6rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-transition:all 0.2s cubic-bezier(0.25,0.46,0.45,0.94);transition:all 0.2s cubic-bezier(0.25,0.46,0.45,0.94);} @supports (display:grid){.fGVhKG{margin:0;}} @media (min-width:64.0625em){.fGVhKG:hover{background-color:#fff;box-shadow:0px 0px 17px -4px rgba(115,108,108,1);-webkit-transform:scale(1.05);-ms-transform:scale(1.05);transform:scale(1.05);}.fGVhKG:hover div>a{opacity:1;}}
/* sc-component-id: Bio__MediaFigure-sc-13auz32-1 */
.gpzdBf{margin-right:1rem;max-width:96px;max-height:96px;border-radius:50%;border:5px solid #fff;overflow:hidden;}
/* sc-component-id: Bio__MediaBody-sc-13auz32-2 */
.gidFlV{-webkit-flex:1;-ms-flex:1;flex:1;}
/* sc-component-id: Bio__Heading-sc-13auz32-3 */
.dibxEQ{font-family:'Exo 2',serif;font-style:normal;font-weight:600;line-height:27px;font-size:1rem;-webkit-letter-spacing:0.1em;-moz-letter-spacing:0.1em;-ms-letter-spacing:0.1em;letter-spacing:0.1em;text-transform:uppercase;margin:0 0 1rem 0;}
/* sc-component-id: Bio-sc-13auz32-4 */
.RLPQv{font-family:Exo 2;font-style:normal;font-weight:normal;line-height:23px;font-size:1rem;color:#828282;margin-top:0;}
/* sc-component-id: Bio__ContactBox-sc-13auz32-5 */
.ezUVjM{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-flex-wrap:nowrap;-ms-flex-wrap:nowrap;flex-wrap:nowrap;margin-bottom:1rem;} .ezUVjM >a{width:33.33%;text-align:center;font-weight:normal !important;color:#828282 !important;padding:0.5rem;border:1px solid #edebeb;margin-right:5px;-webkit-transition:all 0.2s cubic-bezier(0.25,0.46,0.45,0.94);transition:all 0.2s cubic-bezier(0.25,0.46,0.45,0.94);} .ezUVjM >a:hover{background-color:#edebeb;} @media (min-width:64.0625em){.ezUVjM >a{opacity:0;}}
/* sc-component-id: ResearchTeam-dl300p-0 */
.fmlbnD{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;display:grid;grid-template-columns:repeat(auto-fill,minmax(350px,1fr));grid-auto-rows:minmax(150px,auto);grid-gap:3rem;padding:6rem 1rem;background:#F5F2F2;} @media (min-width:46.0625em){.fmlbnD{padding:10rem 5rem;}@media screen and (orientation:portrait){.fmlbnD{padding:10rem 10rem;}}} @media (min-width:64.0625em){.fmlbnD{padding:10rem 5rem;}}</style></head><body><div id="root"><div class="content" data-reactroot=""><div><header><nav class="Navigation__Nav-qabwmo-0 cMyULX"><a class="active" aria-current="page" href="https://bit-ml.github.io/">Home</a><a href="https://bit-ml.github.io/#research">Research</a><a href="https://bit-ml.github.io/#team">Team</a></nav></header><main><div class="Hero-ajom9h-0 igvuDy"><h1 class="Hero__Heading-ajom9h-1 iNYyOL">Engaging with the broader Machine Learning Community.<!-- --> </h1><div class="Featured__FeaturedContainer-xm4c49-0 iAdFiH"><div class="Featured__FeaturedWrapper-xm4c49-1 khWPBl"><a class="Featured__FeaturedLink-xm4c49-2 kjUrxn" href="https://bit-ml.github.io/blog/post/bitdefender_at_eeml2019/"><h2 class="Featured__FeaturedHeading-xm4c49-3 fUYniY">Bitdefender at EEML 2019</h2><p class="Featured__FeaturedSynopsis-xm4c49-4 jYpeLT">Read about our team&#x27;s experience of organizing and participating at Eastern European Machine Learning summer school in Bucharest.</p></a><a class="Featured__FeaturedLink-xm4c49-2 kjUrxn" href="https://bit-ml.github.io/blog/post/bitdefender_at_tmlss2018/"><h2 class="Featured__FeaturedHeading-xm4c49-3 fUYniY">Bitdefender at TMLSS 2018</h2><p class="Featured__FeaturedSynopsis-xm4c49-4 jYpeLT">Bitdefender participated with eight of its members at the first edition of the Transylvania Machine Learning Summer School that took place at the end of July 2018 in Cluj-Napoca.</p></a></div></div></div><div class="Home__SpecialtyContainer-sc-1ffwrsf-0 bKCobW" id="research"><div class="Specialty__SpecialtyWrapper-pq5osx-0 gZUyBX"><div class="Specialty__SpecialtyPanel-pq5osx-1 eOWfHV"><h2 class="specItem Specialty__Heading-pq5osx-2 fHjLkB">Computer Vision<!-- --> </h2><div class="specItem grows"><p class="Specialty__Description-pq5osx-3 iDbEyj">We tackle fundamental unsupervised learning approaches that we consider to be the key to true unconstrained learning, which best simulates how humans discover the surrounding world. We aim to combine the unsupervised visual perception with supervised cognitive video representation. We want to build systems that understand our world by only watching videos.
And we go further, teaching the system to describe, in natural language, the discovered elements.<!-- --> </p></div><p class="specItem Specialty__Team-pq5osx-4 cHvJpV">Elena Burceanu, Iulia Duță, Ema Haller, Andrei Nicolicioiu, coordinated by Marius Leordeanu<!-- --> </p></div><div class="Specialty__ProjectsWrapper-pq5osx-5 kiWNqH"><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Unsupervised learning of objects from video sequences</h3><p class="Project__Text-d9zx3a-3 xFCli">We address an essential problem in computer vision, that of unsupervised foreground object segmentation in video, where a main object of interest in a video sequence should be automatically separated from its background. Video object segmentation and object discovery are strongly related tasks, but we tackle the problem from a fully unsupervised perspective, building object representations from raw video sequences. An efficient solution to this task would enable large-scale video interpretation at a high semantic level in the absence of the costly manual labeling.</p><p class="Project__Text-d9zx3a-3 xFCli">We are focused on generating foreground object soft masks based on automatic selection and learning from highly probable positive features. We show that such features can be selected efficiently by taking into consideration the spatio-temporal appearance and motion consistency of the object in the video sequence. We also emphasize the role of the contrasting properties between the foreground object and its background. Our work is also focused on theoretically proving the properties of our unsupervised learning method, which under some mild constraints is guaranteed to learn the correct classifier even in the unsupervised case.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://arxiv.org/abs/1704.05674" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">E. Haller, M. Leordeanu</span>, Unsupervised Object Segmentation in Video by Efficient Selection of Highly Probable Positive Features, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> In The IEEE International Conference on Computer Vision (ICCV), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2017</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://arxiv.org/abs/1907.02731" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">E. Burceanu, M. Leordeanu</span>, A Spectral Approach to Unsupervised Object Segmentation in Video, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> under review, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2019</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="./papers/govos_arxiv.pdf" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">E. Haller, M. Leordeanu</span>, Spacetime Graph Optimization for Video Object Segmentation, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> under review, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2019</span></a></li></ul></section></div><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Graph methods for video processing</h3><p class="Project__Text-d9zx3a-3 xFCli">Given a video we want to have an understanding of the scene and to be able to identify the key events in order to extract useful information. In a video there can exist multiple entities interacting in complex ways. Usually these interactions are happening in a local neighbourhood, but the distant ones have also an essential role. The classical models of video understanding often process information in a local way.</p><p class="Project__Text-d9zx3a-3 xFCli"> We propose to improve this, by using graph methods that have a strong, explicit bias towards modeling relationships and at the same time being able to model long range interactions.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://arxiv.org/abs/1904.05582" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">A. Nicolicioiu, I. Duță, M. Leordeanu</span>, Recurrent Space-time Graph Neural Networks, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in the Conference on Neural Information Processing Systems (NeurIPS), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2019</span></a></li></ul></section></div><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Video Captioning</h3><p class="Project__Text-d9zx3a-3 xFCli">Video captioning is the task of describing a video in natural language. It lies at the intersection of computer vision, natural language processing and machine learning requiring both high level visual comprehension and the ability to produce meaningful sentences. Our goal is to detect objects and events in a video and be capable of understanding the interactions between them in spatial and temporal dimensions.</p><p class="Project__Text-d9zx3a-3 xFCli">We investigated multiple methods to analyze a video and extract information from it. To overcome limitations of the task and the available data, we design multiple models, to explore different video encoding strategies, to explore intermediate video-language representation and to investigate the gains brought by additional tasks and features. We propose a method for video captioning by selecting from the results of multiple encoder-decoder models. Our selection method based on consensus among multiple sentences is more likely to produce results with the same meaning as the video. We designed a methods that surpassed the state-of-the-art results on the challenging MSR-VTT dataset.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://arxiv.org/abs/1806.01954" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">I. Duță, A. Nicolicioiu, V. Bogolin, M. Leordeanu</span>, Mining for meaning: from vision to language through multiple networks consensus, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The British Machine Vision Conference (BMVC), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2018</span></a></li></ul></section></div><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Unsupervised Object Tracking</h3><p class="Project__Text-d9zx3a-3 xFCli">Object tracking is one of the first and most fundamental problems that has been addressed in computer vision. While it has attracted the interest of many researchers over several decades of computer vision, it is far from being solved.</p><p class="Project__Text-d9zx3a-3 xFCli">The task is hard for many reasons. Difficulties could come from severe changes in object appearance, presence of background clutter and occlusions that might take place in the video.</p><p class="Project__Text-d9zx3a-3 xFCli">The only ground-truth knowledge given to the tracker is the bounding box of the object in the first frame. Thus, without knowing in advance the properties of the object being tracked, the tracking algorithm must learn them on the fly. It must adapt correctly and make sure it does not jump toward other objects in the background. That is why the possibility of drifting to the background poses on of the main challenges in tracking.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="./papers/SocietyOfTrackingParts.pdf" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">E. Burceanu, M. Leordeanu</span>, Learning a Robust Society of Tracking Parts using Co-occurrence Constraints, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The European Conference on Computer Vision (ECCV), at Visual Object Tracking workshop, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2018</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://arxiv.org/abs/1705.09602" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">E. Burceanu, M. Leordeanu</span>, Learning a Robust Society of Tracking Parts, <span class="Project__BibYear-d9zx3a-8 kVaVTK">2017</span></a></li></ul></section></div></div></div><div class="Specialty__SpecialtyWrapper-pq5osx-0 gZUyBX"><div class="Specialty__SpecialtyPanel-pq5osx-1 iuMdWb"><h2 class="specItem Specialty__Heading-pq5osx-2 fHjLkB">Natural Language Processing<!-- --> </h2><div class="specItem grows"><p class="Specialty__Description-pq5osx-3 iDbEyj">A large amount of today&#x27;s data is stored in databases. Building AI tools that facilitate the access to knowledge requires processing of natural language and structured data. We focus on neural approaches for natural language interfaces to databases, in particular structure-aware and semi-supervised methods.<!-- --> </p></div><p class="specItem Specialty__Team-pq5osx-4 cHvJpV">Florin Brad in collaboration with Traian Rebedea, Ionel Hosu, Radu Iacob<!-- --> </p></div><div class="Specialty__ProjectsWrapper-pq5osx-5 kiWNqH"><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Natural Language Interface to Databases</h3><p class="Project__Text-d9zx3a-3 xFCli">Natural Language Interface to Databases (NLIDB) bridges the gap between technical and non-technical users by allowing the latter to query large amounts of structured data through the use of instructions written in natural language.</p><p class="Project__Text-d9zx3a-3 xFCli">Despite long-standing research efforts, progress has been slow and widespread adoption has failed to pick up. Data-driven approaches have been hindered by the lack of large parallel corpora to train the models on, but recent datasets alleviate this problem. We seek to improve existing SEQ2SEQ models by leveraging syntax to guide the generation process and by using semi-supervised techniques to overcome the low parallel data regime.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://arxiv.org/abs/1707.03172" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">F. Brad, R. Iacob, I. Hosu, T. Rebedea</span>, Dataset for a Neural Natural Language Interface for Databases (NNLIDB), <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The Proceedings of the Eighth International Joint Conference on Natural Language Processing (IJCNLP, Volume 1: Long Papers), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2017</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="http://www.aclweb.org/anthology/C18-1043" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">I. Hosu, R. Iacob, F. Brad, S. Ruseti, T. Rebedea</span>, Natural Language Interface for Databases Using a Dual-Encoder Model, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The Proceedings of the 27th International Conference on Computational Linguistics (COLING), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2018</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="./papers/SYNLIDB_ICTAI_2018.pdf" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">F. Brad, R. Iacob, I. Hosu, S. Ruseti, T. Rebedea</span>, A Syntax-Guided Neural Model for Natural Language Interfaces to Databases, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The Proceedings of the IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2018</span></a></li></ul></section></div><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Visual Question Answering</h3><p class="Project__Text-d9zx3a-3 xFCli">The VQA task refers to answering questions about an image. Existing methods fuse image and text representations and are able to exploit superficial correlations and produce the correct answer, but often for the wrong reason. The recently introduced Visual Commonsense Reasoning dataset facilitates cognition-level understanding on top of recognition, by requiring not only the correct answer, but also a justification for it. We improve existing methods by leveraging structured representations of images.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="#" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">F. Brad</span>, Scene Graph Contextualization in Visual Commonsense Reasoning, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The IEEE International Conference on Computer Vision (ICCV) - Pre.Register Workshop, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2019</span></a></li></ul></section></div></div></div><div class="Specialty__SpecialtyWrapper-pq5osx-0 gZUyBX"><div class="Specialty__SpecialtyPanel-pq5osx-1 kDQhEd"><h2 class="specItem Specialty__Heading-pq5osx-2 fHjLkB">Reinforcement Learning<!-- --> </h2><div class="specItem grows"><p class="Specialty__Description-pq5osx-3 iDbEyj">Within the field of artificial intelligence reinforcement learning seems a natural setting for training agents that interact with the world we are living in. We engage in furthering the field by developing agents able to learn continuously in different environments. We are also investigating models sitting at the intersection of generative models and reinforcement learning.<!-- --> </p></div><p class="specItem Specialty__Team-pq5osx-4 cHvJpV">Tudor Berariu, Florin Gogianu, Ștefan Postăvaru, collaborating with Andrei Nica<!-- --> </p></div><div class="Specialty__ProjectsWrapper-pq5osx-5 kiWNqH"><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Continual Learning</h3><p class="Project__Text-d9zx3a-3 xFCli">Recent advances in machine learning are still limited to stationary tasks, but general purpose intelligence would require agents able to acquire knowledge in a continual manner dealing with interleaving tasks. Lifelong learning scenarios deal exactly with this problem: training agents on new tasks while preserving performance on old ones.</p><p class="Project__Text-d9zx3a-3 xFCli">We are currently exploring memory based, optimization related and architectural methods to train neural models in lifelong learning scenarios.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"></ul></section></div><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Malmo AI Challenge</h3><p class="Project__Text-d9zx3a-3 xFCli">The Microsoft Malmo AI Challenge proposed a time-extended stag hunt scneario build on top of the well-known Minecraft platform. In order to maximize its payoff in such a game an agent needs to predict the level of commitment to a collaborative strategy of the other player, decide on a specific plan and navigate through the environment to execute it.</p><p class="Project__Text-d9zx3a-3 xFCli">Multi-agent setups pose additional optimization problems stemming from the non-stationarity of the training objective. Also, situated environments ask agents to learn dynamic strategies capable of dealing with sudden changes in the course of action (such as another playing abandoning the collaborative strategy).</p><p class="Project__Text-d9zx3a-3 xFCli">We approached the contest by training agents through deep reinforcement learning techniques using recurrent neural networks for policies and for value estimation. We also added auxiliary loss functions (such as next reward, or next frame prediction) in order to complement the sparse reward signal.</p><p class="Project__Text-d9zx3a-3 xFCli"> Our submission ranked second for the AI Summer School placement prize and third for the Microsoft Azure for Research Grant prize.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="./learning-maximize-return.pdf" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">A. Nica, T. Berariu, F. Gogianu, A.M. Florea</span>, Learning to Maximize Return in a Stag Hunt Collaborative Scenario through Deep Reinforcement Learning, <span class="Project__BibPublished-d9zx3a-9 hCNZb">in The International Conference on Machine Learning (ICML), at Reinforcement Learning Workshop, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2017</span></a></li></ul></section></div><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Towards a generative Experience Replay</h3><p class="Project__Text-d9zx3a-3 xFCli">Within the field of reinforcement learning with deep neural network estimators, this research proposal is concerned with investigating and then augmenting or replacing the Experience Replay mechanism found in most of the state-of-the art methods with shallow and flexible, action-conditional models of the environment working not in the pixel space but at the level of low-dimensional intermediate representations</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"></ul></section></div></div></div><div class="Specialty__SpecialtyWrapper-pq5osx-0 gZUyBX"><div class="Specialty__SpecialtyPanel-pq5osx-1 eVvUPv"><h2 class="specItem Specialty__Heading-pq5osx-2 fHjLkB">Cryptography<!-- --> </h2><div class="specItem grows"><p class="Specialty__Description-pq5osx-3 iDbEyj">Lattice-based cryptography is a great promise for post-quantum cryptography. We build advanced primitives whose security relies on the hardness of lattice problems.<!-- --> </p></div><p class="specItem Specialty__Team-pq5osx-4 cHvJpV">Miruna Rosca, Radu Țițiu, coordinated by Damien Stehlé, Benoît Libert. Mădălina Bolboceanu<!-- --> </p></div><div class="Specialty__ProjectsWrapper-pq5osx-5 kiWNqH"><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Hardness of lattice problems</h3><p class="Project__Text-d9zx3a-3 xFCli">Lattice-based cryptography is a great promise for post-quantum cryptography. It aims at harnessing the security of cryptographic primitives in the conjectured hardness of well-identiﬁed and well-studied algorithmic problems involving euclidean lattices. In order to build post-quantum cryptographic primitives based on lattices, we actually make use of some intermediate, more versatile problems, Learning With Errors (LWE) and Shortest Integer Solutions (SIS), which are provably at least as hard as classical lattice problems.</p><p class="Project__Text-d9zx3a-3 xFCli">To obtain more efficient primitives, different structured variants of LWE and SIS have been introduced. We are interested in studying the hardness of all these problems, giving reductions between them and using them to build new cryptographic primitives.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://eprint.iacr.org/2018/494" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">M. Bolboceanu, Z. Brakerski, R. Perlman, D. Sharma</span>, Order-LWE and the Hardness of Ring-LWE with Entropic Secrets, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The Annual International Conference on the Theory and Application of Cryptology and Information Security (Asiacrypt), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2019</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://eprint.iacr.org/2018/170" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">M. Rosca, D. Stehlé and A. Wallet</span>, On the Ring-LWE and Polynomial-LWE Problems, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in the proceedings of International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), pages 146-173, Springer, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2018</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://eprint.iacr.org/2018/1035" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">M. Bolboceanu</span>, Relating Different Polynomial-LWE problems, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in 11th International Conference on Security for Information Technology and Communications (SECITC), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2018</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://eprint.iacr.org/2017/628" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">M. Rosca, A. Sakzad, D. Stehlé and R. Steinfeld</span>, Middle-Product Learning with Errors, <span class="Project__BibPublished-d9zx3a-9 hCNZb">In the proceedings of International Cryptology Conference (CRYPTO), pages 283-297, Springer, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2017</span></a></li></ul></section></div><div class="Project-d9zx3a-0 gjaDKl"><section class="Project__ProjectContent-d9zx3a-1 dziJzz"><h3 class="Project__Heading-d9zx3a-2 Ouvyh">Cryptographic primitives from LWE</h3><p class="Project__Text-d9zx3a-3 xFCli">My research project is concerned with building advanced cryptographic primitives, which mainly rely their security on the Learning With Errors problem (LWE). Together with my PhD advisor, Benoit Libert, we are currently working on improving known constructions of some Functional Encryption schemes.</p><p class="Project__Text-d9zx3a-3 xFCli">Some Applications of this primitive: run learning algorithms on encrypted data; perform statistical tests on the sensitive encrypted data that one could find for instance in a hospital or in a bank.</p><ul class="Project__BibList-d9zx3a-4 kdLuCp"><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">B.Libert, R. Țițiu</span>, Multi-Client Functional Encryption for Linear Functions in the Standard Model from LWE, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The Annual International Conference on the Theory and Application of Cryptology and Information Security (Asiacrypt), </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2019</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="https://eprint.iacr.org/2019/908" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">B. Libert, K. Nguyen, A. Passelègue, R. Țițiu</span>, Simulation-Sound Proofs for LWE and Applications to KDM-CCA2 Security, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> under review, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2019</span></a></li><li class="Project__BibItem-d9zx3a-5 fQZxgf"><a class="Project__BibLink-d9zx3a-6 aGFSx" href="" target="_blank"><span class="Project__BibAuthors-d9zx3a-7 OMggx">R. Țițiu, B.Libert, D. Stehlé</span>, Adaptively Secure Distributed PRFs from LWE, <span class="Project__BibPublished-d9zx3a-9 hCNZb"> in The Theory of Cryptography Conference (TCC), invited to The Journal of Cryptology, </span><span class="Project__BibYear-d9zx3a-8 kVaVTK">2018</span></a></li></ul></section></div></div></div></div><div class="ResearchTeam-dl300p-0 fmlbnD" id="team"><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/elena_burceanu.jpg" alt="elena_burceanu.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Elena Burceanu<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">My interest is in understanding videos in an unsupervised manner, currently working on object tracking for my PhD at University of Bucharest and Institute of Mathematics of the Romanian Academy. I have a strong background in Mathematics and Physics and I have finished my BSc in Computer Science and the MSc in Distributed Systems at University Politehnica Bucharest.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:eburceanu@bitdefender.com">email</a><a href="https://github.com/ilarele">github</a><a href="https://twitter.com/ilarele">twitter</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/florin_brad.jpg" alt="florin_brad.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Florin Brad<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">I am interested in neural generative models for natural language processing, especially for code generation. In particular, I am interested in leveraging discrete structure (syntax trees) to improve the expresiveness of the latent space and to guide the generation process.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:fbrad@bitdefender.com">email</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/iulia_duta.jpg" alt="iulia_duta.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Iulia Duță<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">I have a BSc in Computer Science and a MSc in Artificial Intelligence at the University of Bucharest. My research focuses on Computer Vision. I am particularly interested in developing techniques for a better understanding and representation of video scene. My current goal is to improve the quality of video descriptions using machine learning approaches.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:iduta@bitdefender.com">email</a><a href="https://twitter.com/DutaIulia">twitter</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/florin_gogianu.jpg" alt="florin_gogianu.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Florin Gogianu<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">Currently pursuing an MSc in Artificial Intelligence from University Politehnica of Bucharest after getting a BSc in Philosophy and graduating from an MSc in History &amp; Philosophy of Science. I have a broad interest in Reinforcement Learning topics and I am currently focusing on questions regarding sample efficiency in the context of model-free value-based methods with neural network estimators.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:fgogianu@bitdefender.com">email</a><a href="https://github.com/floringogianu">github</a><a href="https://twitter.com/FlorinGogianu">twitter</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/andrei_nicolicioiu.jpg" alt="andrei_nicolicioiu.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Andrei Nicolicioiu<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">My focus is on deep learning methods for Computer Vision problems. I studied at University Politehnica of Bucharest, where I obtained a Bachelor&#x27;s degree and a Master&#x27;s degree in Artificial Inteligence. I began with studying some lower-mid level computer vision problems like occlusions regions segmentation and then I leaned over some higher level tasks, like multi-label classification in video, captioning in video, finding common representations between video and language.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:anicolicioiu@bitdefender.com">email</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/emanuela_haller.jpg" alt="emanuela_haller.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Emanuela Haller<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">I am a second year PhD student, co-supervised by Marius Leordeanu (Institute of Mathematics of the Romanian Academy) and Adina Magda Florea (University Politehnica of Bucharest). I have a BSc in Computer Science and a MSc in Artificial Intelligence, both from University Politehnica of Bucharest. My main focus is the problem of unsupervised learning and I am currently working on the task of unsupervised learning of objects from video sequences.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:ehaller@bitdefender.com">email</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/miruna_rosca.jpg" alt="miruna_rosca.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Miruna Roșca<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">I am interested in post-quantum cryptography, with a focus on lattice-based solutions. I have a strong background in mathematics and I am currently a second year Phd student in cryptography at École Normale Supérieure de Lyon, working with Damien Stehlé on hardness of lattice problems over rings. More info about me here: http://perso.ens-lyon.fr/miruna.rosca/<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:mrosca@bitdefender.com">email</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/madalina_bolboceanu.jpg" alt="madalina_bolboceanu.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Mădălina Bolboceanu<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">I obtained both my undergraduate and Master degrees in Mathematics from the University of Bucharest. My goal is to use my strong mathematical skills and experience in mathematical contests and olympiads to solve cryptographic challenges. I am interested in applications of lattices in cryptography, including lattice based homomorphic encryption schemes.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:mbolboceanu@bitdefender.com">email</a><a href="https://twitter.com/molbozaur">twitter</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/radu_titiu.jpg" alt="radu_titiu.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Radu Țițiu<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">I am in my second year as a PhD student in cryptography, under the supervision of Benoit Libert at ENS de Lyon. My focus is on lattice-based cryptography, which proposes promising cryptographic schemes in the eventuality that quantum algorithms could be implemented on quantum devices. Moreover, lattice-based cryptography enables the construction of some provably secure advanced cryptographic primitives including Identity Based Encryption, Attribute Based Encryption, Functional Encryption, Homomorphic Encryption.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:rtitiu@bitdefender.com">email</a></div></div></div><div class="Bio__BioBox-sc-13auz32-0 fGVhKG"><figure class="Bio__MediaFigure-sc-13auz32-1 gpzdBf"><img src="./bio/marius_leordeanu.jpg" alt="marius_leordeanu.jpg"/></figure><div class="Bio__MediaBody-sc-13auz32-2 gidFlV"><h4 class="Bio__Heading-sc-13auz32-3 dibxEQ">Marius Leordeanu<!-- --> </h4><p class="Bio-sc-13auz32-4 RLPQv">I am an Associate Professor (Senior Lecturer) at the University Politehnica of Bucharest and senior researcher at the Institute of Mathematics of the Romanian Academy. I am interested in the nature of intelligence, life and consciousness. In particular, my research focuses on computer vision, machine learning and robotics. At the university I teach the graduate level computer vision and robotics classes. I have received a Ph.D. in Robotics from Carnegie Mellon University in 2009 and Bachelor degrees in Mathematics and Computer Science from the City University of New York, 2003.<!-- --> </p><div class="Bio__ContactBox-sc-13auz32-5 ezUVjM"><a href="mailto:leordeanu@gmail.com">email</a></div></div></div></div></main></div></div></div><script type="text/javascript">window.__CSS_CHUNKS__ = {}</script><script type="text/javascript">
    window.__routeInfo = {"path":"/","templateID":0,"sharedPropsHashes":{"posts":"ZGLzgP"},"localProps":null,"allProps":{"specialties":[{"title":"Computer Vision","description":"We tackle fundamental unsupervised learning approaches that we consider to be the key to true unconstrained learning, which best simulates how humans discover the surrounding world. We aim to combine the unsupervised visual perception with supervised cognitive video representation. We want to build systems that understand our world by only watching videos.\nAnd we go further, teaching the system to describe, in natural language, the discovered elements.","people":"Elena Burceanu, Iulia Duță, Ema Haller, Andrei Nicolicioiu, coordinated by Marius Leordeanu","projects":[{"title":"Unsupervised learning of objects from video sequences","description":"We address an essential problem in computer vision, that of unsupervised foreground object segmentation in video, where a main object of interest in a video sequence should be automatically separated from its background. Video object segmentation and object discovery are strongly related tasks, but we tackle the problem from a fully unsupervised perspective, building object representations from raw video sequences. An efficient solution to this task would enable large-scale video interpretation at a high semantic level in the absence of the costly manual labeling.\nWe are focused on generating foreground object soft masks based on automatic selection and learning from highly probable positive features. We show that such features can be selected efficiently by taking into consideration the spatio-temporal appearance and motion consistency of the object in the video sequence. We also emphasize the role of the contrasting properties between the foreground object and its background. Our work is also focused on theoretically proving the properties of our unsupervised learning method, which under some mild constraints is guaranteed to learn the correct classifier even in the unsupervised case.","papers":[{"authors":"E. Haller, M. Leordeanu","title":"Unsupervised Object Segmentation in Video by Efficient Selection of Highly Probable Positive Features","year":"2017","link":"https://arxiv.org/abs/1704.05674","published":" In The IEEE International Conference on Computer Vision (ICCV)"},{"authors":"E. Burceanu, M. Leordeanu","title":"A Spectral Approach to Unsupervised Object Segmentation in Video","year":"2019","link":"https://arxiv.org/abs/1907.02731","published":" under review"},{"authors":"E. Haller, M. Leordeanu","title":"Spacetime Graph Optimization for Video Object Segmentation","year":"2019","link":"./papers/govos_arxiv.pdf","published":" under review"}]},{"title":"Graph methods for video processing","description":"Given a video we want to have an understanding of the scene and to be able to identify the key events in order to extract useful information. In a video there can exist multiple entities interacting in complex ways. Usually these interactions are happening in a local neighbourhood, but the distant ones have also an essential role. The classical models of video understanding often process information in a local way.\n We propose to improve this, by using graph methods that have a strong, explicit bias towards modeling relationships and at the same time being able to model long range interactions.","papers":[{"authors":"A. Nicolicioiu, I. Duță, M. Leordeanu","title":"Recurrent Space-time Graph Neural Networks","year":"2019","link":"https://arxiv.org/abs/1904.05582","published":" in the Conference on Neural Information Processing Systems (NeurIPS)"}]},{"title":"Video Captioning","description":"Video captioning is the task of describing a video in natural language. It lies at the intersection of computer vision, natural language processing and machine learning requiring both high level visual comprehension and the ability to produce meaningful sentences. Our goal is to detect objects and events in a video and be capable of understanding the interactions between them in spatial and temporal dimensions.\nWe investigated multiple methods to analyze a video and extract information from it. To overcome limitations of the task and the available data, we design multiple models, to explore different video encoding strategies, to explore intermediate video-language representation and to investigate the gains brought by additional tasks and features. We propose a method for video captioning by selecting from the results of multiple encoder-decoder models. Our selection method based on consensus among multiple sentences is more likely to produce results with the same meaning as the video. We designed a methods that surpassed the state-of-the-art results on the challenging MSR-VTT dataset.","papers":[{"authors":"I. Duță, A. Nicolicioiu, V. Bogolin, M. Leordeanu","title":"Mining for meaning: from vision to language through multiple networks consensus","year":"2018","link":"https://arxiv.org/abs/1806.01954","published":" in The British Machine Vision Conference (BMVC)"}]},{"title":"Unsupervised Object Tracking","description":"Object tracking is one of the first and most fundamental problems that has been addressed in computer vision. While it has attracted the interest of many researchers over several decades of computer vision, it is far from being solved.\nThe task is hard for many reasons. Difficulties could come from severe changes in object appearance, presence of background clutter and occlusions that might take place in the video.\nThe only ground-truth knowledge given to the tracker is the bounding box of the object in the first frame. Thus, without knowing in advance the properties of the object being tracked, the tracking algorithm must learn them on the fly. It must adapt correctly and make sure it does not jump toward other objects in the background. That is why the possibility of drifting to the background poses on of the main challenges in tracking.","papers":[{"authors":"E. Burceanu, M. Leordeanu","title":"Learning a Robust Society of Tracking Parts using Co-occurrence Constraints","year":"2018","link":"./papers/SocietyOfTrackingParts.pdf","published":" in The European Conference on Computer Vision (ECCV), at Visual Object Tracking workshop"},{"authors":"E. Burceanu, M. Leordeanu","title":"Learning a Robust Society of Tracking Parts","year":"2017","link":"https://arxiv.org/abs/1705.09602"}]}]},{"title":"Natural Language Processing","description":"A large amount of today's data is stored in databases. Building AI tools that facilitate the access to knowledge requires processing of natural language and structured data. We focus on neural approaches for natural language interfaces to databases, in particular structure-aware and semi-supervised methods.","people":"Florin Brad in collaboration with Traian Rebedea, Ionel Hosu, Radu Iacob","projects":[{"title":"Natural Language Interface to Databases","description":"Natural Language Interface to Databases (NLIDB) bridges the gap between technical and non-technical users by allowing the latter to query large amounts of structured data through the use of instructions written in natural language.\nDespite long-standing research efforts, progress has been slow and widespread adoption has failed to pick up. Data-driven approaches have been hindered by the lack of large parallel corpora to train the models on, but recent datasets alleviate this problem. We seek to improve existing SEQ2SEQ models by leveraging syntax to guide the generation process and by using semi-supervised techniques to overcome the low parallel data regime.","papers":[{"authors":"F. Brad, R. Iacob, I. Hosu, T. Rebedea","title":"Dataset for a Neural Natural Language Interface for Databases (NNLIDB)","year":"2017","link":"https://arxiv.org/abs/1707.03172","published":" in The Proceedings of the Eighth International Joint Conference on Natural Language Processing (IJCNLP, Volume 1: Long Papers)"},{"authors":"I. Hosu, R. Iacob, F. Brad, S. Ruseti, T. Rebedea","title":"Natural Language Interface for Databases Using a Dual-Encoder Model","year":"2018","link":"http://www.aclweb.org/anthology/C18-1043","published":" in The Proceedings of the 27th International Conference on Computational Linguistics (COLING)"},{"authors":"F. Brad, R. Iacob, I. Hosu, S. Ruseti, T. Rebedea","title":"A Syntax-Guided Neural Model for Natural Language Interfaces to Databases","year":"2018","link":"./papers/SYNLIDB_ICTAI_2018.pdf","published":" in The Proceedings of the IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)"}]},{"title":"Visual Question Answering","description":"The VQA task refers to answering questions about an image. Existing methods fuse image and text representations and are able to exploit superficial correlations and produce the correct answer, but often for the wrong reason. The recently introduced Visual Commonsense Reasoning dataset facilitates cognition-level understanding on top of recognition, by requiring not only the correct answer, but also a justification for it. We improve existing methods by leveraging structured representations of images.","papers":[{"authors":"F. Brad","title":"Scene Graph Contextualization in Visual Commonsense Reasoning","year":"2019","link":"#","published":" in The IEEE International Conference on Computer Vision (ICCV) - Pre.Register Workshop"}]}]},{"title":"Reinforcement Learning","description":"Within the field of artificial intelligence reinforcement learning seems a natural setting for training agents that interact with the world we are living in. We engage in furthering the field by developing agents able to learn continuously in different environments. We are also investigating models sitting at the intersection of generative models and reinforcement learning.","people":"Tudor Berariu, Florin Gogianu, Ștefan Postăvaru, collaborating with Andrei Nica","projects":[{"title":"Continual Learning","description":"Recent advances in machine learning are still limited to stationary tasks, but general purpose intelligence would require agents able to acquire knowledge in a continual manner dealing with interleaving tasks. Lifelong learning scenarios deal exactly with this problem: training agents on new tasks while preserving performance on old ones.\nWe are currently exploring memory based, optimization related and architectural methods to train neural models in lifelong learning scenarios.","papers":[]},{"title":"Malmo AI Challenge","description":"The Microsoft Malmo AI Challenge proposed a time-extended stag hunt scneario build on top of the well-known Minecraft platform. In order to maximize its payoff in such a game an agent needs to predict the level of commitment to a collaborative strategy of the other player, decide on a specific plan and navigate through the environment to execute it.\nMulti-agent setups pose additional optimization problems stemming from the non-stationarity of the training objective. Also, situated environments ask agents to learn dynamic strategies capable of dealing with sudden changes in the course of action (such as another playing abandoning the collaborative strategy).\nWe approached the contest by training agents through deep reinforcement learning techniques using recurrent neural networks for policies and for value estimation. We also added auxiliary loss functions (such as next reward, or next frame prediction) in order to complement the sparse reward signal.\n Our submission ranked second for the AI Summer School placement prize and third for the Microsoft Azure for Research Grant prize.","papers":[{"authors":"A. Nica, T. Berariu, F. Gogianu, A.M. Florea","title":"Learning to Maximize Return in a Stag Hunt Collaborative Scenario through Deep Reinforcement Learning","year":"2017","link":"./learning-maximize-return.pdf","published":"in The International Conference on Machine Learning (ICML), at Reinforcement Learning Workshop"}]},{"title":"Towards a generative Experience Replay","description":"Within the field of reinforcement learning with deep neural network estimators, this research proposal is concerned with investigating and then augmenting or replacing the Experience Replay mechanism found in most of the state-of-the art methods with shallow and flexible, action-conditional models of the environment working not in the pixel space but at the level of low-dimensional intermediate representations","papers":[]}]},{"title":"Cryptography","description":"Lattice-based cryptography is a great promise for post-quantum cryptography. We build advanced primitives whose security relies on the hardness of lattice problems.","people":"Miruna Rosca, Radu Țițiu, coordinated by Damien Stehlé, Benoît Libert. Mădălina Bolboceanu","projects":[{"title":"Hardness of lattice problems","description":"Lattice-based cryptography is a great promise for post-quantum cryptography. It aims at harnessing the security of cryptographic primitives in the conjectured hardness of well-identiﬁed and well-studied algorithmic problems involving euclidean lattices. In order to build post-quantum cryptographic primitives based on lattices, we actually make use of some intermediate, more versatile problems, Learning With Errors (LWE) and Shortest Integer Solutions (SIS), which are provably at least as hard as classical lattice problems.\nTo obtain more efficient primitives, different structured variants of LWE and SIS have been introduced. We are interested in studying the hardness of all these problems, giving reductions between them and using them to build new cryptographic primitives.","papers":[{"authors":"M. Bolboceanu, Z. Brakerski, R. Perlman, D. Sharma","title":"Order-LWE and the Hardness of Ring-LWE with Entropic Secrets","year":"2019","link":"https://eprint.iacr.org/2018/494","published":" in The Annual International Conference on the Theory and Application of Cryptology and Information Security (Asiacrypt)"},{"authors":"M. Rosca, D. Stehlé and A. Wallet","title":"On the Ring-LWE and Polynomial-LWE Problems","year":"2018","link":"https://eprint.iacr.org/2018/170","published":" in the proceedings of International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), pages 146-173, Springer"},{"authors":"M. Bolboceanu","title":"Relating Different Polynomial-LWE problems","year":"2018","link":"https://eprint.iacr.org/2018/1035","published":" in 11th International Conference on Security for Information Technology and Communications (SECITC)"},{"authors":"M. Rosca, A. Sakzad, D. Stehlé and R. Steinfeld","title":"Middle-Product Learning with Errors","year":"2017","link":"https://eprint.iacr.org/2017/628","published":"In the proceedings of International Cryptology Conference (CRYPTO), pages 283-297, Springer"}]},{"title":"Cryptographic primitives from LWE","description":"My research project is concerned with building advanced cryptographic primitives, which mainly rely their security on the Learning With Errors problem (LWE). Together with my PhD advisor, Benoit Libert, we are currently working on improving known constructions of some Functional Encryption schemes.\nSome Applications of this primitive: run learning algorithms on encrypted data; perform statistical tests on the sensitive encrypted data that one could find for instance in a hospital or in a bank.","papers":[{"authors":"B.Libert, R. Țițiu","title":"Multi-Client Functional Encryption for Linear Functions in the Standard Model from LWE","year":"2019","link":"","published":" in The Annual International Conference on the Theory and Application of Cryptology and Information Security (Asiacrypt)"},{"authors":"B. Libert, K. Nguyen, A. Passelègue, R. Țițiu","title":"Simulation-Sound Proofs for LWE and Applications to KDM-CCA2 Security","year":"2019","link":"https://eprint.iacr.org/2019/908","published":" under review"},{"authors":"R. Țițiu, B.Libert, D. Stehlé","title":"Adaptively Secure Distributed PRFs from LWE","year":"2018","link":"","published":" in The Theory of Cryptography Conference (TCC), invited to The Journal of Cryptology"}]}]}],"people":[{"name":"Elena Burceanu","bio":"My interest is in understanding videos in an unsupervised manner, currently working on object tracking for my PhD at University of Bucharest and Institute of Mathematics of the Romanian Academy. I have a strong background in Mathematics and Physics and I have finished my BSc in Computer Science and the MSc in Distributed Systems at University Politehnica Bucharest.","img":"elena_burceanu.jpg","contact":{"mail":"eburceanu@bitdefender.com","twitter":"ilarele","github":"ilarele"}},{"name":"Florin Brad","bio":"I am interested in neural generative models for natural language processing, especially for code generation. In particular, I am interested in leveraging discrete structure (syntax trees) to improve the expresiveness of the latent space and to guide the generation process.","img":"florin_brad.jpg","contact":{"mail":"fbrad@bitdefender.com","twitter":"","github":""}},{"name":"Iulia Duță","bio":"I have a BSc in Computer Science and a MSc in Artificial Intelligence at the University of Bucharest. My research focuses on Computer Vision. I am particularly interested in developing techniques for a better understanding and representation of video scene. My current goal is to improve the quality of video descriptions using machine learning approaches.","img":"iulia_duta.jpg","contact":{"mail":"iduta@bitdefender.com","twitter":"DutaIulia","github":""}},{"name":"Florin Gogianu","bio":"Currently pursuing an MSc in Artificial Intelligence from University Politehnica of Bucharest after getting a BSc in Philosophy and graduating from an MSc in History & Philosophy of Science. I have a broad interest in Reinforcement Learning topics and I am currently focusing on questions regarding sample efficiency in the context of model-free value-based methods with neural network estimators.","img":"florin_gogianu.jpg","contact":{"mail":"fgogianu@bitdefender.com","twitter":"FlorinGogianu","github":"floringogianu"}},{"name":"Andrei Nicolicioiu","bio":"My focus is on deep learning methods for Computer Vision problems. I studied at University Politehnica of Bucharest, where I obtained a Bachelor's degree and a Master's degree in Artificial Inteligence. I began with studying some lower-mid level computer vision problems like occlusions regions segmentation and then I leaned over some higher level tasks, like multi-label classification in video, captioning in video, finding common representations between video and language.","img":"andrei_nicolicioiu.jpg","contact":{"mail":"anicolicioiu@bitdefender.com","twitter":"","github":""}},{"name":"Emanuela Haller","bio":"I am a second year PhD student, co-supervised by Marius Leordeanu (Institute of Mathematics of the Romanian Academy) and Adina Magda Florea (University Politehnica of Bucharest). I have a BSc in Computer Science and a MSc in Artificial Intelligence, both from University Politehnica of Bucharest. My main focus is the problem of unsupervised learning and I am currently working on the task of unsupervised learning of objects from video sequences.","img":"emanuela_haller.jpg","contact":{"mail":"ehaller@bitdefender.com","twitter":"","github":""}},{"name":"Miruna Roșca","bio":"I am interested in post-quantum cryptography, with a focus on lattice-based solutions. I have a strong background in mathematics and I am currently a second year Phd student in cryptography at École Normale Supérieure de Lyon, working with Damien Stehlé on hardness of lattice problems over rings. More info about me here: http://perso.ens-lyon.fr/miruna.rosca/","img":"miruna_rosca.jpg","contact":{"mail":"mrosca@bitdefender.com","twitter":"","github":""}},{"name":"Mădălina Bolboceanu","bio":"I obtained both my undergraduate and Master degrees in Mathematics from the University of Bucharest. My goal is to use my strong mathematical skills and experience in mathematical contests and olympiads to solve cryptographic challenges. I am interested in applications of lattices in cryptography, including lattice based homomorphic encryption schemes.","img":"madalina_bolboceanu.jpg","contact":{"mail":"mbolboceanu@bitdefender.com","twitter":"molbozaur","github":""}},{"name":"Radu Țițiu","bio":"I am in my second year as a PhD student in cryptography, under the supervision of Benoit Libert at ENS de Lyon. My focus is on lattice-based cryptography, which proposes promising cryptographic schemes in the eventuality that quantum algorithms could be implemented on quantum devices. Moreover, lattice-based cryptography enables the construction of some provably secure advanced cryptographic primitives including Identity Based Encryption, Attribute Based Encryption, Functional Encryption, Homomorphic Encryption.","img":"radu_titiu.jpg","contact":{"mail":"rtitiu@bitdefender.com","twitter":"","github":""}},{"name":"Marius Leordeanu","bio":"I am an Associate Professor (Senior Lecturer) at the University Politehnica of Bucharest and senior researcher at the Institute of Mathematics of the Romanian Academy. I am interested in the nature of intelligence, life and consciousness. In particular, my research focuses on computer vision, machine learning and robotics. At the university I teach the graduate level computer vision and robotics classes. I have received a Ph.D. in Robotics from Carnegie Mellon University in 2009 and Bachelor degrees in Mathematics and Computer Science from the City University of New York, 2003.","img":"marius_leordeanu.jpg","contact":{"mail":"leordeanu@gmail.com","twitter":"","github":""}}],"posts":[{"title":"Bitdefender at EEML2019","slug":"bitdefender_at_eeml2019","authors":"Tudor Berariu","categories":"event","galleries":"eeml2019_pictures:6","featured_img":"/galleries/eeml2019_pictures/full_house.jpg","date":"September-20-2019","contents":"<p>This summer Bitdefender’s Machine Learning Research Team participated in the\nsecond edition of the Eastern European Machine Learning Summer School, this\nyear’s highlight event for our fast growing machine learning community in\nRomania.</p>\n<p><img src=\"/galleries/eeml2019_pictures/group.jpg\" alt=\"eeml2019_group_photo\" title=\"EEML2019 group photo.\"></p>\n<h2 id=\"what-is-eeml-\">What is EEML?</h2>\n<p>The EEML Summer School gathers top researchers from universities and\ncompanies, offering an intensive week full of lectures and practicals to\nparticipants ranging from undergrad students to post-doc researchers.</p>\n<p>The summer school was successful in its mission to connect Eastern Europe\nwith the rest of the world, welcoming attendees from 42 different countries.\nBeing true to the schools values on inclusiveness, women represented almost a\nthird of the participants, more than the usual ratio encountered in STEM.</p>\n<p>Furthermore, at the national level, EEML offered the chance to meet and\nstrengthen the collaboration between research hubs in Iași, Cluj-Napoca,\nTimișoara, and Brașov.</p>\n<blockquote>\nI like the fact that, although the school grows visibly in size, you still\nhave a chance to talk and share opinions with speakers and other\nparticipants.\n<footer>\nIulia Duță, Bitdefender\n</footer>\n</blockquote>\n\n\n<p>Bringing everyone together for a full week makes EEML an ideal place to\nexchange ideas with others working on similar topics, to draw inspiration\nfrom others’ approach to their problems, and this adds a valuable extra to\nthe overall learning experience.</p>\n<blockquote>\nIt's been more than two months since I participated at EEML but the ideas\nfloated there and the relationships I started still seem highly relevant to\nme - as should be the case for any good summer school. I fondly remember the\nthings I learned from David Nagy (<a\nhref=\"https://twitter.com/dvgnagy\">@dvgnagy</a>) on Bayesian reasoning, the\ndiscussions I had with Rob (<a\nhref=\"https://twitter.com/RobertTLange\">@RobertTLange</a>) on RL, multi-agent\nsystems and much more or the reading suggestions on causal inference I got\nfrom Jeroen Berrevoets (<a\nhref=\"https://twitter.com/J_Berrevoets\">@J_Berrevoets</a>). EEML also\nfacilitated starting a research collaboration with Błażej Osiński and Maciej\nWołczyk after discussing it over some beers.\n<footer>\nFlorin Gogianu, Bitdefender\n</footer>\n</blockquote>\n\n\n<h2 id=\"bitdefender-among-organizers\">Bitdefender among organizers</h2>\n<p>EEML’s mission was laid out by a group of successful romanian researchers\nthat wanted to ensure eastern european talent pool gets access to the same\nresources as the more established academic centers in the western world.\nDoina Precup, Răzvan Pașcanu, and Viorica Pătrăucean were the leading\norganizers of the summer school, and their dedication made everything\npossible.</p>\n<p>Bitdefender was one of the Gold Sponsors of EEML proving its commitment to\nadvance the local research community and invest in local education. Beside\nthat, our own Elena Burceanu was one of the organizers of EEML, putting a lot\nof time and effort into making sure that every little detail will happen like\nclockwork.</p>\n<p><img src=\"/galleries/eeml2019_pictures/bitdefender_booth.jpg\" alt=\"bitdefender_booth\" title=\"Elena and Ștefan at the Bitdefender booth.\"></p>\n<p>EEML is a huge event so it was all hands on deck. Therefore we put the\nvolunteer yellow shirts on, took a deep breath, a sip of coffee and joined\nthe efforts. Our colleagues helped participants finding their way from the\nairport to the accommodation, prepared the welcome bags, they made sure every\nvideo projector works, and that there are spare batteries for the microphones\nin the lecture room.</p>\n<p><img src=\"/galleries/eeml2019_pictures/volunteers.jpg\" alt=\"eeml_volunteers\" title=\"EEML volunteers welcoming the participants.\"></p>\n<p>The summer school kicked off with a humorous presentation about the Romanian\nculture from our own Florin Brad. That moment provided everyone with the\nenergy to go through the full week.</p>\n<!-- ![florin_brad](/galleries/eeml2019_pictures/florin_brad.jpg \"Florin Brad giving the tour.\") -->\n\n<p><img src=\"/galleries/eeml2019_pictures/florin_brad.jpg\" alt=\"Florin Brad\"\n    title=\"Florin Brad giving the tour.\" height=520 /></p>\n<p>In Florin’s own words:</p>\n<blockquote>\nThis year I held a five minute presentation about Romania during the opening\nof the summer school. They say the best way to learn something is to explain\nit to other people, so I hope people have learned one or two things about our\ncountry (because I sure have).\n<footer>\nFlorin Brad, Bitdefender\n</footer>\n</blockquote>\n\n\n<h2 id=\"the-lectures\">The lectures</h2>\n<p>The lectures at EEML covered a broad range of topics in Machine Learning\naiming to both build the theoretical foundations for the newcomers and also\nhint to the research frontiers in the domain.</p>\n<p>Furthermore, the diverse background of the speakers enabled a wide variety of\nlectures, ranging from pragmatic descriptions of the latest research results\nto theoretical crash courses in the different areas covered by the summer\nschool.</p>\n<h2 id=\"posters\">Posters</h2>\n<p>EEML participants were encouraged to present their current work in a poster\nsession that spanned over two evenings (actually well into the night). That\nwas a great opportunity for them to get feedback from the seniors.</p>\n<blockquote>\nIt was a great opportunity to present my work at EEML, getting a chance to\nshare opinions with amazing people with varied expertise in computer vision.\nThis experience had a positive impact and helped me shape my next research\nsteps as many discussions brought a new perspective over my work.\n<footer>\nEmanuela Haller, Bitdefender\n</footer>\n</blockquote>\n\n<p>Our team unveiled three internal research efforts. Iulia Duță and Andrei\nNicolicioiu presented their novel recurrent space-time graph neural networks\nfor video understanding (also accepted at NeurIPS!!!). Emanuela Haller\ndiscussed her work on spacetime graph optimization for video object\nsegmentation. Florin Gogianu introduced his idea to use uncertainty to\nprioritize past experiences for training agents through reinforcement\nlearning.</p>\n<blockquote>\nEEML also provided me with the opportunity of presenting my work on\nBayesian-inspired prioritization measures for the Experience Replay buffer\nduring one of the two poster sessions. It was actually my first\nconference-like poster session I attended so it was a rather interesting\nexperience. I received some great suggestions from Diana Borsa which might\nhelp me polish this research further and also got some follow-up questions\nfrom the people I met there. Somehow I also got one of the Best Poster Awards\nand that wrapped-up nicely the great experience I had attending EEML.\n<footer>\nFlorin Gogianu, Bitdefender\n</footer>\n</blockquote>\n\n\n<h2 id=\"teaching-assistants\">Teaching assistants</h2>\n<p>EEML practicals were challenging applications of the theory discussed during\nthe lectures. The subjects were quite diverse covering Image classification,\nreinforcement learning, recurrent neural nets for language models, or\ngenerative models such as variational autoencoders. The teaching assistants\nhad the job to guide the participants into solving this tasks, and leave the\nsummer school with a consistent experience in training deep neural models\nusing Tensorflow. The TAs were researchers from prestigious universities, and\ncompanies such as DeepMind, and Bitdefender.</p>\n<blockquote>\nFor me, it was quite a different perspective this time, as I join the summer\nschool as a teaching assistant, an attribution that I loved to hold as it\ngave me the opportunity, besides captivating lectures held by top researchers\nall over the world, to interact more with participants and other TAs. It was\na great mix of receptive students, awesome devoted colleagues and a strong\nteam behind all the labs that prepares everything to be fun and valuable._\n<footer>\nIulia Duță, Bitdefender\n</footer>\n</blockquote>\n\n\n<p>Our own colleagues, some of which are also teaching at University Politehnica\nof Bucharest or University of Bucharest (Iulia, Florin, Andrei, Ștefan, Ema,\nTudor) got involved in guiding the participants through the new stuff in each\nlaboratory.</p>\n<blockquote>\nI am a teaching assistant at the university, but here it was way more\nchallenging given the hard topics and the smart and enthusiastic people\nattending. Brilliant researchers were also acting as teaching assistants and\nI really appreciate interacting with them during and before the labs. It was\nvery interesting to see how other TAs explain the topics they are experts\non. \n<footer>\nAndrei Nicolicioiu, Bitdefender\n</footer>\n</blockquote>\n\n\n<h2 id=\"epilogue\">Epilogue</h2>\n<p>EEML was a huge opportunity for Bucharest to join and interact with the\nrelevant actors in the machine learning community. Bitdefender is proud to be\nso deeply involved in bringing this event to our city.</p>\n<blockquote>\nFacilitating new relations between like-minded researchers at the beginning\nof their careers is not the only thing making EEML great. I felt I gained a\nlot by participating or assisting in discussions with researchers like Răzvan\nPașcanu, Doina Precup, Andrei Rusu or Akata Zeynep. Simply hanging around\nthese people and hear them discussing any topic for 20 minutes can leave you\nwith tons of insights and problem formulations they've deeply thought about\nand to which you could not have access to otherwise.\n<footer>\nFlorin Gogianu, Bitdefender\n</footer>\n</blockquote>\n\n\n"},{"title":"Bitdefender at TMLSS2018","slug":"bitdefender_at_tmlss2018","authors":"Elena Burceanu","categories":"event","galleries":"tmlss2018_posters:5, tmlss2018_pictures:5","featured_img":"/galleries/tmlss2018_pictures/salina_1.jpg","date":"August-2-2018","contents":"<p><strong>Bitdefender’s Machine Learning Unit</strong> participated with eight of its members\nat the first edition of the <a href=\"https://tmlss.ro/\">Transylvania Machine Learning Summer\nSchool</a> that took place at the end of July 2018 in\nCluj-Napoca.</p>\n<p>The organizers are well known DeepMind figures (and also Romanians :D), Doina\nPrecup, Razvan Pascanu and Viorica Patraucean and RIST institute from Cluj -\nLuigi Malago and Razvan Florian.</p>\n<p>The motivation for the Summer School was to facilitate new relations between\nthe Machine Learning research community of Eastern Europe with the rest of\nthe world by encouraging collaboration between students and researchers\nfrom Eastern Europe with other more developed research centers and also by\noffering more accessible fees to the school attendees.</p>\n<p>Since the school had excellent lecturers, the competition was tough. From over\n800 participants, only 100 were selected to participate and to present a poster\nwith his/hers research results or work in progress. The diversity of the\nparticipants was a keypoint in the selection, roughly half of the\nparticipants were Romanians.</p>\n<p>Some words about the schedule. Each day we attended to several lectures, some\nof them introductory, other more advanced. At the end of the day we had lab\nsessions on Math, Computer Vision, Language, Generative Models, Unsupervised\nLearning and Reinforcement Learning. At the poster sessions, each of us had up\nto 4h30 to present the poster to anyone interested walking by. A detailed\nprogram can be found here tmlss.ro/programme.php.</p>\n<p>Even though there were six full and exhausting days, our team really enjoyed it\nand tried to get the most out of the lecturers and from the interactions with\nthe participants and the lecturers. Bitdefender was very well represented and\nwe came back home inspired and eager to continue our research.  More, we also\ngot a Best Poster award for Computer Vision (yey!).</p>\n<p>Actually our team enjoyed this experience so much that some of them decided to\ngive an individual account of some of the highlights of TMLSS2018 and the\nlectures that gave them a long lasting impression.</p>\n<p><strong>Tudor Berariu</strong>:</p>\n<p><em>The Transylvanian Machine Learning Summer School represents the perfect venue\nfor all who are interested in pursuing a research career in machine learning.\nFor those that are yet unfamiliar with the field TMLLSS gives the chance to\ngrasp the fundamental theory of a broad set of core topics. At the same time,\nthe more experienced participants are being offered a glimpse of the top\nresearchers’ reflections on the open problems in the field. All in all, Cluj\nwas the place to be this July if interested in machine learning.</em></p>\n<p><em>Amidst all those captivating lectures spanned over six days one caught my eye\nin particular: <strong>Ulrich Paquet’s</strong> introduction to Variational Autoencoders.\nAlthough I held some lectures on the topic myself in the past years, I found\nUlrich’s <a href=\"https://youtu.be/xTsnNcctvmU\">presentation</a> remarkable for its\nconcise, visual, extremely clear, while at the same time strongly theoretical\nelucidation of what and how VAEs do achieve. I wish my lectures were half as\ngood as his.</em></p>\n<p><em>Later that day, while visiting the boring salt mine near Turda, I also had the\nchance to chat with Ulrich and ask him a few questions regarding generative\nmodels, especially Boltzmann Machines. His exact words <em>Do not give up research\non Boltzmann Machines.</em> convinced me to recommence my efforts on training\ndifferent flavours of energy-based models to structure knowledge in\nreinforcement learning setups.</em></p>\n<p><strong>Iulia Duță</strong>:</p>\n<p><em>The main focus of the summer school was to popularize the machine learning\nfields in Eastern Europe, so the lectures’ content was a mixture of\nintroductory and more advanced topics. Knowing that beforehand, I arrived in\nCluj both with the goal of capturing speaker’s intuitions on fundamental\nconcepts in machine learning and insights on open, more recent challenges in\nthe field.</em></p>\n<p><em>One of the lectures that I remarked as being well structured and\ncreated exactly for this purpose was <strong>Dumitru Erhan’s</strong> presentation: “Computer\nVision: Way  Beyond  Classification”. He build the lecture incrementally, from\na classical computer vision problem (Object Detection) to a much hotter topic\n<a href=\"https://arxiv.org/abs/1612.05424\">Domain Adaptation</a>. Both of them were\npresented in terms of how the field evolved over time, highlighting advantages\nand disadvantages of each approaches. While the first part addresses a common\nproblem for computer vision researchers, Domain adaptation is a topic\nextremely important for all machine learning subfields, being a worth studying\nopen problem. So besides useful knowledge and interactions, TMLSS showed us a\nglimpse of novel research and a long bibliography to follow.</em></p>\n<p><strong>Andrei Nicolicioiu</strong>:</p>\n<p><em>The lecture that impressed me the most was the one about Graph Networks\npresented by <strong>Răzvan Pașcanu</strong>. He showed intuitions about the priors existing\nin deep networks and the need for models better suited for domains having\nstrong <a href=\"http://arxiv.org/abs/1806.01261v2\">relational priors</a>. We were\npresented in a unified way different views of the domain from general graph\nnetworks to simpler graph convolutional architectures, showing the usefulness\nof relations both in problems involving data with a more obvious graph\nstructure as molecules and in unexpected places like the interactions of\nmemories in a recurrent model. The presentation certainly inspired me to study\nmore about these approaches and how can they be applied in my research.</em></p>\n<p><strong>Dana Axinte</strong>:</p>\n<p><em>Neural networks, gradients, posters, enthusiasm, magic. These are some of the\nsettings of the first Transylvanian Machine Learning Summer School held in\nCluj, Romania. Looking through the schedule and speakers, I expected the school\nto be strong and that I would have the chance to settle new concepts while I\nwould engage with other participants with different backgrounds, but same\ninterests.</em></p>\n<p><em>I liked the balance between theoretically focused lectures and the ones that\npresented the industry usage of those concepts. Such pair of lectures that I\nparticularly appreciated were the ones by <strong>Doina Precup</strong> who took a\nprogressive approach in presenting value-based methods with function\napproximation for reinforcement learning and the one of <strong>Raia Hadsell</strong> who\nshowed how deep reinforcement learning can be applied into\n<a href=\"http://arxiv.org/abs/1610.04286v2\">robotics</a>, with the challenges that exists\nand the possible <a href=\"http://arxiv.org/abs/1804.00168v2\">solutions</a>.</em></p>\n<p><em>One of the parts of the summer school that I especially enjoyed was the panel\nsession where some lecturers shared their opinion on different interesting\ntopics. The reproducibility of experiments, the future of AI research and\nadvices to increase the local AI ecosystems, as what could be the involvement\nof governments, companies and teachers were few of the issues discussed. There\nwere no hype or buzzwords, but the lecturers were people who know today’s\nlimitations, needs and questions and get things done step by step, with an\nengineering mindset. A fundamental feeling with which I left is that it is not\nonly about learning, but sharing the experience and knowledge gained to empower\nothers to create and to strengthen the community and motivation.</em></p>\n<h3 id=\"conclusions\">Conclusions</h3>\n<p>It was only the first edition, but every detail was at a very high standard. In\nthe last days, we presented with Traian Rebedea the advantages of having the\nnext Summer School edition here in Bucharest. Looking forward to TMLSS2019 :)</p>\n"},{"title":"Recurrent Space-time Graph Neural Network","slug":"recurrent-space-time-graph-neural-nets","authors":"Iulia Duță, Andrei Nicolicioiu","categories":"blog","featured_img":"/galleries/rstg_pictures/featured_graph_nets.jpg","date":"November-26-2019","contents":"<!-- featured_img: \"/galleries/rstg_pictures/side_banner5.png\" -->\n\n<p>We introduce in this post our <strong>Recurrent Space-time Graph Neural Network</strong> (RSTG) architecture designed for learning video representation and  especially suited for tasks heavily relying on interactions.</p>\n<p>Let’s begin by considering the key components of video understanding that our method should include. Being able to detect and localise objects is the first crucial thing that we need to do, then we should combine them in various ways to create complex scenes. <em>Whole is greater than the sum of its parts</em>, but is it always true, and what are the conditions of this happening? </p>\n<p>In order to form a greater whole, objects must have some kind of connections between them. A set of random objects that do not correlate in any way doesn’t bring much additional information.  Entities could form different types of connections, they could be semantically related or they could be related by their physical position in space and in time. These kinds of relations happen in both image and video, with the time dimension of the video adding more difficulty, greatly increasing the amount and complexity of interactions happening in the scene.</p>\n<p>Let’s define the kind of interaction happening in the video by examining a few examples.</p>\n<!-- ![eeml2019_group_photo](/galleries/rstg_pictures/rstg.gif \"RSTG architecture.\") -->\n<p><img src=\"/galleries/rstg_pictures/interaction_videos.gif\" alt=\"eeml2019_group_photo\" title=\"Temporal interaction.\"></p>\n<p>Above, the yellow car and the grass appear together during the whole video, but the event <em>the car crosses the line</em> is characterized by a specific interaction. We call those types of interactions happening at the frame level <strong>spatial</strong> interactions. On the other hand, the event <em>the yellow car overtakes the red car</em> can not be captured from a single frame as it only makes sense across time. We call this <strong>temporal</strong> interactions.</p>\n<p>The entities that interact shouldn’t necessary be close to each other, either in time or in space, since there could also exist <strong>long-range</strong> interactions. In the above picture, the man and the moon are connected regardless of their distance in the image.</p>\n<h2 id=\"analysing-videos-with-spatio-temporal-graph-models\">Analysing videos with spatio-temporal graph models</h2>\n<p>Models commonly used in Computer Vision, based on convolutional networks, implicitly capture interactions in both space and time dimension, but they are biased towards local, short-range relations. </p>\n<p>We propose a method designed to explicitly model relations and that is capable of capturing long range connections. Our model fits into the broader category of <strong>graph neural networks</strong> (GNNs). We process the video imposing a graph structure in order to explicitly model interactions between different entities. GNNs usually send  messages between each pair of  nodes, thus easily modeling binary interactions. Higher order interactions could be achieved by multiple such message-passing iterations. </p>\n<p>The animation below illustrates the main components of our model.</p>\n<!-- ![eeml2019_group_photo](/galleries/rstg_pictures/rstg.gif \"RSTG architecture.\") -->\n\n<img src=\"/galleries/rstg_pictures/rstg.gif\" alt=\"RSTG architecture.\"    title=\"RSTG architecture.\" max-width=130% width=3584   />\n\n\n<p>At each time step, we <strong>create nodes</strong> by extracting information from features given by a convolutional network. Each node corresponds to a different fixed region of the input and we connect them if they come from neighbouring regions or if they overlap, as shown in the figure above. Using multiple scales helps us to capture entities of different sizes  and also to connect distant regions more easily.</p>\n<p>For the two types of interactions described above, we create two separate processing components. For each time step, we design a <strong>space processing</strong> stage to model the spatial interaction by iteratively  message-passing. This involves 3 steps: sending messages between each pair of connecting nodes, aggregating the messages received at each node by an attention mechanism and updating each node based on the current state and the aggregated message. We design a <strong>time processing</strong> stage to model temporal interactions by sending messages in time, only between the states of the same node, in a recurrent fashion. More specifically, at each time step, each node receives information only from its corresponding state from the previous time step.</p>\n<p>To model more expressive spatio-temporal interactions and to give it the ability to reason about all the information in the scene, with knowledge about past states, we alternate the two processing stages, as shown in the animation below.</p>\n<p><img src=\"/galleries/rstg_pictures/rstg_stages.gif\" alt=\"rstg_stages\" title=\"Alternative Space and Time Processing Stages.\"></p>\n<p>By having multiple space iteration, we go from local to more global processing, modeling interactions happening between an increasing number of entities situated at increasingly longer distances. We want to have some temporal information at every such steps, in order to model interactions that takes into account the history.  In order to combine the same kind of features from different time steps, we only connect the k-th spatial stage with the k-th stage from the previous step as shown in the animation.</p>\n<p>We can pool all the nodes into a vector representation used for the final prediction or we could project back each node into its initial corresponding region, forming a feature volume with the same size as the graph input so that our model could be used as a module inside any other architecture.</p>\n<!-- ![florin_brad](/galleries/eeml2019_pictures/florin_brad.jpg \"Florin Brad giving the tour.\") -->\n\n<!-- <img src=\"/galleries/eeml2019_pictures/florin_brad.jpg\" alt=\"Florin Brad\"    title=\"Florin Brad giving the tour.\" height=520 /> -->\n\n\n\n\n<!-- ![eeml_volunteers](/galleries/eeml2019_pictures/florin_brad.jpg \"EEML volunteers welcoming the participants.\") -->\n\n\n\n\n<h2 id=\"synthetic-dataset\">Synthetic Dataset</h2>\n<p>Training on a large real world dataset takes a lot of time and computational resources and could also involves hidden biases that could mask the capabilities of a model. For example, because of an unbalanced dataset, the activity of skiing could be detected only from the context of a snowy scene.  Thus, we designed a synthetic dataset where the complexity comes from the necessity of explicitly modeling spatial and temporal interactions but in a cleaner, simpler environment.</p>\n<img src=\"/galleries/rstg_pictures/digits.gif\" alt=\"SyncMNIST\"    title=\"SyncMNIST\" width=600   />\n\n<p>Our <a href=\"https://github.com/IuliaDuta/RSTG\">SyncMNIST</a> datasets involves videos where the goal is to detect a pair of digits that move synchronous among others that move randomly. On this dataset, we validate our key design choices by conducting ablation studies. </p>\n<p>We show that it is important to have both processing stages, each having its own set of parameters and to have multiple alternating temporal and spatial stages. We note that our model is also improved by incorporating positional embeddings in the node features. Our final model, that includes all these elements  surpasses strong models such as I3D and Non-Local.</p>\n<h2 id=\"real-world-experiments\">Real world experiments</h2>\n<p><img src=\"/galleries/rstg_pictures/smt-smt2.gif\" alt=\"smt_example\" title=\"a) Failing to put something into something because something does not fit b) pretending to put something into something\"></p>\n<p>To validate the capability of our model, we evaluate the RSTG model on a human-object interaction dataset - <a href=\"https://20bn.com/datasets/something-something/v1\">Something-Something v1</a> . This dataset contains fine-grained actions, that can not be distinguished solely on their context, where the interactions between entities across the entire video are essential.  We compare against top-models in the literature and obtain state of the art results.</p>\n<p><img src=\"/galleries/rstg_pictures/chart.svg\" alt=\"results\" title=\"Something-Something Results\"></p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>We hope that graph based methods would be more broadly  adopted in visual domain tasks, especially those where interactions play a crucial role and that our methods brings more evidence that such models could be successfully applied in such tasks. </p>\n<p>Our proposed RSTG model, seen as a spatio-temporal processing module, could be used for other various problems. Key aspects proved by our experiments,  such as the creation of a graph structure from convolutional features or the coupled but factorised time and space processing could be integrated into other models.</p>\n<p>We released the code of our models and SyncMNIST dataset\n<a\nhref=\"https://github.com/IuliaDuta/RSTG\">here</a>.</p>\n<p>More details  about our work could be found in our\n<a\nhref=\"https://papers.nips.cc/paper/9444-recurrent-space-time-graph-neural-networks\">paper</a>: </p>\n<p>Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu, <em>Recurrent Space-time Graph Neural Networks</em>,   In Advances in neural information processing systems (<em>NeurIPS 2019</em>).</p>\n"}]},"siteData":{"title":"Bitdefender Machine Learning & Crypto Research Unit","description":"Bitdefender Machine Learning & Crypto Research Unit goals are to further the fields of machine learning and criptography while engaging with the international research community and to develop the local AI&ML scene by supporting and participating in local conferences, lecture and research groups.","tagline":"Engaging with the broader Machine Learning Community.","tags":["machine-learning","research","bitdefender"]}};</script><script defer="" type="text/javascript" src="https://bit-ml.github.io/bootstrap.fdc7d33e.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/templates/src/pages/Home.09cabfc6.js"></script><script defer="" type="text/javascript" src="https://bit-ml.github.io/main.b0acd7e8.js"></script></body></html>